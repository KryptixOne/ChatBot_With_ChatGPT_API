I made a robot that punishes me if it detects that if I am procrastinating on my assignments [P]. nan. Give it your credit card and have it donate every time you stop working. Making that robot was procrastinating for sure as well. amazon would like to know your location. Here‚Äôs the development process and code: https://youtu.be/YPSazrEqlxo

Lmk your thoughts!. That is cool. However, procrastinating is a great thing to do. Most of my favorite papers and projects I've worked on come from me getting up from my desk and walking around the department looking for people to have coffee and random discussions so I don't have to work. So while maybe studying is important not to procrastinate, I have never found it detrimental in the long run.. Does it give you a spanking?. Only issue is you can defeat the robot and still procrastinate. This is amazing.. Please don‚Äôt give this to my employer.. Building this setup myself for sure would be a great way to procrastinate on my thesis ü§î. How humans became slaves to their robot overlords: Genesis.. Nice work!. Amazing implementation. Nice work. Even though building that robot was definitely procrastination. 1. Bravissimo!. I need this right now. [deleted]. This is very Dystopian tech.. Haha dang thats wild.. U/savevideobot. so what if you tape the pencil to the back of your phone?. Thanks for nothing. This will never be good. In any fashion or duty, unless you submit to it. What happens if you don‚Äôt submit?. Please don't.... As long as toothbrush is in hand, you're doing good work, frand. Kids getting a job at Amazon. That's awesome dude! I love it.. What if You move away and sit on the bed with your phone?. Now you have to program it to prevent you from disabling it, and that kids is how Skynet started.. Making that was hardcore procrastination. Great way to give yourself tinnitus. There are TWO LIGHTS üòÇ. *Amazon wants to know your location*. Now THIS is "machine learning". Does anyone know how is the network detecting multiple objects at once? Can a network have variable output sizes for detecting more than one object?. That sound is so bad, it annoyed me with my headphones a meter away from me. Michael Reeves ain't got shit on you. Thats an amazing accomplishment. üåå. Studying is a massive waste of time. It is glorifying the special human characteristic of being terrible at retaining information. The simple solution would be to manufacture a memory retention system within your brain that didn‚Äôt totally suck and didn‚Äôt require you to study in the first place. Should be able to simply copy the information into your head.. Lol üòÇ. Sal would be proud. That pen flip lmao. Keyboard?. Keyboard name? Also which pre-trained model did you use?. I think my procrastination is so strong I‚Äôd need pepper spray from the robot to truly scare me into submission. So what phones are those and is it using the camera?. At first I was reading punches.... Genious!. Pomodoro Technique, you deserve breaks. [Posted on Reddit]. https://youtu.be/TTm7RzLKHIw

You should up your flash-bang game.. Every employer in the world drooling at the idea and wondering what slow frog boil method they'll use to get there.. How‚Äôs the K8 Pro?. I was going to comment on this but I think I‚Äôll just leave it till tomorrow.. Dope,

Although I dunno how you flicked your pencil at ur monitor like that I could never.. this shit is so stupid, hide the phone off camera also pretend yo wobble the pen all the time so it think youre doing something, waste of time but im sure amazon would love this , they already stick the camera to their trucks and measure how often drivers are distracted and arent thinking about their work.What a shitty use of AI.Its supposed to help people and not help to punish people by non stop checking up on them.. the pencil flick on the monitor man. cool project. I read ‚Äúpunches me‚Äù. Kept waiting for the robot punch. Disappointed.. Is this the next thing companies are gonna put in to increase work?

Sir, this is great power and you have great responsibilities that come with. Don't sell this algorithm. Song?. your brian needs breaks. it‚Äôs ok.. This was the push I needed to delete Candy Crush and Clash of Clans from my phone. This is a great idea! I'm going to implement at my office so my employees stay focused! /s. AWESOME. Now imagine all the assignments you could have done instead of building this robot. Could we say it's procrastinating?.  lol plz dont show this code to the ccp. Great work, mate!. Who monitors if the robot is procrastinating?. This belongs in r/LateStageCapitalism. Me with ADHD:

 "alright robot, you're going to have to kill me".. Make the robot *later*.. The pencil flip, so good.. Instead of the high pitched beeping it should play industry baby. shut up and take my money. Nice keyboard! What it's called?. I think the best way to prevent wasting time is to turn off the phone). The man created hell. You made the robot while procrastinating? XD. This is great. *Until schools have them.*. What DB tech are you using to communicate with detector?. I don't think negative reinforcement is a good way to deal with that.... Let's see Paul Allen's procrastination punishment robot.. I need this!!. I would simply turn the robot off, I am too devoted to procrastination. 

I am inevitable.. Name of the song?. Good idea but blinding your eyes will probably decrease your ability to focus. Nice. But this ain't a robot and all you have done is recognize the mobile phone using a camera, not recognize 'procrastination'. I guess this is the difference between what technical specs says and what a marketing guy says.. Procrastobot. wow what a torture jail time equipment. My cat HATED that omg.. Didn't think we'd be automating doms anytime soon .... Bro can you share this app with us?. Cute but also scary! Modern-day equivalent of a whip.. Love this! Wonder how much more productive I would have been in college with this kind of tech.. I think my robot will punish other people when Im procrastinating.. Honestly how dare you make this bro. The robots existence is to punish its master from indulging in the same activity that created its own life.   


The robot might interpret it as its own creation is a mistake, thus leading to a low self esteem and daddy issues.   


Thus it's good that it makes you not procrastinate, as you will become more successful and be able to afford psychiatric help for the low self esteem issues and heal it from its existential dread.. what if you use the phone behind book trick?. Punishment not severe enough. Make it tase you with a projectile taser.. Sadist bot critical systems online and fully operational. A dream for all capitalists. Bruhhh. Reminds me of that old pact website, where you paid a monthly subscription to promise to visit a gym and then if you went (by tracking your GPS) it would pay you back yours + a share of everyone who didn't. 

Of course people eventually abused it  and I don't think it exists anymore, at least not in the same form.

Edit: looks like it shut down many years ago https://www.mobihealthnews.com/content/khosla-backed-fitness-startup-pact-shuts-down. ‚ÄúI PICKED UP MY CALCULATOR YOU ASS HOLE‚Äù. To the Flat Earth Organization. Capitalism likes this one trick.. sacrifices must be made for the greater good. the dragon procrastinator, our battle will be legendary. It‚Äôs called an investment. It's the good kind of procrastination: You might not be doing what you were told to do, but you're developing equally valuable skills.. They have shops that use this to tell what you're buying, so there's no checkout. They definitely could do this if they wanted.... Can your algorithm differentiate between a cellphone and a calculator?. Oh god know, please don‚Äôt make this open source. What if Amazon finds it.. I see the video, but where's the code?. what you are doing is more like taking a break. But here the procrastination would be like using your phone after solving 1 or 2 questions for 5-30 minutes while doing a set of 20 questions. It's possible that you might not even solve that set on the same day.. It always depends on the type and amount of procrastination. For some people, procrastination means doing something that‚Äôs more fun than their main task, but it‚Äôs still kind of productive and actually fun. For others, it means relieving pressure from work by doing something more mundane that gives you immediate gratification, like browsing Reddit. And that can be in total mind numbing and unproductive.. Nah dude, procrastinating is awful.. For work projects and for people who work 9-5 procrastination is like "what is that?" You don't even know.

For personal projects where there's no boss and no deadline or any immediate real life consequences procrastination is the biggest difference in performance. And really THE ONLY difference.

It goes from working on a personal project for 5 hours a day to working on a personal project for 5 min and then being distracted for the rest of the evening.

I'm pretty sure most people can't really accomplish anything worthwhile 99% of it is because of procrastination. 1% is because of natural talent/intelligence.. Your experience does not mean procrastination = ‚Äúa great thing to do‚Äù. Maybe you didn‚Äôt find it detrimental but someone else might. 

OP made something awesome. You made a comment to talk about yourself. üò¥. No it‚Äôs supposed to be a punishment.  Don't give instructors any ideas lol. That's the punishment. It'll annoy you till you put the phone down. Just the lights to the face would be easier to ignore.. This guy read 1984 and said, "Why should the government have all the fun?". You can easily build one by fine-tuning YOLO. You can read up on this, but the way these algorithms work is by guessing a bunch of bounding boxes and predicting class probabilities for each one. It's actually very straightforward to build object detectors--all you have to do is label a few images (you can even use an online image labelling software). Check out [https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5) for more details--given labelled data, you can probably get a model working in under 30 minutes.. Yes, PM me I can answer any algorithmic questions. I agree, most of the knowledge we had to memorize is useless over lifespan, people should be specialised in one particular subject , the school system is made so everyone ould kinda fiture out his future work but it doesnt really work that well IMO.Theres a lot of things they dont teach at school for example how to deal with court case, how to use connections to find a job, how important is CV in some work enviroments, they want to milk the students for loans... oh well.Whenever somethign weird happens just lookup moneytrail and youll find the answer.. he‚Äôs watching through the webcam üëÄ. Keychron K3 Ultra-slim Wireless Mechanical Keyboard (Version 2). Keyboard: Keychron K3 Ultra-slim Wireless Mechanical Keyboard (Version 2)

Pre-Trained Model: YOLO Object Detection for Python. No the phones are purely for the flashlight, the webcam in the middle is doing the CV. Watch the full video for the rig setup: https://youtu.be/YPSazrEqlxo. amazing. Keychron K8 Pro. yup. Firebase Firestone. Watch the vid I show how i implemented it. Code in description as well: https://youtu.be/YPSazrEqlxo. Haha. Vid of how I made it here: https://youtu.be/YPSazrEqlxo 

Object detector code in description. Yeah. Code is in the description of the full video.

https://youtu.be/YPSazrEqlxo. [deleted]. Killed procrastination with procrastination. Haha, man, I have the same problem as OP

I'll crank out some awesome script or app no problem, but only if it feels like I'm putting off something more important

I need to figure out a way to trick myself into thinking that my day job is a way to procrastinate from something. It can from the front. Like it can see the buttons of a calculator but on the backside it‚Äôs a coinflip. Amazon is going to hit you with a PIP no matter what, may as well just embrace it.  The dead can never die. The link to the object detector code is in the video description. Reddit has never been a productive use of my time.. ¬ø? I didn't hear anything :-/

Old deaf human 1 - Plantation owner robot 0. Oh I already took the idea you fool. Very helpful answer, I will give it a try. Thank you.. Thanks.. Cheers, I will see through it and figure out how Firebase interact with the program.. The greater good. crusty jugglers. The gooder great. Narb. I mean, in a sense you're procrastinating / putting off becoming homeless.  


You're welcome :D. Awesome job. 

I will see the code but it seems cameras can be detected.. Well, that wasn't there before.
But thanks for updating it.. Plantation owner robot's floppy disk: Error 404. A great big bushy beard!. I cracked the code, dawg

The trick is that while you're working, you're not taking care of your children. If I had kids, this would 110% be my #1 answer lolThe pain and excitement. nan. Upper management doesn't care.. real pros just switch to Œ±=0.1 
easy. How many of you are using p values in industry?. No one is happy with an insignificant little p. Lol RA Fisher and his arbitrary number.. I know this a meme, but remember that 0.05 is arbitrary, you can still go forward with one that is larger, there is no law that says 0.05 is the only valid one.. "trending towards significance". p=0.0499, reaching statistical insignificance.

I would say this is a false positive. What's the distribution like? Show me the data!. Just ‚Äúremove outliers‚Äù and p < 0.05, boom!. p < 0.005 or bust dawg.. The fact that this arbitrary threshold is still so deeply embedded in academia is proof much of the academic research community is focused on publishing research, not necessarily publishing useful research.. Could you crunch the numbers again?. p-values are weird. They're simultaneously overrated by people who don't understand what they are and yet underrated by people who do.. ...until you learn that you can make [an experiment](https://youtu.be/tLM7xS6t4FE) that shows a statistically significant probability that dead fish can answer questions.... p=0.068

p=0.07. Sometimes you have to repeat the experiment 20 times for it to work.. Hopefully you don't need a bonferoni adjustment!!!. I'm glad there's finally some stats talk in this sub. It's usually comp sci and programming dominated.

But uh, give me a big enough sample size and I'll make you a model that shows everything is significant.  Since data science is usually big data sets, pretty much everything ever is going to be p<0.000000000000.

Word of caution to folks who are new-ish to industry:  Don't be the guy who presents 'highly significant' findings of p<0.05 on a data set of 1 million observations, or even a couple hundred thousand observations.

You might be able to get away with it, but eventually you're going to run into someone who can torpedo you.....!. [deleted]. As a statistician background‚Ä¶. This is 100% accurate in the private field. What do you use p-value for? I'm a data scientist for almost 4 years and don't understand why you need it. Dont you have other metrics such as ROC AUC, F1 (macro/micro) , losses, accuracy, MSE, L1, R2 score, ...???. Can anyone explain for someone who is only a couple months into programming? üòÅ. A lot of conversation about .05 being an arbitrary number but if you set your CI at 95% at least you can say that your population estimate does not include 0. Or am I incorrect?. Here is the reasoning for some P-Values and 0.05 is 2 deviations.

https://en.m.wikipedia.org/wiki/68‚Äì95‚Äì99.7_rule. [deleted]. What kinda data scientist uses p-values?

EDIT: I‚Äôm actually dead serious. What data science projects are y‚Äôall working on that uses p-values? Don‚Äôt most of us work with datasets big enough to make the use of p-values kinda silly?. I reaaly hope some day this thing is not longer used. I know nothing about the subject matter. someone explain joke plz. Oh Jesus. P value , the last refuge of people who have no fucking clue what you are doing or why.

P= 0.03 better than p= 0.24. 

P=0.049 is no different than p=0.051


Moronic academics that feel special as gatekeepers are ruining the usefulness of data science.. Actually since they tested twice here you need to account for multiple testing correction and the true 0.05 false positive rate is more like 0.025.. I worked in predictive analytics at an insurance company and we would only toss variables if they were > .5 ... 

Underwriters have a *gut feeling* that those variables are predictive, so we have to use them.. just include 20 variables in your model... you're welcome. üôÑ. Lololol. Who the hell is out here relying on p-values in 2021?. #ReJeCTdANulL. Now that is quality product.. Better switch to Bayesian factor :). Just got through reading a whole article on this. Statistics is about measuring uncertainty. Trying to shoehorn every measurement into fitting that p value is silly.. I don‚Äôt get it :/. Oh the truth in this üòÇüòÇüòÇ. Am I going crazy or are these facial expressions backward? The top one is supposed to be happy and the bottom is unhappy, right? The numbers don't match.. You say that.. but tech firms still evaluate AB testing at .05 which really is crazy. We really need a more gradient approach for non-life-or-death decisions.. No one should care. Those are the same number for all practical
Purposes. But that increases the type II error. We use them in finance on credit risk models. There's certainly a decent amount of emphasis on p-values. You can get away with a high p-value variable in your model but the amount of justification required on why you have decided to include a non-significant variable just makes it a pain in the ass.. Pharma clinical trials yep. P-values, as applied to business problems, are a risk management tool. Nearly nobody in business knows how to assess risk, so they're rarely useful.. I‚Äôve ‚Äúused‚Äù them as in produced them. But quickly realised nobody gives a rats ass about them.. Tech marketing. Yes, but the higher up in leadership you go, the less anyone wants to hear about it. 

An inconclusive experiment is a failure, and you've lost rapport with them. 

A conclusive experiment in the direction opposite to what they've been writing in their whitepapers is likewise a failure, and you've lost rapport with them.

Just run some descriptives until you find the average that lets them say "see? I told you so!" in their next whitepaper or all-hands meeting. You'll be famous, in no time.. We use them to evaluate all our A/B tests for our video games. Marketing campaigns to determine lift in A/B tests. My experience has been that management isn't satisfied unless the p-value is less than 0.05. Same with the few times I've done regression modeling.. My team doesn't deploy a new model unless it shows stat sig improvement in an A/B test.. Plant breeding, especifically genomics, but it's usually a corrected p value. I actually prefer a small p. It‚Äôs the big painful p‚Äôs that I dislike.. It was Neyman and Pearson who popularized binary hypothesis testing. Fisher was always mindful that 0.05 was a convenient, but arbitrary cutoff. Fisher had this to say:

> [‚Ä¶] no scientific worker has a fixed level of significance at which from year to year, and in all circumstances, he rejects hypotheses; he rather gives his mind to each particular case in the light of his evidence and his ideas.. '19 outa 20' if you wanna sound really convincing. king of statistics here to say that this is untrue. if you set alpha > 0.05 regardless of context you will be thrown in jail.. [deleted]. Yeah, the problem is choosing one in some principled way. In a lot of cases, I'm wary of giving non-stats people (or stats people with fewer qualms about data dredging) another lever to make it easy to get a green light out of their experiment so they can brag to management.. or away from

It‚Äôs misleading to apply the sentiment of a direction. If you looked multiple times you need to account for that bro. 1

2

3

NaN

58901

NaN

NaN

NaN

NaN

There you go. How you like them datas?. p-values can be derived from many different parametric models. Usually a chi-squared, normal distribution (usually standard normal i.e Z), or t-distribution. But it really depends on the data.

Incidently, statistically independent tests for a null model will generate p-values that follow a continous uniform distribution between 0 and 1. Anything that either results in a non-null model or is not actually statistically independent (e.g. some tests are correlated so produce similar p-values more often than they don't) will produce a beta distribution. A beta distribution is just a uniform distribution that is skewed.. oops removed wrong outliers p = 0.1 now. This is the only meaningful comment in this entire thread.. would just have to adjust for multiple testing anyways. lmao. https://www.psychology.mcmaster.ca/bennett/psy710/readings/BennettDeadSalmon.pdf. P=0.069. Haha I was just going to say that!. Sorry, can you elaborate on it a bit, why would huge datasets result in all covariates being significant?. Why is pvalue a problem with bigger datasets?. no one deserves to be poor!. Hypothesis testing. Common example, evaluating the results of an A/B test experiment.. Data scientist for 4 years, yet conflates p-values with loss functions? How would you conduct a DoE using the aforementioned metrics? .... This is a statistical concept, not a programming concept. To describe it really roughly, when we analyze results of something we ask ourselves, "Can we conclude that something important is happening here? Or are these results just a matter of chance?" A P value is what we use to determine what the chances are that the results would occur - the lower the p value, the lower the chances. This means that there is some kind of important observable correlation happening, because the results are not just a matter of chance. Statisticians can determine what p value they will deem "statistically significant." A very common one is .05. If an experiment yields something less then .05 p value, they will label that statistically significant, but more than that they will say it can't be concluded that something is happening here. This is somewhat arbitrary and it is a human categorization. It doesn't have to be .05. It could be less, it could be more depending on the context. This meme is making the joke that we would consider .051 not statistically significant, but .049 would be, highlighting that this is an arbitrary distinction. Hopefully I've explained that correctly, please let me know if I misexplained anything.

If you want to learn more about this concept, which you definitely should if you're going into any data-based job, you'll want to google "p-value" and "statistical significance".. This might be more of a science/stats joke than programming. 

Basically, general convention in science/stats is that p<0.05 is considered a significant relationship and, generally, neccessary for publication. So the bottom photo is just barely scraping by but it doesn't matter as long as you get less than 0.05. 

0.05 is an arbitrary number, and things like p-hacking or adding new trials to try and reach it can result in false positives. Some people have suggested moving to 0.01, and some clinical research where 0.05 would be nearly impossible might be okay with higher values. But generally there is a perception that the idea that 0.05 is some holy number is a source of frustration for many.. P value measures how likely, if there‚Äôs no real effect, you would be to seemingly ‚Äúfind an effect‚Äù of whatever size you found in your sample. Lower is better, because that indicates it‚Äôs less likely you got a spurious result.

Many studies use the threshold of p<0.05 (less than a 1/20 chance you‚Äôd see something like X if no real effect exists), so some relatively unethical folks engage in ‚Äúp-hacking‚Äù whereby they manipulate the value down to juuust below 0.05.

Really, especially in our big-data era, one should aim for p values a hell of a lot lower than 0.05. When you have X million data points, a 1/20 chance is basically bound to happen in a large subsample of them.. You really can‚Äôt say that even at 95%. Anytime you've collected data under two conditions and your hypothesis is that the two conditions won't change the data.

I.e. collecting internal body temperatures of people wearing socks vs not wearing socks where you hypothesise socks are irrelevant to body temp.. Yes. Studies funded by people who don't want to reject the null hypothesis.

"Ooops. Inconclusive. Shucks. There's just not enough data. Rats! Better keep on businessing as usual, I guess.". Its true, i hardly see it in my day to day work especially in deep learning... Very often used in product data science when evaluating the impact of product changes. healthcare is a huge one. I worked at a bank for a bit and we used them all the time as our regulating body didn't like black-box models. As a result, you're pretty much left with GLMs and well, p-values.. It's a joke man. Yes that is the joke.. healthcare always has and always will. I'm taking a regression class for my MBA and in the first class the prof complained about how the p<0.05 threshold is absolutely ridiculous and that p value should be used as a clue in the puzzle rather than the be-all/end-all cutoff. There is so much different risk tolerance across industries and sectors that it doesn't make sense to use one universal #.. What do say when it leads to a bunch of conflicting conclusions?. Oh believe me - there are plenty of folks taking a gradient approach. If you‚Äôre lucky they know just enough stats to know where they‚Äôre taking risks and making assumptions vs blindly letting an invalid conclusion guide their decision making.. Me and my homies hate type 2 error. And?. You must be new here. Clinical trials have prescribed analytic procedures though. In many cases the ‚Äúanalyst‚Äù is just someone with a bachelors running a SAS script. The data scientists in pharma usually work on the earliest phases of drug discovery or (more commonly) for the business side doing finance/process optimization.. if you do this, then you're the problem. This is precisely why we need more math minded individuals getting into business facing roles and then evangelizing changing directions when wrong or at the very least, admitting the data doesn't support the decision but proceeding anyways.. Not sufficiently dismissive I suppose. Too high p value? Straight to jail.

Too low p value, believe it or not also jail.. They are already on my door knocking, who snitched???. oh shit hello mr CEO of statistics. Once, on another subreddit, I said that 0.05 isn't a magical number. There is no statically significant difference between 0.05 and 0.06. 

Yes, as you can guess, I was lectured on how wrong I am.. As a physicist, if your choice of p-value mattered, your experiment was shit. 0.1, 0.05, 0.01, all the  classic choices are very low bars. Show me a p-value that needs writing in scientific notation!. I get what you are saying, but I would call 0.0499 ‚Äûtrending away from significance‚Äú. 0.051 cannot really be trending away from significance because it is already not significant. But in principle you are right, we do not know the ‚Äûdirection‚Äú. I only looked once! I swear! I'm a Dr.!. If you could just make those NaNs disappear, then you got yourself a Nature or Science paper. Think about it... The sample size should be enough.. And that's when they will begin to hate the name Bonferroni.. 69 lmao. Not OP, but the reason is statistical power. The more observations you have the greater your statistical power, which is the probability your test will obtain a statistically significant result from your sample assuming that one actually exists in the population. With great power comes the ability to detect extremely small effects as statistically significant. 

P-values are a convenient tool for making inferences when we don't have the resources to collect giant samples, but with big data, it makes more sense to estimate effect sizes to get an idea of how much something matters rather than using a p-value to decide whether something matters. 

Perhaps not absolutely everything you throw into a model would come out as significant, but with enough data, pretty much anything you could reasonably imagine to affect your outcome variable would. A p-value in most cases is testing against the null hypothesis, or 0 effect, and when you have 99% power to detect even tiny effects, you will find them, and at some point the idea of p-values becomes silly.. For a consistent estimator xÃÑ,  we have: P(|xÃÑ - Œº| > Œµ) ‚Üí 0 as the sample size n ‚Üí ‚àû , aka convergence in probabilities. As a result, tiny values of Œµ become significant when n is extremely large.. See my reply above.. Can you describe it further please? How do you evaluate A/B testing with p-values?. DoE?
Can you just give me an example of where pvalues are useful?. You guys are awesome for explaining this to a newbie like myself. I feel like you just gave me a sneak peek into my first data science class coming up in August haha.. Thanks for the detailed response! That makes total sense. I will pretend to read the joke again for the first time and ‚Äúlol‚Äù. Lol. You're right. "I'm 95% confident...". [deleted]. Why not use effect size? It‚Äôs the effect size you need for doing any kind of cost-benefit analysis. You don‚Äôt avoid uninterpretable models by relying on p-values from linear models, you avoid uninterpretable models by fitting *simpler* models. Linear models are great for this, but not because they ‚Äúhave p-values‚Äù. They‚Äôre great because you can convert the *effect sizes* into units that anyone with a basic math education can understand.

So far, all the examples people have given me of the usefulness of p-values have been cases where the effect sizes should have been used.. I get it. But some things are too serious to joke about :). This is correct. P value - put incredibly simply - is just the chance that an observation was by happenstance. As a data scientist its on you to decide what percent chance you are comfortable with - .05 is just a general guideline and is certainly not a hard and fast rule. People who are new to statistics tend to fixate on 0.05 as a rule when its not.

Edit: Still find this meme funny though.. To some extent I agree, I‚Äôm a Bayesian and don‚Äôt really ascribe to NHST frameworks. 

But, if you are using p-values, you need to remember what the cutoff threshold is for. Controlling your error rate. If you treat it as a continuous clue, you‚Äôre going to end up with an unknown error rate that fluctuates. AKA you won‚Äôt replicate findings at an expected rate. I disagree with this proff with p-values, but agree with his sentiment.. If the test is like 'what design works best' then you go with whatever direction the person or team with the biggest stake in the project wants to go. Like there is room for discussion on using .05 as the defining point for something that isn't 'will this drug save lives or cause explosive shits'.. i get it; it's not my type either.. Kinda yeah, did I miss anything? Lemme catch up. [deleted]. Yes that was indeed my point. Thank you for rephrasing.. Checkout the paper "Mindless Statistics" for a fun and comprehensive discussion on the matter. Also, no guidelines on how to pick the right number.. This kind of behaviour is never tolerated in Boraqua,

P hackers, we have a special jail for p hackers.

You are fudging data? right to jail.

throwing ML at all your problems? Right to jail. Right away.

resampling until you get a statistically significant conclusion? jail.

testing only once? jail.

Saying you can solve every business problem with only statistics? You right to jail.

You use a p value that is too small, believe it or not - jail.

You use a p value that is too large? Also jail. Over sig under sig.

You have a presentation to the business and you speak only in nerd and don't use charts? Believe it or not jail, right away.. p value set to 1/20?

Floating point error. That's right, jail.. [deleted]. Well said homie. That simulated binomial distribution under your fingernails is calling you a liar!. Impute the mean!. Would changing the cutoff to say 0.0005 be a reasonable method to avoid detecting minor effects? As you said though, the effect size is what we should be looking at first anyways.. Design of experiment. As for your question, anything involving ANOVA which is at the core of DoE.. Also have a look at p hacking.. You're welcome, stay curious.. Ending misuse of p < 0.05 wouldn't entail valuing p > 0.05. There's no reason to desire a larger type II error rate (chance of rejecting the null when you shouldn't have).

I don't know every case against significance testing, but the cases I've heard against it are incidental or involve machine learning and distance measures being better:

1. 0.05 still leaves 5% chance of rejecting the null in error. That's not 0%, so someone could always beg for more research, and now the implementation of your conclusions is put on hold.
2. Null hypothesis rejection is really complicated, and many people without the training can misapply it. If you're tracking multiple KPIs, you have to adjust your alpha (and the adjustment rule is just a rule of thumb). If you "peek" while the experiment is running, you have to adjust your alpha. Easy for novices to miss those.
3. Hypothesis testing relies on assumptions that can't easily be verified in reality. Especially when the variables you're testing are continuous. You have to assume the population you're studying follows a normal distribution. That's called into question sort of like how "Homo Oeconomicus" is called into question in the Economics space. I think binomial variables are a little safer to test for significance, on the other hand. You can derive variance for those rather than having to measure or assume it.
4. Machine learning is providing other ways to brute force comparisons among groups.. Goodness of fit tests. A high p-value suggests may suggest model adequacy. So if you had a small p-value for a goodness of fit test, you might need to adjust the model.. Yes and hypothesis testing is used to determine the statistical significance of the measured effect.. >You don‚Äôt avoid uninterpretable models by relying on p-values from linear models, you avoid uninterpretable models by fitting simpler models.

&#x200B;

Yes, that's why I said we were left with GLMs. You're misinterpreting me; I said we were using GLMS *and* p-values, as in, anything that relies on a specified family of distribution. The regulating body wants to know if the population is stable? They won't accept anything other than a Chi-Squared test aka p-values because they're SAS-using dinosaurs.

&#x200B;

> Linear models are great for this, but not because they ‚Äúhave p-values‚Äù. They‚Äôre great because you can convert the effect sizes into units that anyone with a basic math education can understand.

&#x200B;

Yes, they're great because we can tell exactly why Billy Bob didn't get his loan approved, which is kinda difficult to do with a NN or a RF.

&#x200B;

I'm not sure why you'd think I'm somehow vouching for all of this, or disagreeing with anything that you've said so far. I am not the regulating body itself, but merely someone who abides by its guideline.. do you know what the words subjective and objective mean?. Isn‚Äôt it more like the chance that a difference of the observed size could emerge by chance given that no true difference exists? So it doesn‚Äôt really say anything about the probability that what you see is random. And yeah the universal .05 stuff is really strange.. >P value - put incredibly simply - is just the chance that an observation was by happenstance. 

&#x200B;

That's just a wrong definition.... Interesting. I wonder what‚Äôs the point of running those tests at all if it‚Äôs so arbitrary.. I agree, I wouldn‚Äôt recommend pharma if you want to focus on pharmacology. But I do think its great place for those with a business/finance orientation. I mean, any big industry is good for us finance folk.. Ugh that's where I started out. Part of the grind switch from bio/clin to DS tho. oh thank god you were being sarcastic.. We have the best data scientists. Because of jail.. I'm stealing this and sharing it at work as though it were mine. Can we turn this into a poster?  If I ever have to go back into the office, I‚Äôm printing this is size 50 font, plastering it on the wall next to my desk.  I think it will cut out at least 60% of the questions I get on a daily basis. The real reason is that high energy physics experiments produce such an insane amount of analyses that using a higher p-value would lead to a rediculuous number of false discoveries.. Agreeing with Walter_Roberts that it makes more sense to interpret the effect size. If you still feel like you really need something like a p-value, you can put a 95% confidence interval around your effect sizes, but with big data the emphasis should be on precisely estimating your effect (getting narrower confidence intervals) rather than making binary decisions at arbitrary thresholds (p<.05 NHST). 

If you are building a model rather than performing a single test, you could for example use AIC or BIC metrics to help you decide which variables to include. These will give you a number which is something like indicating how much variance you've accounted for penalized by the number of variables in your model, then compare this number among different models.. this is data analytics, not data science.  
There are other ways (and more recent ones) to measure feature importance. [deleted]. That‚Äôs not how it works‚Ä¶. Gotcha, I did misinterpret what you were saying then. I completely understand doing what you gotta do for a regulatory body.. Its not wrong - when I said 'put incredibly simply' it should have indicated that im stripping out all nuance from the definition - but I should have expected someone pulling the 'welllll akshullllyyy' nonsense. 

Put slightly less simply - but still not overly nuanced  - the p-value represents the chance that the result (or any result more extreme) from an experiment, is due to chance (i.e. supporting the H0) as opposed to a true effect (i.e. supporting H1) in the data.. it's generally to pick which is best. If you allow me to pick the absolute most prime example to support why 'choosing the most statistically significant option isn't always correct'

Imagine a fashion e-commerce website of some kind. they are revamping their design. they narrow it down to two designs. The stats nerds conclude that design A raises the median size of the cart by X% and design B falls short of .05 but had it cleared it, then the nerds would also conclude that it raises prices by X%.

Well design B, from an aesthetic / design perspective is more in line with the desired "aesthetic" of the company. Maybe it's using colors that match the brand logo, or the company is about simplicity so it's an minimalistic interface idk. Anyways, the company is gonna *should* with B. Because there is something to be said about a cohesive brand image that isn't captured in statistical significance testing.

Maybe the company doesn't make as much money with design B instead of A. But a company that understands it's identity and communicates that identity will, all things equal, do better than a company that doesnt.. It‚Äôs not arbitrary. 0.05 value is 2 standard deviations for a normal distribution.. Yeah I love data science and the wisdom to which it leads.

But working with business leaders makes me cynical.. I hope they like it.. No offence, but you have no formal stats education, right?. I think you‚Äôre misreading the emotions. Bottom guy isn‚Äôt mad, he‚Äôs excited. Top guy is in pain.. Again, that's not correct. It's the probabilities to observe a value as extreme as you did given the null hypothesis is true. You might think it's pedantry but that's irrelevant.. Idk I work with a lot of stats nerds (joking..) and it makes me wonder why we waste the energy on so many tests that return (not statistically different) positive/neutral results. I was with you right up until the last paragraph where you say the company won‚Äôt make as much money, but that companies with coherent brand always do better. What is your definition of better if it‚Äôs not making more money?!

I guess you mean they do make more money overall in the long run by having a coherent brand, but not necessarily from this specific decision? It just reads a little funny to say that they won‚Äôt make more money but would do better!. I interpreted this as not needing to rely on .05 depending on the situation which then could make it arbitrary. I might have misinterpreted though.. Math minded people think of things differently. You're immersed in these rigors and structure that aren't inherently human. People are *bad* at stats.

It will gets better as older business people phase out. But we're gonna continue having this problem so long as companies do not put data based decision making as a core competency. And that requires all senior management to not only understand at least the core fundamentals but be a paragon for statistical / analytical thinking.

it's ironic that the way to a better maths based company is through better people / social management.. nope, learned all by myself. started in Kaggle mostly and never saw  how this kind statistics are useful.. I'm really trying to see how this formal stats can help me in my daily job. > It's the probabilities to observe a value

"...represents the chance that the result"

> as extreme as you did

"...(or any result more extreme)"

> given the null hypothesis is true.

"is due to chance (i.e. supporting the H0)"

Literally said the same thing -  you're splitting hairs that do not need to be split by pontificating over precise wording.. because the alternative is making a decision with no information or only gut information.. Yeah. The last part. You might make a brand decision that isn‚Äôt the most valuable in the short term. But the decisions in a collective of decisions around brand management can and often do provide more value than the short term financial decision.. And I was trying to see why you seem to be allergic to statistics that aren't branded as machine learning. You do you.

&#x200B;

PS: most kaggle notebooks are done by people without an education, and therefore prone to containing a lot of sketchy stuff. You'd probably be better off with actual books.. I mean, just use the proper definition next time. It's not the probability of something occurring by chance and the last thing we need on this sub is more statistically illiterate people.. Literally ~~the same thing~~ inverse conditional probabilities:

> the chance the result would occur due to chance alone (i.e., chance of observing the result given the null)

P(D|H)

> the chance the result did occur due to chance alone / is due to chance alone

P(H|D). And gut bacteria is no basis for government.. I‚Äôm sure there‚Äôs a philosophical analogy about in/out of bag prediction - but I can‚Äôt quite grasp it.[D] A Demo from 1993 of 32-year-old Yann LeCun showing off the World's first Convolutional Network for Text Recognition. nan. The fact that they also had to know the location of the numbers and that the algorithm was robust to scale changes is impressive for 1993

It's not like they just solved MNIST in 1993, it's one step above that. Every data scientist today is truly standing on the shoulders of giants.. awesome to see. TIL audio hasn‚Äôt been invented until 1994. And yet websites still think those obfuscated texts are a good test for robots. Anyone know who the other guys at the end are?. Man, these guys were the real engineers.. Actually, he was 32 years old when he pressed the button. He was 33 by the time he got the results back.. Never going to complain about not having a strong enough GPU again. Very cool.. Wonder what was the RAM and computing power of the system.. Many don‚Äôt know it, but before it was done such text recognition was considered impossible, just like AGI and other hard problems. I think text recognition in mail was the first successful real world application of AI.. MNIST irl. that was certainly more wholesome than the other historic computer vision video, [https://www.youtube.com/watch?v=8VdFf3egwfg](https://www.youtube.com/watch?v=8VdFf3egwfg). But the question is: is it the validation set? üòÅ. Very inspiring as I remember these days. Lot of hard work and at the cutting edge.. Uh.  Sorry, no.

[The CNN was invented by Hubel and Weisel in 1959, the year before Yann LeCun was born, under the name "neocognitron."](https://en.wikipedia.org/wiki/Neocognitron) 

LeCun also didn't make them first.

[The CNN was first implemented by Kunihiko Fukushima in 1979](https://search.ieice.org/bin/summary.php?id=j62-a_10_658), 14 years before this video

(Reference translated is Journal of the Institute of Electronics, Information and Communication Engineers A Vol.J62-A No.10 pp.658-665, October 25, 1979, ISSN 0373-6091)

What Yann LeCun actually brought to the party was the modern approach to training them.  He did that in 1984, not 1993.. [deleted]. Nice keeb.. u/savevideo. That is so satisfying. The first set of numbers was Yann LeCun's phone number at bell labs.. Still accurate than tesseract lol üòÇ. So why am I still doing captchas. Yann LeCun's tweet on who the other guys are, and who the cameraman is - 
https://twitter.com/ylecun/status/1347268914263306242?s=20. Better than tesseract. But still, to this date, they cannot recognize traffic lights. incredible! pay tribute to him. So why did it take 30 years to get this far?. On some comments about possible tweaks/tricks in this video:

I have had the privilege to attend professor Yann's classes at NYU. 
From whatever little I understand of him - he has high levels of integrity, and I do not see him trying some cheap tweaks and fixes...He was committed to solve a problem in the best way possible and not just for likes and hearts ‚ò∫Ô∏è.

And without high level of integrity, you can't go from lab to national level in short time. 
 
People often underestimate what it takes to be unanimously accepted as one of the godfathers of current hottest trend. This doesn't discount the effort of forefathers or future generations...
... but let's not undermine Prof's integrity and commitment by making such frivolous comments. In fact, it is only our loss, if we fail to see that.. Cant see his right hand. Outside of the CNN achievements the rest is actually impressive too, and I'm absolutely amazed that the interface is so responsive. In 1993.. I'll never understand why this didn't blow up like it should have when they succeeded in doing this. Should've been in the news all over the place for months.

AI winter my backside. What a boss!. Fukushima‚Äôs neocognitron came almost two decades earlier.. So then what took so long for it to catch on? Why did it take another 30 years if they knew the power of cnn's?. Amazing!  I‚Äôve cited Professor LeCunn multiple times and am always humbled by his work ‚Äî this is why I tell students that they are standing on the shoulders of giants when they do research.  Love this video!!!. Are you sure you‚Äôre a robot?. WOW !!! Impressive !. Where was the video shot?. I guess too many people underestimate what could be accomplished with a little and tons of passion and time. Agree - it was 6 years later until MNIST was even released.. I guess they had a preprocessing step to identify, center and scale each digit image before feeding into the neural network. It‚Äôs not that hard with feature engineering.. The video has lots of cuts, and the numbers never obscures an important part of the image...    I suspect each of those tests had tweaking and tuning to make it work.... Love how happy they look!. I was born 1982. We didn't start hearing shit until 1995. That was an absolutely wild year. It created a real musical renaissance.. Can confirm.  That's the year I got a sound card.. there is a reason why captcha is becoming obsolete. At least the text based version.

Also, captcha actually digitize books. This is why there are 2 tests, not 1. So in a sense, we were training the robots filling the captchas.. I don‚Äôt think it‚Äôs meant to filter that way. Bots usually are built with speed in mind so it recognises and fills in the blanks virtually immediately.

That and captchas are also useful for labelling training datasets manually (user input). But correct me if I‚Äôm wrong though.. It serves two purposes. It defeats 99.99% of bots, and it maps images to human inputs to train their image recognizer networks.. Unless someone cares enough about your little website to train an AI to solve your captcha they're still not a terrible idea. I don't think there are any AIs that are generic enough to solve *all* obfuscated text captchas yet.

Obviously it's not going to work for large sites but none of them use that method anymore anyway.. Am son of the guy in the chair (Rich Howard, collaborator and director of the silicon integrated circuit lab at the time). He said the guy in orange was a technician and computer whiz named Donnie Henderson.. err, Kurzweil had an OCR product in 1976: [https://en.wikipedia.org/wiki/Ray\_Kurzweil#Mid-life](https://en.wikipedia.org/wiki/Ray_Kurzweil#Mid-life). Bayesian classifiers as the first email spam filter?

Not sure the year, but our lives would be completely different if it wasn‚Äôt for it.. > Many don‚Äôt know it, but before it was done such text recognition was considered impossible

By the time LeCun did this, text recognition was common at banks for scanning checks, in children's toys, and was the basis of the Cue:CAT.

You're making this up.

OCR was common by the early 1970s, almost 30 years before this.. No. You are NOT correct about Hubel and Weisel.

Hubel and Weisel did research on visual cortex in real brains (in cats) and it was awesome (they got Nobel Prize for it). But they did not invent CNNs.

You can read their paper \[1\] you don't have to be a biologists to understand most of it. From their work one can deduce what neurons in V1 do. It was later even verified that some of these neurons realize functions similar to Gabor filters, but (as I remember) that was even later then neocognitron.

It is true that their findings did *inspire* creators of neocognitron \[2\] but that's about it.

\[1\] [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363130/pdf/jphysiol01298-0128.pdf](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363130/pdf/jphysiol01298-0128.pdf)

\[2\] Fukushima, Kunihiko, and Sei Miyake. "Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition." *Competition and cooperation in neural nets*. Springer, Berlin, Heidelberg, 1982. 267-285.. And to add to this, people thought NN's were a joke until a CNN won an image recognition contest in 2012, which is what put them on the map.  Before that they were obscure and overlooked.. Hubel and Wiesel, building upon the work of Vernon Mountcastle, analyzed the structure and organization of neurons in the visual cortex of cats.

Fukushima did not use convolutional layers or convolutional operations for the neocognitron, therefore it does not fit the description of convolutional neural network.

It does fit the description of deep learning though.. at least, read the title. ###[View link](https://redditsave.com/info?url=/r/MachineLearning/comments/kuc6tz/d_a_demo_from_1993_of_32yearold_yann_lecun/)


 --- 
 [**Info**](https://np.reddit.com/user/SaveVideo/comments/jv323v/info/)&#32;|&#32; [**Feedback**](https://np.reddit.com/message/compose/?to=Kryptonh&subject=Feedback for savevideo) &#32;|&#32; [**DMCA**](https://np.reddit.com/message/compose/?to=Kryptonh&subject=Content removal request for savevideo)&#32;|&#32;[**Donate**](https://ko-fi.com/getvideo). Yes. Back then, the proportion of developers who could hand-write a new graphics algorithm in assembler or C was considerably higher, since that was often how it was done anyway. Necessity is a great motivator. The non-ML part of this problem is more tedious than difficult.. Maybe I‚Äôm misunderstanding, but isn‚Äôt the whole point of CNNs that the location of the digits doesn‚Äôt matter?. This system ended up deployed in banks to parse written checks, so I don't think it was tweaked just for these examples, but they did expect to have fully visible digits.. I have had the privilege to attend professor Yann's classes at NYU. 
From whatever little I understand of him - he has high levels of integrity, and I do not see him trying some cheap tweaks and fixes...He was committed to solve a problem in the best way possible and not just for likes and hearts ‚ò∫Ô∏è.

And without that level of integrity, you can't go from lab to national level in short time.. [removed]. I remember when we got sound in school for the first time there was alot of realization of where smells were actually coming from that day. Imagine if The Bends was the first sound you ever heard.. Yeah, well I was born in 72 and we ate rocks for breakfast!. Lies and slander, PC speaker was readily available on PC before soundcards became a thing.. Yeah, on a PHPBB forum I manage, the bots can get through the text-based captchas very easily. But they still struggle with simple questions like "In what State is this club based?". Not anymore. Google stopped doing that a while ago. You‚Äôre definitely correct about the captchas.  
It‚Äôs no coincidence that most of the objects they ask you to recognize are cars, crosswalks etc.  
They basically get free labor to help them build a giant dataset fir training self driving cars.. what would be the problem with a little delay?. As I make my living making bots and doing automation, captcha is just part of the job. Solving captcha isn‚Äôt a special thing.. ‚ÄúI‚Äôm not a robot‚Äù - select crosswalk, identify license plates, etc. are for training self driving vehicles and finding the house address was for google maps. 

We should be paid for doing reCaptchas. However some people actually do get paid for these tasks.. That's super cool lol. Did this invention have a big impact on their career?. Correct me if I'm wrong, but doesn't normal font imply a "set font" rather than handwritten characters?

Still impressive but a different problem from MNIST and generally reading the messy writing of humans.. But Schmidhuber had already written the paper in 1962. > Bayesian classifiers as the first email spam filter?

You're off by about 9 years.  Bayesian classifiers didn't emerge as spam filters until approximately 1996.  They are currently believed to be first published by [Sahami et al in 1998](http://robotics.stanford.edu/users/sahami/papers-dir/spam.pdf).  That paper describes secretly internally using the technique in late 1996, and is the earliest known published discussion.  The internet at large caught on in 1999, just 22 years ago.

The word SPAM actually comes from IRC and MUDs; we had spam filters long, long before email had spam, thanks to terminal washes and things of that nature.  The earliest known IRC spam filter was the `anarchy eris.berkeley.edu` stripper, which didn't work well enough, and led to the split of Jakko's original network to create eris-free net (EFnet is fundamentally named for a spam host removal.)

If you count the invention of the q-line as an anti-spam strategy, then IRC invents spam filtering in 1991.  If you require message or origin testing, IRC invents it in 1992 instead.

If you're old enough, you remember when Bayesian Filtering turned spam filtering from an ongoing joke into something that actually worked.  This was one of `gmail`'s early advantages.. cuecat was a barcode scanner. Never did anything resembling text recognition. Nor were there any children's toys in the 90s or before that did anything of the sort (though they might do interesting stuff to convince *children* that they could!). And check recognition worked by "cheating" ‚Äî first, using a special typeface with super easily distinguished characters and uniform size and spacing, and second, [printing it with magnetic ink](https://en.wikipedia.org/wiki/Magnetic_ink_character_recognition) so that the scanner didn't have to find the data it wanted among any kind of visual background. Everything except the routing and account numbers was invisible to it.. > It is true that their findings did inspire creators of neocognitron [2] but that's about it.

Uh, no, they're where that name comes from.

What specific difference do you imagine exists between the neocognitron and CNNs?  They're both striding convolutions as a reduction for inputs.. I'm not sure why you believe this.  Neural networks have been a big deal since the 1950s, taking down investments of half a billion at a time from the military for 70+ years now.. Has this changed really ? :) In number of engineers with these skills, certainly, in proportion of developers, this remains to be seen. Python is the syntactic sugar but who goes really in and looks under the rug ?. The assm skill was crazy back in the day! Nowadays I wouldn't use assm even with an 8bit microcontroller because I'm too lazy.. Today's software are thousands times less efficient, because of all the overhead have been added layers on top of layes don't do any real work. Think about after all the closest, cabinets, drawers, boxes, organizers and wrappers, you still get the same pair of old socks and everyone cheers: "Yeah! It works! We got the socks!", that's what modern software actually is. But thank to these overhead, this industry have enough investment to support millions of overpaid software engineers, and most important of all, thousands of billionaires.. CNN is robust to translation but not invariant to scale and rotation. Max pooling can be used to to combine detectors that trained for different scales and rotations.. Did LeCunn make a lot of money from it?. I don't doubt that his approach works, or his scientific integrity - simply that for each demo he might have loaded a different model for example (trained for different sizes or handwritten/typed text).. This thread feels like r/KenM material. At least you were born after color was invented, back in '53.. I would struggle too. Or "What is god"?. I wish I could opt out. I don‚Äôt want to train skynet lol. It greatly reduced the rate at which a bot can do whatever. With no delay something like filling out a form could probably be done thousands of times a second, but if you introduce a 0.1s delay by requiring some model to run then suddenly the maximum rate you can automatically fill out the same form is 10 times a second.

Additionally, any more hurdles will naturally mean people need to be more sophisticated to get past them and you'll filter out a lot of the lowest effort bots.. Also running a model involves computing costs. That sounds fun. You have a site or a blog?. Rich was already close to retirement at the time, so not really. Not sure about Donnie.. Yann LeCun got the turing award for it. In the wiki page (I put it at the right chapter) they state it was supposed to be "omni-font" as in reading all types of text, while *older* systems only recognized some set fonts. Note that there were already functional devices. Of course, those probably were of much worse quality than LeCun's small CNN, I just wanted to point out the person I'm responding to is full of shit.. > Correct me if I'm wrong, but doesn't normal font imply a "set font"

1. You're wrong
1. Kurtzweil didn't invent this either
1. The work being discussed here, the CNN, is actually from the late 1950s, from before LeCun was born. **[Magnetic ink character recognition](https://en.wikipedia.org/wiki/Magnetic ink character recognition)**

Magnetic ink character recognition code, known in short as MICR code, is a character recognition technology used mainly by the banking industry to streamline the processing and clearance of cheques and other documents. MICR encoding, called the MICR line, is at the bottom of cheques and other vouchers and typically includes the document-type indicator, bank code, bank account number, cheque number, cheque amount (usually added after a cheque is presented for payment), and a control indicator. The format for the bank code and bank account number is country-specific. The technology allows MICR readers to scan and read the information directly into a data-collection device.

[^(About Me)](https://np.reddit.com/user/wikipedia_text_bot/comments/jrn2mj/about_me/) ^- [^(Opt out)](https://np.reddit.com/user/wikipedia_text_bot/comments/jrti43/opt_out_here/) ^(- OP can reply !delete to delete) ^- [^(Article of the day)](https://np.reddit.com/comments/k9hx22)

**This bot will soon be transitioning to an opt-in system. Click [here](https://np.reddit.com/user/wikipedia_text_bot/comments/ka4icp/opt_in_for_the_new_system/) to learn more and opt in. Moderators: [click here](https://np.reddit.com/user/wikipedia_text_bot/comments/ka4icp/opt_in_for_the_new_system/) to opt in a subreddit.**. NNs have definitely had a ton of research, so I agree that they weren't overlooked. However, up until 2012 they weren't very useful for most applications. Throughout the 2000s, SVMs and tree-based models (like random forests) were SOTA for most tasks. So most researchers put their focus there. 

2012 marked a transition though, as we then had the hardware support to efficiently train much larger models. This allowed NNs to become SOTA in many tasks and thus the explosion in interest. I learned it here: https://youtu.be/uXt8qF2Zzfo. In terms of what I intended to say, it's changed a lot. It wasn't an obvious career intially, so it caught a lot of people with a passion for it. The normal path for anyone who wanted visual output or realtime performance was to learn C and assembly. Operating systems were permissive, and memory mapping for access to video memory was either straightforward or documented well enough. Being able to do such things came with the job.. and if someone couldn't do it, that'd disqualify from a big chunk of the industry.

I think you may have been referring to necessity being a great motivator.. and its converse -- that lack of necessity is a great blocker. Yep, I would agree. Lots of people in ML would now struggle somewhat with these basic graphical operations, even though the preparatory learning and experience required for it is now much less.. I try to do and it is not pretty. Years of toil to make that one layer of cnn faster by inventing new winograd based algorithms. Working on the models are always more recognized.. I think that's really cynical. Memory safe languages are a gigantic benefit to society in terms of security and stability.

Such inefficiencies being permissible has allowed technology to flourish; a lot of programs would never have been written without being wasteful, see VS code vs Vim or Slack over IRC. IRC and Vim are nice cannot be mainstream and the only editor respectively. I don't see online web apps existing like Google Docs if everything had to be native speed fast. I've seen multiple homeless people with a card reader selling magazines, that's how cheap software has got over time that even homeless people have contactless.

Arguably the progression of technology isn't what I'd have wanted to see but it isn't all bad. You can't help but wonder why something is slow on your 4GHz multicore CPU at times though haha.. No, he was an employee at Bell Labs, the product and patents belonged to Bell Labs.

When AT&T spun off Lucent in 1996, the patents went that way but the computer vision researchers stayed in the remaining AT&T Labs, and they couldn't even sell or improve the product without having the rights to the patents.

LeCunn was an underdog for most of his life, the deep learning explosion only started happening around 2012 with AlexNet, when conv nets started getting all the attention.. Here's a sneak peek of /r/KenM using the [top posts](https://np.reddit.com/r/KenM/top/?sort=top&t=year) of the year!

\#1: [KenM on billionaires](https://i.redd.it/tl38stlg70g41.jpg) | [164 comments](https://np.reddit.com/r/KenM/comments/f1j7a9/kenm_on_billionaires/)  
\#2: [Ken M on conspiracy theorists](https://i.redd.it/7inbjzicewo41.jpg) | [88 comments](https://np.reddit.com/r/KenM/comments/fp01kq/ken_m_on_conspiracy_theorists/)  
\#3: [One of my favorites over the years.](https://i.imgur.com/vjhwVXg.jpg) | [139 comments](https://np.reddit.com/r/KenM/comments/hmhwzo/one_of_my_favorites_over_the_years/)

----
^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/joo7mb/blacklist_viii/). Automation can be a very secretive thing and very grey so I can't talk about projects or the details.. big if true. Yup, that exact sentence speaks about normal fonts, I referred to.. >  However, up until 2012 they weren't very useful for most applications.

At that time, they were already in use by every call center and bank on earth, were running in every copy of Windows, MacOS, and Android, had dominated speech to text for almost 20 years, et cetera.

Between Windows and MacOS, they were in over 50% of US homes.

For color, ***The US phone system started using neural networks for de-noising in 1959, bringing their use to almost 200 million people***.

.

> 2012 marked a transition though, as we then had the hardware support to efficiently train much larger models.

Respectfully, this is just kind of not true.. I'm sorry, I'm not watching a 50 minute video to try to figure out why you believe that one of the world's largest intellectual pursuits was obscure or overlooked until an image recognition contest.

My expectation is that whatever the video actually said was misunderstood.  Have a timestamp?. ‚ÄúLack of necessity is a great blocker‚Äù - I‚Äôm stealing that. \>No, he was an employee at Bell Labs, the product and patents belonged to Bell Labs.

I would just like to point out that in other countries (e.g. Germany, France Japan), inventors of a patent are entitled to a percentage of the revenue that this invention generates.

This is not the case in the US, though.. Oh yes, sorry. I think it's not a single set font, but at least several. But I also think you're right and this was made for printed fonts, so "normal" might mean "very common fonts".. I'm not saying they weren't useful. They clearly had use cases as you mentioned. 

But if you look through ML papers you can clearly see an increase in interest after 2012. And in my experience as an ML engineer, there was a similar increase in interest on the business side after 2012 as well (though often lagging behind SOTA by a few years). He says it in the beginning of the video.. I figured it's probably all printed fonts that aren't cursive or Comic Sans. You're definitely right that it's multiple, I think the limitation is just on the type of font.. > But if you look through ML papers you can clearly see an increase in interest after 2012.

ML papers still haven't caught up to their 1950s heyday, either in volume or in range.  As an issue of measurable fact, we continue to reel not just from the second AI winter, but also from the first.

No, sir, today we are not inventing Lisp or Symbolics.  

You keep saying SOTA.  This suggests to me that you're an internet fan.  Actual academics and actual industry people don't say that.

Please have a good day.. I watched the first three minutes.  I don't see anything supporting your claim, or any related evidence.  A timestamp would provide falsifiability, but you declined.

There is ample evidence that these were being used by industry for decades, taught at thousands of universities, being discussed by the United Nations.

Anyone who's ever seen Star Trek: TNG or Terminator 2 had seen them in the popular consciousness for decades at this point.

Every bank had been using them for check scanning for 20+ years at the described point.

There were more than a dozen instances where over a billion dollars was invested at a single time into the "overlooked and forgotten until an image contest" field.

Please have a nice day.. Clearly you haven't read many papers published in the last decade then. For better or worse, the term SOTA does show up in recent deep learning papers.... I've also definitely heard it used in my experience within industry as well. It's not super common, but that's a really weird thing to try to gatekeep on. The opening concept is conveyed from 00:00 to 5:22.. I'm sorry you keep ignoring the evidence and referring to wide swaths of time that do not seem to say what you claim.

Claims are concrete.  If he actually says this, you should be able to give a timestamp.  I can't find it, and doubt your interpretation.

Common sense says that even if he does say this, just looking at the contrary evidence would be enough to set him aside.  Mark Z Jacobsen is also a teacher at a prestigious university, y'know?  So is Scott Atlas.

If the evidence disagrees with an academic, believe the evidence.  I can't even find the academic saying what you claim, and it seems like you can't either.

Please have a good day.. I was in the industry before 2012.  I have first hand experience.  I remember it too.  If you will not take it from an MIT professor teaching the topic, then who will you take it from?. > I remember it too. If you will not take it from an MIT professor teaching the topic, then who will you take it from?

Actual history and evidence are fine, thanks.  I already covered this material:

> If the evidence disagrees with an academic, believe the evidence. I can't even find the academic saying what you claim, and it seems like you can't either.

In the meantime, ***this MIT professor does not actually say the thing you keep pretending he's saying***.

Feel free to look up the two names I just gave.  One is a solar crank, also an honored Stanford professor, with a habit of suing people to silence them from pointing out his mistakes.  The other is Trump's medical mess (similarly Stanford.)

Want an MIT professor?  Brian Josephsen is a dual-nobel winning MIT physicist who thinks climate change isn't real and sat in court saying cigarettes don't cause cancer.

If I can point to their extensive use in every corner of society, that is sufficient to demonstrate that they were not overlooked or forgotten.

***I'm sorry you're clinging to something a professor didn't even say.  However, until you can be specific about where he says it, you don't get to stand on his reputation at all, this way.***  Even if you did find it, the burden of evidence would simply show that he's incorrect.

.

> I was in the industry before 2012. I have first hand experience.

Pressing X.

.

The reason I keep saying "please have a good day" is that I am trying to politely end the conversation. >In the meantime, this MIT professor does not actually say the thing you keep pretending he's saying.

Here is the actual transcript from the beginning of the video:

>PATRICK WINSTON: It was in 2010, yes, that's right. It was in 2010. We were having our annual discussion about what we would dump fro 6034 in order to make room for some other stuff. And we almost killed off neural nets. That might seem strange because our heads are stuffed with neurons. If you open up your skull and pluck them all out, you don't think anymore. So it would seem that neural nets would be a fundamental and unassailable topic.

>But many of us felt that the neural models of the day weren't much in the way of faithful models of what actually goes on inside our heads. And besides that, nobody had ever made a neural net that was worth a darn for doing anything. So we almost killed it off. But then we said, well, everybody would feel cheated if they take a course in artificial intelligence, don't learn anything about neural nets, and then they'll go off and invent them themselves. And they'll waste all sorts of time. So we kept the subject in.

>Then two years later, Jeff Hinton from the University of Toronto stunned the world with some neural network he had done on recognizing and classifying pictures. And he published a paper from which I am now going to show you a couple of examples. Jeff's neural net, by the way, had 60 million parameters in it. And its purpose was to determine which of 1,000 categories best characterized a picture.

And he goes on about the topic.. Seems like you're badly misunderstanding his story.  He's talking about the MIT curriculum, not the national industry and consciousness.  No wonder you tried so hard not to be specific.

The reason I keep saying "please have a good day" is that I am trying to politely end the conversation. In 2010 the view on NNs was, "nobody had ever made a neural net that was worth a darn for doing anything."

And before you start spouting off single perceptrons and calling them neural networks, keep in mind before 2012 people didn't casually call those neural networks (Where's the network?).  It wasn't until 2012 with the CNN that people started to consider neural networks worth anything.. > > > > The reason I keep saying "please have a good day" is that I am trying to politely end the conversation
> >
> > The reason I keep saying "please have a good day" is that I am trying to politely end the conversation

The reason I keep saying "please have a good day" is that I am trying to politely end the conversation[D] The machine learning community has a toxicity problem. It is omnipresent!

**First** of all, the peer-review process is *broken*. Every fourth NeurIPS submission is put on arXiv. There are DeepMind researchers publicly going after reviewers who are criticizing their ICLR submission. On top of that, papers by well-known institutes that were put on arXiv are accepted at top conferences, despite the reviewers agreeing on rejection. In contrast, vice versa, some papers with a majority of accepts are overruled by the AC. (I don't want to call any names, just have a look the openreview page of this year's ICRL).

**Secondly,** there is a *reproducibility crisis*. Tuning hyperparameters on the test set seem to be the standard practice nowadays. Papers that do not beat the current state-of-the-art method have a zero chance of getting accepted at a good conference. As a result, hyperparameters get tuned and subtle tricks implemented to observe a gain in performance where there isn't any.

**Thirdly,** there is a *worshiping* problem. Every paper with a Stanford or DeepMind affiliation gets praised like a breakthrough. For instance, BERT has seven times more citations than ULMfit. The Google affiliation gives so much credibility and visibility to a paper. At every ICML conference, there is a crowd of people in front of every DeepMind poster, regardless of the content of the work. The same story happened with the Zoom meetings at the virtual ICLR 2020. Moreover, NeurIPS 2020 had twice as many submissions as ICML, even though both are top-tier ML conferences. Why? Why is the name "neural" praised so much? Next, Bengio, Hinton, and LeCun are truly deep learning pioneers but calling them the "godfathers" of AI is insane. It has reached the level of a cult.

**Fourthly**, the way Yann LeCun talked about biases and fairness topics was insensitive. However, the *toxicity* and backlash that he received are beyond any reasonable quantity. Getting rid of LeCun and silencing people won't solve any issue.

**Fifthly**, machine learning, and computer science in general, have a huge *diversity problem*. At our CS faculty, only 30% of undergrads and 15% of the professors are women. Going on parental leave during a PhD or post-doc usually means the end of an academic career. However, this lack of diversity is often abused as an excuse to shield certain people from any form of criticism.  Reducing every negative comment in a scientific discussion to race and gender creates a toxic environment. People are becoming afraid to engage in fear of being called a racist or sexist, which in turn reinforces the diversity problem.

**Sixthly**, moral and ethics are set *arbitrarily*. The U.S. domestic politics dominate every discussion. At this very moment, thousands of Uyghurs are put into concentration camps based on computer vision algorithms invented by this community, and nobody seems even remotely to care. Adding a "broader impact" section at the end of every people will not make this stop. There are huge shitstorms because a researcher wasn't mentioned in an article. Meanwhile, the 1-billion+ people continent of Africa is virtually excluded from any meaningful ML discussion (besides a few Indaba workshops).

**Seventhly**, there is a cut-throat publish-or-perish *mentality*. If you don't publish 5+ NeurIPS/ICML papers per year, you are a looser. Research groups have become so large that the PI does not even know the name of every PhD student anymore. Certain people submit 50+ papers per year to NeurIPS. The sole purpose of writing a paper has become to having one more NeurIPS paper in your CV. Quality is secondary; passing the peer-preview stage has become the primary objective.

**Finally**, discussions have become *disrespectful*. Schmidhuber calls Hinton a thief, Gebru calls LeCun a white supremacist, Anandkumar calls Marcus a sexist, everybody is under attack, but nothing is improved.

Albert Einstein was opposing the theory of [quantum mechanics](https://en.wikipedia.org/wiki/Albert_Einstein#Einstein's_objections_to_quantum_mechanics). Can we please stop demonizing those who do not share our exact views. We are allowed to disagree without going for the jugular. 

The moment we start silencing people because of their opinion is the moment scientific and societal progress dies. 

Best intentions, Yusuf. >Thirdly, there is a worshiping problem.

Thank you. I was going to make a meta-post on this topic, suggesting that the subreddit put a temporary moratorium on threads discussing individual personalities instead of their work‚Äîobvious exceptions for huge awards or deaths. We need to step back for a moment and consider whether the worship culture is healthy, especially when some of these people perpetuate the toxicity you're writing about above.. We actually wrote a paper regarding some of the above points. Kind of a self-criticism: https://arxiv.org/abs/1904.07633

Some other points we touched:
"lack of hypothesis" & "chronic allergy to negative results" 

And we discussed (without claiming always applicable) the possibility of results-blind peer review process.. Some of these are rampant in academia in general, what hasn't happened elsewhere is the spotlight (and $$$) that has been thrown at CS/ML in past few years.  We see what fame/fortune does to a lot of people (outside academia) we are not immune to the lesser parts of human behavior.. This is common in academia. Still worth criticisizing if it makes any difference.. Thanks for writing this. I can strongly attest the 'publish or perish' mentality. In my experience, ML researchers seem to live on an entirely different planet revolving around NeurIPS and/or CVPR. The first thing a guy I had to work with on a project asked me was the acceptance rate of the conferences I publish at. I am not even a ML researcher. Entirely ridiculous. Most of them truly have a huge superiority complex they should address.. >**Thirdly,** there is a *worshiping* problem. Every paper with a Stanford or DeepMind affiliation gets praised like a breakthrough. For instance, BERT has seven times more citations than ULMfit. The Google affiliation gives so much credibility and visibility to a paper.

I totally agree with the premise... but, I think a lot of people forget just how easy it was to load up BERT and take it for a spin. The effort the authors put into the usability of the model helped immensely.. TLDR; politics sucks. Unfortunately, you can never escape politics, no matter which field you escape to. I started doing scientific research because I imagined the system to be a fair meritocracy. It's science after all. If you don't like politics, academia is one of the worst places to be. This is the sad truth. This is not a recent phenomenon, and it's not just ML. It has always been this way. It's just more visible now because more people are new to the field and surprised that it's not what they expected.

As long as the academic system functions the way it does and is protected by gatekeepers and institutions with perverse incentives, this will never change. What can you do? Lead by example. Don't play the game and exit the system. Do independent research. Do something else. Don't be driven by your ego that tells you to compete with other academics and publish more papers. Do real stuff.

It's very difficult to reform a system from within. Reform comes when enough people decide to completely exit a system and build an alternative that has a critical mass.. Wow, this post is making me *seriously* rethink applying for an ML graduate program.. Yes this is just crazy how hard the ML community manages to clash and tear itself apart regularly. 
I follow both the physics community and the ML community and it‚Äôs quite hard to imagine physicists trash talking this hard and politicizing every aspect of their research. Ok ML has social influences but this is just ridiculous to see people pushing their political beliefs through their research ...
Concerning reproducibility and the race to publish I think it‚Äôs simply because ML is extremely competitive with regard to other fields (physics for example).. Albert Einstein was absolutely *not* opposed to quantum mechanics, by any stretch of the imagination. Saying Einstein was opposed to QM is like saying Alan Turing was against computers; Einstein was one of the founding fathers of QM.

What Einstein took issue with, was the Copenhagen interpretation of QM.  Many/most physicist working in foundational QM today share his view on that.. > papers by well-known institutes that were put on arXiv are accepted at top conferences, despite the reviewers agreeing on rejection.

Wait, can someone provide an example of this?. The focus on quantity over quality is a big one. We should be focusing on quality research instead of trying to increase our publication count. Also, the focus on just throwing more data at larger models like GPT-3 is a super bad direction for the field to be going in. Rather than actual innovation it's just larger models and more data and making things even more exclusive to the large companies and labs with 1000s of GPUs and tons of funding and resources.. > If you don't publish 5+ NeurIPS/ICML papers per year, you are a loser

No, that's not true. You're only expected to publish 5+ papers every year in your 4th / 5th year Ph.D! Before then, you're only expected to publish 2-3 papers a year, and before Ph.D as undergrad or masters you only need 1-2!. I don't think LeCun was insensitive. I think he was *painted* insensitive after the fact, but what I saw was him taking a stance, documenting it, being personally attacked without any reply to his arguments, and then dismissed with "if you aren't a black woman you have no right to talk", which is ridiculous.

What's doubly annoying is that I *wanted* to see a counterpoint to LeCun's arguments, because I wanted to learn more about what the problem is and see what it was he was missing, but the counterargument was "you aren't black so you're wrong". I left that debate thinking LeCun was right and that some people do the racial struggle a disservice by being entitled and trying to blame racism for anything they don't like to hear.. [deleted]. This stuff is almost directly related to the size of the field. I started in the speech recognition field when it was a sleepy niche field. The conferences were collegial, people knew each other and their various pet projects.

The moment speech recognition became commercially viable, the conferences drastically changed.  The big guns swooped in and entirely dominated the conferences, the papers had the same problems OP described, with little scientific value, just gaming the process to get a higher number nobody could produce.. In other words, humans bad.. >**Secondly,** there is a *reproducibility crisis*.

I am working on 3D Pose Estimation and I really feel this problem right now! There aren't that many datasets and most papers use the dataset "Human3.6M". Its large, but also very specific. So many projects tweak the "postprocessing" so that they account the specific setup of Human3.6M ... and so my results on "free living samples" are worse.. Money and fame.

Almost all of what you describe comes from newer people who want fame (cite me!) more than advances in science. It's because with the (somewhat justified) hype around ML in the industry, fame turns you into a millionaire.

Just wait until there is no longer money falling from the sky in this field, and all those toxic persons will simply vanish like a gradient in an MLP too deep. With them, the factual problems with reviews and reproducibility will also vanish, and things will be enjoyable and rigorous again.. Forgive me for being new. But what is this obsession with releasing new papers? Is papers seen as some way to get a salary or something? If you really wanted to do AI research, would it not be better to be payed by a private company?. You are correct, but it's not a problem for ML specifically ,it's a general problem. We are living in strange days, where it's not about what you do/publish, but with whom you are associated. We have an inflation of paper submissions, because we use it as an KPI. We have diversity issues, because we involving color, gender in our criteria to form a team. It's not about who you are, it's about what sex, color or whatever you have. We need a diversity of mindset, not of biological features. Saying you don't consider race as a criteria, makes you a racist. Insane.. I‚Äôm really disappointed with how Anandkumar acts on Twitter. For example, [she said ‚Äúyou are an idiot‚Äù to a ~~high school student~~ young researcher](https://twitter.com/carlesgelada/status/1248693492039053312?s=21) for suggesting that we only teach about neural nets in ML classes. 

She deleted the reply but then [tweeted out another response](https://twitter.com/animaanandkumar/status/1248332790090756096?s=21), again referring to the original tweet as ‚Äúidiocy‚Äù. 

How someone can do things like this and be a director at Nvidia and have 30k followers is beyond me.

Edit: Apparently he isn‚Äôt a high school student, sorry for the mistake. My point was mainly that public figures shouldn't make personal attacks on young researchers, or anybody for that matter. 

To put it another way: imagine if a white male researcher called a young female researcher an idiot on a public forum. Many (including myself) would find that to be unacceptable. Yet Anand seems to have gotten away with it here.. >The moment we start silencing people because of their opinion is the moment scientific and societal progress dies.

"Science progresses one funeral at a time"

https://en.m.wikipedia.org/wiki/Planck%27s_principle. I hope your comments about the broad, chilling social impact of this work don‚Äôt go unnoticed. Thanks for writing this up. Many of these problems exist across all academia though. The big underlying problems are our ancient, outdated ways of communicating scientific findings (separate manuscripts and prose that can only be updated by completing a new project) and the way we do scientific quality checks (an, in practice random selection of 2-3 community peer reviewers). Also, a belief in an only recently established incentive system (number of completed projects written up in manuscripts) that might increase the overall amount of completed projects, but is often to the detriment of quality and increases the amount of shoddy research and researchers in the system.

The first two problems only exist because submitting papers to peer review was the best that could exist before the digital age. The system has just not been adapted to the digital age yet because people who currently have most power did not have their formative years in this age, and either don't realise its possibilities or are dissatisfied by the ancient ways too, but know that substantial changes are better left to the new generation.

It is in the hands of the current, new generation of scientists to change the scientific system for the better, and move it to the digital age. We all realise its problems and don't have to submit to problematic practices thats improvements are overdue.. i will never voice my opinions in academia because i don't want to risk being cancelled. but i agree with majority of this post.. I totally agree with 99% of your stuff. All of them are great points.

Although I will contest one of these points:

> machine learning, and computer science in general, have a huge diversity problem

I will say, in my experience, I did not find it to be particularly exclusionary.                      
(I still agree on making the culture healthier and more welcoming for all people, but won't call it a huge diversity problem, that is any different from what plagues other fields)              
I also think it has very little to do with those in CS or intentional rejection of minorities/women by CS as a field.

Far fewer women and minorities enroll in  CS, so it is more of a highschool problem than anything. If anything, CS tries really really hard to hire and attract under represented groups into the fold. That it fails, does not necessarily mean it is exclusionary. Many other social factors tend to be at play behind cohort statistics. An ML person knows that better than anyone.               

There is a huge push towards hiring black and latino people and women as well. Far more than any other STEM field. Anyone who has gone to GHC knows how much money is spent on trying to make CS look attractive to women. ( I support both initiatives, but I do think enough is being done) 


A few anecdotes from the hackernews thread the other day, as to greater social reasons for women not joining tech.

Sample 1:

> There's one other possible, additional reason.
I recently asked a 17-year-old high school senior who is heading to college what she's planning to study, and she said it would be mathematics, biomedical engineering, or some other kind of engineering. She's self-motivated -- says she will be studying multi-variate calculus, PDEs, and abstract algebra on her own this summer. She maxed out her high school math curriculum, which included linear algebra as an elective.

>Naturally, I asked her about computer science, and she said something like this (paraphrasing):

> "The kids who love computers at my high school seem to be able to spend their entire day focusing on a computer screen, even on weekends. I cannot do that. And those kids are mostly boys whose social behavior is a little bit on the spectrum."

> While I don't fully agree with her perspective, it makes me wonder how many other talented people shun the field for similar reasons.

Sample2: 

> My niece had almost the exact same opinion despite having multiple family members who didn't fit that description, including her mother! It wasn't until I introduced her to some of my younger female co-workers that she committed to being a CS major. She's now a third generation software engineer, which has to be fairly unique.

> I've talked to her about it and she can't really articulate why. I'm closer to the nerd stereotype in that I'm on the computer a lot but her mother (my sister) definitely is not. I think it's mostly pop and teen culture still harboring the antisocial stigma. I'll have to talk to her some more.
There is probably some connection with video games, in that boys overwhelmingly play games where girls do not. I don't think the games cause the disparity; whatever it is that draws boys to VGs is what draws them to CS as well

You can't blame the field for being unable to fight off stigma imposed by 80-90s movies on an entire generations.

For example, there is no dearth of Indian women in CS. (I think it is similar for Chinese people too). Both societies did not undergo the collective humiliation of nerds that the US went through, and CS is considered a respectable 'high status' field, where people of any personality type can gel in. Thus, women do not face the same kind of intimidation. This is a "US high school and US culture" problem. Not a CS problem. 

> Going on parental leave during a PhD or post-doc usually means the end of an academic career. 

To be fair, this is common to almost all academic fields. CS is no exception and I strongly support the having more accommodations for female employees in this regard. 

Honestly, look at almost all "high stress, high workload" jobs and men are over-represented in almost all areas. Additionally, they tend to be a very particular kind of obsessive "work is life" kind of men. While women are discouraged form having such an unhealthy social life, men are actively pushed in this direction by society. IMO, we should not be seeking equality by pushing women to abide by male stereotypes. Maybe, if CS became a little better for everyone, it would benefit all kinds of people who are seeking healthier lives, men and women alike. This actually flows quite well into your next point of "cut-throat publish-or-perish mentality".. >In contrast, vice versa, some papers with a majority of accepts are overruled by the AC. (I don't want to call any names, just have a look the openreview page of this year's ICRL).

I agree with a lot of what you are saying, but I think this point is a bit unfair. I've encountered situations where 2/3 of the reviews are glowing, but there are pervasive, major errors in the mathematical descriptions of things. The paper doesn't make sense.

I think there are serious issues with getting enough competent reviewers to deal with the deluge of ML papers being submitted right now and that many reviewers, including well qualified ones, are not putting enough time into reviews.

For me to do a thoughtful review (I've been reviewing for NeurIPS, ICML, AISTATS for 6 years) takes me *at least* 5 hours per paper. I see people saying that they spend <2 hours per review. The following is a particularly egregious example of this, a professor at a world-class university *starting his reviews 2 days after the deadline*:

[https://imgur.com/a/hfUIhZz](https://imgur.com/a/hfUIhZz)

Because of this its becoming more crucial for the ACs and meta-reviewers themselves to make  judgement calls on papers' worthiness and cannot rely so much on the reviewers.

e:formatting. Btw, this race to publish strongly encourages publication with few experimental soundness and that don't improve on nothing but rather are just telling a story that is sound ( unfortunately sound stories rarely are able to justify deep learning successes ). Then verify it by few experiments obviously discarding any of them that would disprove the initial claim ... I feel like I spent one year reading such papers to realize the field I'm working on has not advanced an inch ... Then you obviously see papers like 'reality checks' to denounce that, but still more useless paper are coming out every day.. I‚Äôd add (your post being an example, no offence):

**Eighthly**: an under appreciation of the importance of statistics. As we know there‚Äôs the CS side and statistics side of ML. The former of which are notoriously dismissive of the importance of the latter. To the point that statistics has almost become a loaded term in the mind of many from the CS side. I myself have had discussions with people here who have literally said that any knowledge of statistics is entirely useless in ML. So let‚Äôs remove the word statistics and focus on (some of) the important aspects that having a strong understanding/appreciation of statistics provides, such as the ability/realisation that understanding the subtle assumptions made in the technique(s) developed are crucially vital. 

Ok some times taking a pragmatic approach rather than tying yourself in knots worrying about inherent assumptions in your technique can speed progress, but it‚Äôs also vital in understanding the limitations of your technique and where it will breakdown - not only from an algorithmic/numerical standpoint, but from a reproducibility standpoint. I‚Äôd argue this is an important causative factor in why your second point exists.. > discussions have become disrespectful. Schmidhuber calls Hinton a thief, Gebru calls LeCun a white supremacist, Anandkumar calls Marcus a sexist, everybody is under attack, but nothing is improved.

Yoshua Bengio is the liberal Canadian knight that will deliver this community.. On point no.6, *moral and ethics*:

In 2019, [Yoshua Bengio tried to promote a new set of guidelines](https://www.nature.com/articles/d41586-019-00505-2 ) developed by a group of not only AI experts but also ethics experts. **You can read the declaration [here](https://www.montrealdeclaration-responsibleai.com/the-declaration)**

Unfortunately, adhering to these principles is still entirely voluntary and it hasn‚Äôt caught on. **You can see the limited list of organizations who have already signed [here](https://www.declarationmontreal-iaresponsable.com/signataires).**

Ignoring the fact there is no clear framework for holding the adhering organizations accountable, it would have been nice to see the community at least adhering on principle.

Edit: As a constructive actionable item, you can still sign the declaration as an individual practitioner, or you could advocate for the organization you work for to sign it.. https://twitter.com/adjiboussodieng/status/1277599545996779521?s=19
Another instance of accusation of misogyny and racism without any basis. Could have just asked about not citing without accusations and playing victim.. > Secondly, there is a reproducibility crisis. Tuning hyperparameters on the test set seem to be the standard practice nowadays. Papers that do not beat the current state-of-the-art method have a zero chance of getting accepted at a good conference. As a result, hyperparameters get tuned and subtle tricks implemented to observe a gain in performance where there isn't any.

PPO Anyone?

> Secondly, there is a reproducibility crisis. Tuning hyperparameters on the test set seem to be the standard practice nowadays. Papers that do not beat the current state-of-the-art method have a zero chance of getting accepted at a good conference. As a result, hyperparameters get tuned and subtle tricks implemented to observe a gain in performance where there isn't any.

> Thirdly, there is a worshiping problem. Every paper with a Stanford or DeepMind affiliation gets praised like a breakthrough. For instance, BERT has seven times more citations than ULMfit. The Google affiliation gives so much credibility and visibility to a paper. At every ICML conference, there is a crowd of people in front of every DeepMind poster, regardless of the content of the work. The same story happened with the Zoom meetings at the virtual ICLR 2020. Moreover, NeurIPS 2020 had twice as many submissions as ICML, even though both are top-tier ML conferences. Why? Why is the name "neural" praised so much? Next, Bengio, Hinton, and LeCun are truly deep learning pioneers but calling them the "godfathers" of AI is insane. It has reached the level of a cult.

I don't want to point fingers but there's marginal improvement in DQN over NFQ but the former has over an order of magnitude more citations than the latter and the difference between the two is who had more compute to test stuff and more memory to store all the 10M transitions..... I strongly agree with you on the first 3 points. For point five I think you underestimate how good 30% is, in mechanical engineering only 13% of B.S. are going to women and electrical engineering is only 12%. Not to say that we are perfect, but 30% is progress. For six I think you leave out that a large portion of research is conducted in the US. So it makes sense that people would be very concerned with the US policy and ignorant of the PRC use of the technology. 

&#x200B;

If you want to discuss further feel free to DM me, I'm literally always down to talk about the state our field and how some of it is a complete shit show.. Perhaps we need a new conference that gives equal merit to negative results. Makes publishing preprints that are not anonymous (and not shared by the author on twitter) and that makes some improvements with the peer-review process so it's less arbitrary. 
I feel like by focusing on merit rather than names that would alleviate some of these issues. Perhaps open discussion could be promoted/rewarded somehow also? and additionally inappropriate conduct punished in the same way. Focus on the science and the ideas not the people. Thank you for writing this. I‚Äôve been observing these things as well, and I think you‚Äôve articulated them very well. I wouldn‚Äôt be surprised if a majority of those in the ML community share much of your views.. Moral and ethics should be part of the curriculum in ML education and paper discussions. If we do not educate people then it's hard to control what any company could do for the sake of profit. I still feel disgusted to have found in a research showcase presentation a database field called IsUyghur. Apparently the subsidiary research lab in China from a silicon valley company was responsible for it. Funny that the company wanted to join people together.. [deleted]. Amen. Academia and especially the ML community have a huge vanity problem - extremely arrogant, dismissive, and even unethical. I'd love to work on a solution to all of this.. Good discussion. I'm not sure what I can do to help the problem. But I will always support any effort to suppress toxicity.. Every single issue listed here is right on the money. I am an MSc student at a top uni and although I have published a few papers in top conferences, the absolute stress and mental headache of the publish and perish mentality and the broader issues mentioned here is strongly motivating me to not pursue a PhD, although I had been set on doing so for a great while.

For the first year of my masters, I was constantly reminded that I don't yet have a published paper yet, and without it (or some amazing internal references/connections) access to good research internships are rare, and without that, goes the chance to build connections and get exposure (the deepmind, Google hype that OP mentioned) that is crucial for success deeper into PhD and beyond. It's as if every step from the day you start uni must be perfectly placed, lest you be banished to academic wilderness. It also didn't help that my work was not in neural net/CV/NLP but in game theory+ML which is more niche meaning less visibility, less interesting to industry and others, and so on. Ofc, one does not and should not do research for "visibility" or "hype" or to publish only in a handful of venues skewed toward deep learning, but unfortunately this seems like the reality of our field. A great many days I honestly felt like I part of some strange cult and wondering what the hell I'm doing here. Even after publishing papers, I didn't feel this anxiety reduce by much.

I honestly loved the work I did and the advisor and peers I worked it, who were all amazing. However, the broader setting is just deeply toxic. ML grad school feels like the cut-throat, constantly selling you and your work, virtue signalling yet indifferent mentality of industry combined with poverty wage and financial struggles of grad school.

I hope that as a community, we listen and act instead of paying lip-service, accept that negative results and failed attempts are an important part of scientific research and not every paper must be SOTA to be meaningful, realize the myriad pressures grad students are under and setting the minimum threshold of success to be k papers/year at n conferences/journal doesn't make a great researcher but rather burnout or reward-hacking, stop putting certain people on pedestals, and we critically question the merits of industry dominating academia with half of top profs/departments being in their payroll in the name of some platitude.. > However, the *toxicity* and backlash that he received are beyond any reasonable quantity

There are many vocal people in DS and tech in general who think critical theory is the only lens to examine the world through rather than it being one of many. It's a real problem and makes it next to impossible to have a conversation with these people. My guess is most of them don't even realize they are engaging in a dialectic which embraces subjective truth. Meanwhile most of us are still using our boring old objective truth to examine the world and try to form reasonable arguments.. At the root of these issues ... we've all noticed an aggressive push for "social justice" in the machine learning community. This has been organized by a small number of politically motivated activists who do not represent the community as a whole, outsiders who aren't ML experts themselves. Its impact on the community has been extremely negative. This can be seen in how LeCun was recently silenced on Twitter, or how some people are now claiming [they should get more citations because of their skin color or gender](https://twitter.com/adjiboussodieng/status/1277599545996779521?s=19).. > Fourthly, the way Yann LeCun talked about biases and fairness topics was insensitive. 

I understand why you might feel you have to say this, but it isn't true, and catering to that mindset is only going to provide a beachhead for future unreasonable backlashes. People who jumped on LeCun overplayed their hand, but they're still in the community, and will happily jump on other innocent remarks the second we let them think they've got a receptive audience for it. Saying that biased datasets cause problems is not a racist act, there are four lights.

> People are becoming afraid to engage in fear of being called a racist or sexist, which in turn reinforces the diversity problem.

Very big agree! We need to incentivize outreach and risk-taking.

> 
Secondly, there is a reproducibility crisis. Tuning hyperparameters on the test set seem to be the standard practice nowadays. Papers that do not beat the current state-of-the-art method have a zero chance of getting accepted at a good conference. As a result, hyperparameters get tuned and subtle tricks implemented to observe a gain in performance where there isn't any.

Does anyone have any suggestions on how to avoid this scenario (other than from a conference gatekeeper's perspective)? I've yet to see any. 

If Method A is innately more able to get use out of hyperparameter tuning than Method B, then in some sense the only way to get a fair comparison between them is to tune the hyperparameters on both to the utmost limit. Abstaining from hyperparameter tuning seems like it means avoiding comparisons that are fair with respect to likely applications of interest.. Hmm, I came from a chemical engineering background, and it sounds like a lot applies to my research area (nano material) as well. I think it's a general issue for academia, and a lot of it comes from the pressure for publishing papers. When the pressure is on, things like reproducibility and integerity are just out of the window. And when everybody tries to use tricks to get paper published, you'll have to do it too if you want to keep up with the performance, it's a horrible arms race.. First of all, I don't have anything to back up my opinion/impression:

As a european, a lot of these points seem like very American patterns in general to me, more than specifically ML-related issues.

That doesn't make anything you said less true, though.. The final point is very correct. Everybody became insane. It is NOT OK to insult LeCun as if he was a nazi!. Yes, a million times of yes. As a junior researcher in this field who is going to start my career as an assistant professor, I am seriously considering quitting research and just go to industry to find a job and work in peace. What is happening right now in the ML community reminds me of what happened in the SU or China in the mid of the last century. This is essentially a kind of silencing -- I don't dare to publicly (say, on Twitter) express my opinion since I know I would easily lose my current job if I do so. Look at Yann, what happened to him in the last few days is astonishing. I understand that there is systematic racism and sexism in this country, but this does NOT mean that everything should be interpreted and explained in this way. Honestly, I feel that some of them are just playing the race/sex card in order to maximize their own utility, e.g., more citations, more visibility etc. What a shame! I never see this happens in maths or theoretical physics. It's a shame that the pursuit of pure research and truth needs to surrender to political correctness..  Quite right, for the most part.   


1. There's no clear consensus for making papers publicly available while under submission. One one side, it means the research is not available while under review which kind of defeats the whole purpose of research (sharing it with everyone, and not sitting around 2-3 months). On the other hand, sharing it and making posts everywhere does compromise anonymity: even if the reviewers don't search explicitly for the paper, they 're highly likely to stumble upon it if their research lies in that area (arXiv update tweets, gs updates, RTs by people they follow, etc). I guess a straightforward solution would be to have a version of arXiv with higher anonymity, where author affiliation is revealed only after decisions (to the journal/conference to which that research is submitted) have been made. We need to think much more about this specific problem.   

2. Reproducibility is indeed an issue. I honestly don't know why we're in 2020 and machine learning papers can still get away without providing code/trained models. Evaluating the trained model (which is, in the majority of ML related papers, the result) by the reviewers via an open-source system, perhaps like a test-bed specific for applications? For instance, evaluating the robustness of a model on Imagenet. This, of course, should happen along with making code both compulsory and running it as well. This may be a problem for RL related systems, but this doesn't mean we shouldn't even try doing this for any of the submissions.   

3. Very true. For some part, it's the responsibility of organizers to not always run after the top 5-6 names, and include younger researchers to help audiences get familiar with a more diverse (and most times, interesting) set of research and ideas. For the other part, it is also up to the researchers to draw the line when they see themselves talking about the same slides at multiple venues over and over again.   

4. This specific instance is somewhat debatable. Compared to the level of backlash and toxicity women and people of color receive online is not even close to what he did. Nonetheless, the discussion could be much cleaner.   

5. I agree with the first half. I do see companies doing *something* about this, but surely not enough. Also, it's a bit sad/sketchy that most AI research labs do not openly release statistics about their gender/ethnicity distributions. "People are becoming afraid to engage in fear of being called a racist or sexist, which in turn reinforces the diversity problem. " There's a very clear difference between 'engage' and 'tone-police'. As long as you're doing the former, I don't see why you should be "afraid".  
 
6. True (but isn't this a problem with nearly every field of science? Countless animals are mutilated and experimented upon in multiple ways for things as frivolous as hair gel) I guess, for instance, people working in NLP could be more careful (or rather, simply avoid) scraping Reddit to help stop the propagation of biases/hate, etc. Major face-recognition providing companies have taken steps to help curb the potential harms of AI, and there is surely scope for more.   

7. " Certain people submit 50+ papers per year to NeurIPS." I'd think most of such people would only be remotely associated with the actual work. Most students/researchers/advisors I know who work on a research project (either via actually leading it or a substantial amount of advising) have no more than 5-6 NeurIPS submissions a year? Nevertheless, universities should be a little relaxed about such 'count' based rules.   

8. "Everybody is under attack, but nothing is improved. ". It's not like Anandkumar woke up one fine day and said "you know what? I hate LeCun". Whatever the researchers in your examples have accused others of, it has been true for the most part. I don't see how calling out someone for sexist behavior by calling them 'sexist' is disrespectful if the person being accused quite visibly is. All of these instances may not directly be tied with research or our work, but it would be greatly ignorant to pretend that we all are just machines working on science, and have no social relations or interactions with anyone. The way you interact with people, the way they interact with you: everything matters. If someone gets called out for sexist behavior and we instantly run to defend such "tags" as "disrespectful", I don't see how we can solve the problem of representation bias in this community.  


Also, kinda funny that a 'toxicity' related discussion is being started on Reddit. lol. > It has reached the level of a cult.  
  
It was always a cult. It almost feel like it was DESIGNED as a cult.. Please make this an open letter that I can sign with my real name.. Welcome to the new era of science. Everybody is right and nobody is wrong.. Hey, really well put.. This is really really good. Thank you.. Can we improve the peer-review process by scrubbing the authors names and research groups from the paper? Any conflicts of interest issues can be determined by the editor.. Is maternity leave really a career ender in your country? Got damn. Where im from, you can‚Äôt even ask an employee/applicant in a jobbinterview if they are planning on having children. It is seen as discrimination, and not a valid reason to hire/fire.. Thank you for raising those important points! 100% agree üëè. I have a different take - the internet (and arguably society as more of it has moved to the internet) has a toxicity problem, but the ML community is not particularly bad.. All interesting points , though I really struggle with your mixing of first , secondly .. firstly secondly,  or first second ... 

Sorry, my supervisor kills me for doing it , and now I am hyper sensitive to it ! üòÅ

That aside, you make some very good  points.. \>Schmidhuber calls Hinton a thief,

No doubt Hinton is a thief, the whole Toronto communities are thieves and gangsta.Hinton community cross site every stupid articles they write.. The good book on the topic that I believe is relevant to this post: [The Coddling of the American Mind: How Good Intentions and ](https://amzn.to/31Sp9UU)  
[Bad Ideas Are Setting Up a Generation for Failure](https://amzn.to/31Sp9UU)  


More people will read it during the quarantine - better :)

&#x200B;

\`\`\`  
The generation now coming of age has been taught three Great Untruths: their feelings are always right; they should avoid pain and discomfort; and they should look for faults in others and not themselves. These three Great Untruths are part of a larger philosophy that sees young people as fragile creatures who must be protected and supervised by adults. But despite the good intentions of the adults who impart them, the Great Untruths are harming kids by teaching them the opposite of ancient wisdom and the opposite of modern psychological findings on grit, growth, and antifragility. The result is rising rates of depression and anxiety, along with endless stories of college campuses torn apart by moralistic divisions and mutual recriminations.  


This is a book about how we got here. First Amendment expert Greg Lukianoff and social psychologist Jonathan Haidt take us on a tour of the social trends stretching back to the 1980s that have produced the confusion and conflict on campus today, including the loss of unsupervised play time and the birth of social media, all during a time of rising political polarization.  


This is a book about how to fix the mess. The culture of ‚Äúsafety‚Äù and its intolerance of opposing viewpoints has left many young people anxious and unprepared for adult life, with devastating consequences for them, for their parents, for the companies that will soon hire them, and for a democracy that is already pushed to the brink of violence over its growing political divisions. Lukianoff and Haidt offer a comprehensive set of reforms that will strengthen young people and institutions, allowing us all to reap the benefits of diversity, including viewpoint diversity.  


This is a book for anyone who is confused by what‚Äôs happening on college campuses today, or has children, or is concerned about the growing inability of Americans to live and work and cooperate across party lines.  
\`\`\`. [deleted]. Well, I found some statements here are actually incorrect or superficial. For example, you cannot simply draw a conclusion based on a single BERT paper without much context, and do not consider a lot of confounding factors (e.g. its results are much better than others). If you just want to reason by a single example, why not look at the two concurrent papers of VAE, [one](https://arxiv.org/abs/1312.6114) from Universiteit van Amsterdam which is cited \~10K times, [the other](https://arxiv.org/abs/1401.4082) from Deepmind which is cited <3K. Can you draw an opposite conclusion from this?. Can someone point me towards anyone wanting LeCun to get off twitter? Or to anyone (other than the guy that said "fuck Yann LeCun" or something like that) attacking him? To me he overreacted wildly and Timnit didn't quit twitter before being far more harassed by his fanboys. [deleted]. Standard 90/10 split 


90% Indians watching **MACHINE LEARNING AI SELF TAUGHT ENTERPRENEUR** YouTube videos.

10% actually working and studying the field with a technical understand above surface level. 

And it‚Äôs no surprise which group is louder and drowns out any actual worthwhile discussions. \> Gebru calls LeCun a white supremacist

Did that actually occur? I tried to follow but don't recall that o\_O. I think the sixth point you made here is so insanely important and undervalued. I do a fair amount of researching disinformation, in particular deep fakes, and the fact that this kind of technology came from academia without any real thought about the danger it could represent is appalling. Facial recognition and other tracking types of technology fall into this same category. I understand they are cool problems and the machine learning technology behind this is truly amazing, but there has to be some kind of moral check.. Thanks for an excellent and needed post. Yes, much of this is in common with academia generally, as many are pointing out. Check out the blog post ‚ÄúUpgrade Your Cargo Cult‚Äù by David Chapman for an excellent and uniquely well-informed take on this issue. He discusses how any scientific field is presently marred by bad science, partly because the sciences are mostly going through rote procedural motions while missing the vital other ingredient that makes science work. So instead of science we end up with something more procedural, which marries nicely with market demands ‚Äî so the result is an overwhelming emphasis on engineering rather than science. ML is a chief example of this. People aren‚Äôt left to explore the territory properly because they‚Äôre pressured to just engineer useful results with no concern as to how they arrived at such results. It‚Äôs a muddling of the research and development ends of the spectrum ‚Äî a muddling which academia is doing a worse job at managing than it seemed to in the 20th century. Is an ‚ÄòML researcher‚Äô really a researcher or just an engineer with more academic qualification?. [removed]. Couldn't agree more. This desperately needed to be said.

Edit: On point  six,David Ha, Joe Redmon and I deeply care about this issue. But, yes, more of the community needs to care about China's abuse of power.. - Points 1, 2 and 7: we need open science.
- Points 3: ignore the churches and churchgoers.
- Point 4: ignore TMZ.
- Point 8: ignore twitter.. I would suggest the following to solve some of the issues.

a)Community moderation on arxiv : We have upvotes, downvotes, comments, and ranking by hot, top, controversial on reddit and mods. This to a large extent enables reddit to be a place where you can voice your thoughts but someone can step in if a situation arises. I remember there was this huge backlash on a recent paper that talked about face detection to identify criminal behaviour. The authors were kind enough to retract their submission. Imagine if they had posted it on arxiv, was there anything anyone could have done about it? 

b)Set guidelines for arxiv: Imagine you get to review a paper but find out that it is from Geoff Hinton, or Yann LeCun.. would you be able to review it in an unbiased fashion? Maybe the authors could upload a blinded submission to arxiv and reveal the names once a) they decide to stop targetting a publication b) the draft gets accepted.

c)Make Codes mandatory: The policy of code-release being optional was largely derived from the systems community where releasing the code meant revealing a lot of properiatary IPs (standard-cell libraries cost billions to model, RTL IP licenses were what earned companies money)..however even they have started gravitating towards open-source (if anyone is interested RISCV, tiny compiler by Austin Henley, JOS by MIT are great starting points) however, AI has started to go the other way, fortunately there are voices speaking out against it.

d) Make ethics compulsory: There is this famous quote by Oppenheimer after they invented the Atom bomb: "I am become death, the destroyer of worlds." AI researchers need to understand this quote applies a lot to them. 
The Atom Bomb killed around 126K people (lowest estimate) in a matter of minutes..Prior to that, if someone had to kill around 126K people, they would need an army that was at least twice that size and would need to fight for at least 20 months (US lost around 6,600 people a month during the war). 
Similarly, research that took around months/years can now be done in minutes/days. This is a tremendous amount of power and people who wield it can shape our future. It is thus important to focus on the "ethics" of AI rather than look at pure accuracy numbers.

e) Better metrics: Increasingly there are models that are able to beat SOTA due to their sheer size. Take BERT for example, Do you think colleges in Africa, Asia would be able to afford the compute costs? How about we rank models based on cost (in terms of power consumed, in terms of money ) and not just based on accuracy?. 

f) While I might disagree with "some" of the language used by Gebru. She has a point. In an increasingly competitive world, if we choose not to stand up for those who do not have a voice, we are choosing to ignore their views and are complicit in silencing them. PhDs are toxic and cutthroat and AI research is even more so. My girlfriend was forced to walk out of a project for speaking out against harassment because the harasser was "intelligent". If people like Gebru are silenced, people like my girlfriend are the ones who will have to pay the price. I would highly recommend watching the documentary called "disclosure" on netflix to understand the consequences of ignoring someone's perspective. If Gebru hadn't spoken out against racism and the danger of facial recognition algorithms, we would still be having companies like clearview.ai mining our data for surveillance. 

g) Understand privilege: This is something ALL AI (and Security) researchers need to understand. If you are a researcher publishing one or two papers in AI (or Security), you have some degree of privilege. Think about what you need to know to be a decent AI researcher today: A fair deal of programming, linear algebra, probability, good vocabulary, free time to keep up with deluge of papers in your field, a good peer group to discuss and brainstorm ideas, and finally resources to conduct experiments. ALL of this is privilege. So when someone is trying to point out an issue, maybe we can listen.. and yes, sometimes the issue may not be presented correctly or the person might use language that we cannot stomach. But the question we must ask ourselves is "What are we losing by just listening to the other person?".. [deleted]. Hit so many nails on the head it sounded like a jack hammer.. I've been reading so many insane things on the internet today. Your post made the tension in my belly finally relax.  I like you.. I appreciate the directness of your points and I shall try (and, inevitably, fail) to emulate that in my response:

> **First** of all, the peer-review process is *broken*.  

It's the peer review system that is broken, it's just a disconnect between the traditional methods for publishing work and the way people actually share ideas and results. Traditional publishing is dead - it has been since the internet, and the final nail in the coffin was social media. Unfortunately, most of academic science has yet to move towards a good alternative. arXiv is one such glimmer on the horizon - instead of going through a slow month or year long process to share your ideas and findings - just self publish and truly allow your work to be judged by your peers (all of them). If it's true only a fourth of NeuIPS submissions are uploaded to arXiv then I am sad it's not far more. 

We need to integrate the countless new mediums of communication and visualization into how we share science. So far, it's mostly the large companies (OpenAI, Deepmind, etc) that present their work with blog posts including multimedia and even interactive visualizations. Instead of condensing everything down to an eight page static paper - we should be encouraging submissions of full multimedia websites - ideally built on a common framework to make powerful visualization tools available to everyone, automatically generate printable versions for the old timers, to enable reviewers to respond/discuss directly on the page (a la OpenReviews) and, if accepted, to base the acceptance on the submission hash to identify tampering and recognize later changes). I am not advocating eliminating the peer-review system, I want make the whole process as transparent and dynamic as possible, and integrate the newest tools and media available.

> **Secondly,** there is a *reproducibility crisis* 

I agree, that's why we need a fundamentally new publishing platform where we can integrate/share code, data, and models directly (rather than occasionally linking to a disparate github repo). Ideally, the framework would have some compute behind it (maybe like Google Colab) so that all the models and code submitted can be run directly in the conference/journal submission page - imagine that: reviewers being able to interactively test people's models rather than going off of nothing but cherry-picked samples.

> **Thirdly,** there is a *worshiping* problem. 

That's probably true, although I can't speak too much about it, as I'm not very involved in the politics of academic research. That being said, all enterprises will inevitably involve some icky politics and favoritism. The best we can do to combat that is make things as transparent as possible.

> **Fourthly**, ... *toxicity*

 This problem goes far being ML/AI research - it has pretty much pervaded throughout all of public discourse at this point. We can discuss the problem of "toxicity" in general, which I would chalk up to our culture having not quite come to terms with the fundamentally different way we have to process information in the information age. However, overall I think AI research (and science in general) does a better job than most areas on that front.

> **Fifthly**, ... a huge *diversity problem* 

I completely agree, and there are plenty of arguments for why we and all of academia has a diversity problem. You are probably familiar with most, and we don't have to get into them, but suffice to say, once again, our cultural and traditional biases and institutions conflict with a more contemporary mentality. What do we do about it? Outreach and transparency - they are slow but they work.

> **Sixthly**, moral and ethics are set *arbitrarily* 

The problem here is a little unclear to me? Is it that people use technology in ways other people don't like? That seems inevitable. Is it that Western culture undervalues the rest of the world? What else is new? Don't get dragged down with the American Exceptionalists in denial as the US heads for economic and social stagnation and decline.

> **Seventhly**, there is a cut-throat publish-or-perish *mentality*.  

Coming from physics research, I agree that the AI field has a dangerously strong publish-or-perish mentality. However, that also means the field is highly dynamic and garners lots of interest/funding. I'm not convinced that a field as closely intertwined with engineering and the private sector does not actually benefit from a shorter project cycle. Additionally, the barrier to entry is virtually non-existent (unlike most other sciences where researchers won't give you the time of day if you don't already have a PhD, and the equipment/expertise necessary for making progress precludes anyone outside of 2-3 groups on Earth from publishing on your topic).

> **Finally**, discussions have become *disrespectful*. 

Again, that's really just a misunderstanding for the way information works in the information age. Attention is a commodity and insulting people still has a high rate of return. This will change for the better as we get a handle on how to process information in this brave new world (especially in informal settings like social media).

Thanks for the points though - it does us well to think critically about not just the "what" in research but also the "how" and "why".. Good points about ecerything except the racial diversity qouta. This post is a grab-bag of unrelated, tired (even if valid) complaints about the field.

>First of all, the peer-review process is broken.

First of all, what does this have to do with toxicity? 

>Every fourth NeurIPS submission is put on arXiv. 

The fact that papers are going up on arXiv is a good thing. The fact that peer-review suffers as a result is bad, and it has been raised and discussed many times but no one yet has a solution. The fact is that we don't currently have a system that both allows for fast dissemination of research and a blind review process. That it has not been fixed is not for the lack of attention or trying.

>Secondly, there is a reproducibility crisis.

Secondly, what does this have to do with toxicity? 

>Papers that do not beat the current state-of-the-art method have a zero chance of getting accepted at a good conference.

This is patently false, and one of many instances of hyperbole in this post.

People have been discussing a "reproducability crisis" in ML, but... where is it? The BERT-class models in ML have been consistently reproduced. To my knowledge the best vision models have similarly had their results reproduced too. Where there's an unreproducable result, it's either been called out and the author responds, or without a response it's taken as an unreproducable result that's ignored. The biggest reproducability problem has to do with access to data and computational resources, but that's by no means the same "reproducability crisis" in other fields.

>Thirdly, there is a worshiping problem. Every paper with a Stanford or DeepMind affiliation gets praised like a breakthrough.

More hyperbole.

>BERT has seven times more citations than ULMfit

It also performs a lot better than ULMFiT. I say this as someone who thinks ULMFiT doesn't get enough spotlight in the LM->encoder sphere. ELMo also basically disappeared overnight because of BERT.

>Next, Bengio, Hinton, and LeCun are truly deep learning pioneers but calling them the "godfathers" of AI is insane. It has reached the level of a cult.

Can you explain, in concrete terms, how using an analogy of "godfather" (which I take in the meaning of a founding leading, rather than from the mafia) is "insane" and "has reached the level of a cult"? Or is that just hyperbole?

>Fourthly, the way Yann LeCun talked about biases and fairness topics was insensitive. However, the toxicity and backlash that he received are beyond any reasonable quantity.

This is the first actual mention of toxicity, and very obviously the trigger for you to rant about the field.

>Fifthly, machine learning, and computer science in general, have a huge diversity problem. 

Let may state this first, and upfront, that while this problem is not unique to ML and CS, it is still an important problem that needs to be addressed. That said, it has nothing to do with toxicity (or specifically, not in the way you're talking about. You're not, for example, talking about how toxicity makes ML less diverse, you're in fact arguing the opposite) and it sounds like just another point to pad out your list of complaints, until:

>this lack of diversity is often abused as an excuse to shield certain people from any form of criticism. Reducing every negative comment in a scientific discussion to race and gender creates a toxic environment. People are becoming afraid to engage in fear of being called a racist or sexist, which in turn reinforces the diversity problem.

I have no idea what on earth you are talking about, or where you are getting into these sort of discussions.

>Sixthly, moral and ethics are set arbitrarily. The U.S. domestic politics dominate every discussion. At this very moment, thousands of Uyghurs are put into concentration camps based on computer vision algorithms invented by this community, and nobody seems even remotely to care. 

You've just described... the Internet in general. Look at the front page of Reddit: it is just as dominated by US politics. Same for Twitter trending. 

>Seventhly, there is a cut-throat publish-or-perish mentality. If you don't publish 5+ NeurIPS/ICML papers per year, you are a looser.

I have never seen someone publicly called a loser for not publishing sufficiently. I'm sure it's happened in specific groups, but it is not generally considered acceptable by the community. That's even putting aside the hyperbole of "publish 5+ NeurIPS/ICML papers per year".

Also, what does this have to do with toxicity?

>Finally, discussions have become disrespectful. ... Gebru calls LeCun a white supremacist

Gebru never called LeCun a white supremacist. Did you distort what she said for the purpose of fanning flames of an argument? Is that the not clearest possible example of "toxicity" you are arguing against?

----

The field is not without its problems, for sure. There are many issues of accessibility, diversity, and dissemination of information that need to be addressed. Most of it has to do with how quickly the field has grown, and the institutions and even social conventions that have not yet adjusted to accommodate its new size and prominence (too many qualified students, too many papers, too many new results). A related part of it is the potential misuse of the technology that we're building and researching. And for all the negativity and online arguments that have gotten of hand, one of the best parts of the field is that a lot of this is done in public, with free communication, and a lot of genuine self-criticism. Ask almost anyone in the field and they would agree that we are not doing enough to address all of these problems, even if we don't yet agree on how we can do better.

Posting a big list of unrelated, hyperbolic complaints stemming from cherry-picked examples (How many labs have PIs who don't know all their PhD students? How often do researchers publicly go after their reviewers?) for the purpose of stirring up a big flamey debate, does nothing to help. You're picking out the worst possible examples to [mischaracterize the field and the community](https://www.reddit.com/r/MachineLearning/comments/hiv3vf/d_the_machine_learning_community_has_a_toxicity/fwiikfx/). If you wanted to have an actual discussion on toxicity, you would have focused on that rather than include a load of unrelated points to make your big rant.

Signing off your message with "Best intentions" does not excuse the rest of your post. Based on your post history I think you do have good intentions but this post is absolutely not productive.. Calm down, young one, too much drama ;)

A lot has changed over the past ~5 years, and the machine learning field really raised the bar on standards imho.

Papers are no longer behind a paywall? And there's code to go with it and results can be reproduced? And open datasets to benchmark against? Ya kidding me? 10 years ago if you took latest state of the art paper and implemented it yourself, you'd find out your performance is somehow worse. That maybe some magic values were not mentioned. Or they hand-picked test sequences. Etc.

People worship Google or Stanford? Few years back, the fashion was about publishing in Nature and Science and chasing impact factors. Either way, exceptional work gets recognized, that's the best you can do anyway. Get published on merit.

So, worried about publishing by all means, marginally pushing the envelope on state of the art and mostly just tinkering with hyper parameters until you get the result you wanted? That's just what academia has been about the past 40 years. It's an issue worth addressing, sure, but it is  not recent and not unique to any given field.

As for the rest.. about sexism, biased datasets, Twitter scandals or democratizing AI.. That's just the scandal of the day. In the end, opinions are like farts.. everyone has them, but maybe it's better to keep it to yourself.. > Thirdly, there is a worshiping problem.

i agree about the godfathers portion.

however the worship of publications from places like Google or DeepMind is unfortunately very well-founded.

if you look at most university papers, they are training over 1/100th the amount of data industry papers use (for good reason).  as a practitioner it just isn't worth your time to look for other papers unless you're chasing the last few basis points.. I used to write "j'accuse" shit like that, then decided to lower my sodium intake. I love the cliques that can form in certain fields in academia. It‚Äôs like extremely smart people that never left their high school personality behind. Main reason why I left: I couldn‚Äôt take being called an idiot and my research trash for nothing other than my institutional affiliation and my PI. Forget that shit.. Are you not contributing to the problem? Many of the threads you started on this subreddit just report on big names, rumors and other shit-stirring. Your points are not even exclusive to machine learning anyway, and could just as well apply to any other area of academia.. I‚Äôm sorry but you have now transgressed against the Twitter clique so I‚Äôd expect quite a bit of pushback on some of these points from them. 

Once the counter pushback begins that will then be considered harassment and this thread will be binned. 3..2..1.... > Albert Einstein was opposing the theory of quantum mechanics

How is this salient?. I agree with some of your points. But this part:

‚Äú**Sixthly**, moral and ethics are set *arbitrarily*. The U.S. domestic politics dominate every discussion. At this very moment, thousands of Uyghurs are put into concentration camps based on computer vision algorithms invented by this community, and nobody seems even remotely to care. ‚Äú

Seriously? So you want this community to stay out of the ongoing BLM stuff and at the same time Uyghurs is the politics that we are supposed to talk about? Aren‚Äôt you having double standards here?. Why is it that you are upset that your politics are not being spoken about?

It's machine learning not political science.

I think your points have the same smell as the part you said about demonizing those who do not share other's views. 

Imagine me being a trump supporter in the ML community.. Its the same thing man we are all just being silenced; so we can focus on the science.. Anything we‚Äôre doing right?. >People are becoming afraid to engage in fear of being called a racist  or sexist, which in turn reinforces the diversity problem.  
>  
>Gebru calls LeCun a white supremacist

Certainly you'd agree that misrepresenting an interlocutor's arguments falls pretty squarely in the disrespect category? It's part of why minorities are afraid to speak up about such issues, thus fueling the diversity crisis. There's a common perception that those who talk about racism/sexism receive acclaim and support from those in power. This has not been my experience (nor for anyone else I know). There is very little to gain from speaking up aside from the hope that the other person will treat others better in the future. In terms of what the speaker loses, well, she's already had her words misrepresented, and now runs the risk of being labeled "aggressive" and "hard to work with" in the backroom conversations that we all know run academia and industry alike.. Just chiming in, I totally disagree with everything said here.

1. Peer review is not broken, it‚Äôs just stupid. Move the conferences to invite-only and get rid of the proceedings. ArXiv is fine for publishing.
2. Only if you care about these sorts of papers. Most papers that aren‚Äôt just incremental nonsense still have robust theory.
3. Hero-worship gives researchers something to aspire to and is a good thing.
4. LeCun did nothing wrong. In fact, the particular instances isn‚Äôt even a good case of ML bias because it just shows the model prediction failing embarrassingly badly. Fixing it requires conventional improvements, not ‚Äúfairer‚Äù datasets and certainly not engineers with a different skin color.
5. Women are prejudiced against ML. That‚Äôs their bigotry, not ML‚Äôs.
6. Yes, the officially sanctioned research of the American Empire is excessively focused on the Empire. Why do you think the Emperor pays you?
7. I will agree with you on this point.. [deleted]. I have no idea how to solve most of these. And yeah, a lot of the social problems are really fucking bad. But I made a post a bit ago on an idea that I had for a new journal, as an experiment to try and solve some of the issues with reproducibility and name recognition worship. It'd be a whole thing to set up, but if I got some support from folks here, I'd be willing to go through with it.

I love Machine Learning. I love the theory and the applications. And a lot of the people are really cool. I want to do what is in my limited power to help.. I grew up wanting to be a scientist but became disillusioned by the idea when it became clear that the problems you mentioned were ubiquitous in modern science.. Most of these points apply to research in any other field as well (just replace some names) and it‚Äôs outrageous!. I'm bored while waiting for a job. If anyone has a paper they would like me to try to reproduce, please send it to me and I'll give it a shot.. This sounds like Academia in general and nowadays Society even more generally. I mean, look at how aggressive people are even here on Reddit with perfect strangers they disagree with for all sort of petty matters. Most people are tribalized, frustrated, echo-chambered and do not know how to debate rationally without starting to insult or demonize others.. Agree. I think this happens anywhere. Outside of academia, there are some ways to control bad things. Do you have any suggestions on how to resolve? Realistically. Maybe one must be content to proceed a small step at a time. But a precise and detailed proposal is needed.. About the space taken by large companies (your third point), I have to say that in my personal experience I've moved in just few years from models I could easily train on even my laptop to models that need a big infrastructure. And since I'm not in a big player team, I have to wait for my experiments in the queue of some shared supercomputer. This is deepening the gap between the research carried out at public structures and the private large companies.. Indeed, it also happens in computer vision.. Thing is there is lots of half knowledge revolved around machine learning. I include myself to it but always try to respectfully make claims or ask questions. Everyone is a data scientist these days simply because it is so overhyped. And there's lots of narcisissm and envy from both experts and beginners. I'd say this contributes a lot to this toxicity.. I come from IT and the culture of worshipping is really funny and not going to last. You're not all Einsteins, you're building on the great work as a group, hiding good work to protect discrete innovations is silly and people should be satisfied to be lucky enough to participate in an amazing day and age. I'm excited about the possibility of working with more diverse researchers too.. There is only one (two) problem with the so-called AI: a lot of money (power).. Wow where can i read more about the reproducibility crisis? Do authors later and come out and admit that they overfit to the test set or anything ?!?!?!. Tuning HPs on test set is a standard practice now: when did this happen? Am I missing something?. Can anyone send a journal paper example showing that "Tuning hyperparameters on the test set seem to be the standard practice nowadays. "?. One explanation of the ‚ÄúBERT‚Äù issue mentioned above is that we all know how popular something is and there is an avalanche effect.

If we didn‚Äôt have this information, and consumed research by reading a journal issue whose papers had varying levels of citations we might be more likely to discover diamonds in the rough. Kind of along the same lines as the problem of social media feeds as echo chambers.. not much more to say other than:

Carpe Jugulum. And above all.. everybody knows that doesn't work. Just like Reddit.. All of your points are valid and this belongingness to anything top-notch and premiere has been very toxic for me. I am right now in an okayish college but I am eager to work with top research labs in my country. Apart from bigger personality cults, researchers who have accomplished something slightly good also have their own mini-cults which is very hard to cross barrier for students like me.. One point not mentioned much in your list: how bad is plagiarism in ML/DL? Are there any movements/organisations to prevent/resolve issues related to this?. And unfortunately some Data Science teams inherit that toxicity.

Here are some some elements that can help:

* place everyone on the same level
* promote diversity
* reward inclusivity and support between teammates. There  is a huge toxicity problem in the field indeed -- credit assignment is  at the top of the list (and I am not talking about the credit assignment  problem in artificial neural networks...).

As  a scientist/researcher/professor in the field myself, I have witnessed a wide  variety of issues ranging from the unethical rejection of  papers from  conferences such as NIPS/ICML and the very broken ICLR to the  exploitation of "noise in the review system", where researchers just  keep submitting the same paper across conferences until they sample the  right set of reviewers who will accept their paper, to plagiarism (and  more commonly, "idea plagiarism").(With respect to ICLR, the  concepts behind OpenReview are good in theory, and I understand its  ideals, but it is implemented poorly in practice, in my opinion.  Ultimately, this creates what I call the "wall of shame" for papers that  have been rejected, making it difficult for graduate students and  researchers to overcome the bad reviews received -- and this is made  worse b/c the reviewers are kept anonymous and thus not held accountable  to their poor reviews).

Ultimately,  what has been created in the field in many ways is what I have called  for many years the "deep learning rat race", where accomplishments are  often just outperforming a benchmark by a percentage point or two.   Furthermore, the review process is not being held to higher standards  (often attributed to the increasing deluge of submissions that place a  tremendous burden on reviewers and conference staff), leading to  situations where some actually reject a paper and then "copy" the idea  for themselves (with no citation at minimum -- again, the "credit  assignment" problem as noted above) in their own work (and if the copier  comes from a prestigious lab, the original source/proposer gets  overshadowed since they do not have the prestige of name that comes with  Stanford or Mila, for example).

I  could go on further and add plenty of details and "war stories" to  accompany some of the issues I have raised above (and this does not even  address the many other problems pointed out in the OP's post). But, in  essence, I think that what the machine learning community, at large, really needs is a drastic "culture change" across all levels (ranging from the  newcomers to the famous/established) addressing problems that plague the  field such as "publish or perish" and "idea plagiarism" (prominent in  the famous/big labs especially) as well as reviewing quality in  conferences.I often find much better reviewing (in general, there  are exceptions) in journals as opposed to conferences, where at least  the researcher is given reasonable and useful constructive feedback that  can be used to improve the paper and address issues in the work (if  they are addressable). Conferences have, especially recently, become a  disappointment for me, more than usual, given that the reviewers will  not even read the rebuttals me and my students carefully craft to abide  by the very strong constraints on word/character limits while still  addressing issues from reviewers that are actually address clearly in  the very text of the paper (of course, this assumes reviewers read the  whole paper -- which is unlikely, given that so their plate is quite  full with many, many reviews overall). Until we induce a deep cultural  shift in the field of machine learning and truly address its "old boys'  club" like scheme (where only those coming from the prestige get their  work recognized), the field will only progress more slowly.

I  will mention though (for fellow professors that share my silent agony),  that part of this change comes from within our own labs. While it is slow  and more challenging to change our institutions, instilling a strong and  healthy culture and set of practices in one's own lab is key to  inducing the cultural shift I wish would happen across the field  globally. If you hold your students to rigor and credit assignment,  build lab comradery (starting by knowing the names of your students, at  the very minimum) and supporting your students whenever you face often  cruel and unethical rejections, and never let your own work slip due to  the many frustrations and issues from the field, I believe your lab can  contribute to a brighter future.. Re your final point: my opinion is that discrimination is indeed disrespectful. Your 5th and 6th points even mentioned that there is a huge ethics and morality problem in ML research, that certain groups of people are left out. Meanwhile calling out someone who hold discriminative views is important such that people are aware of these toxic opinions. That said accusing someone of being a thief is bizarre, but white supremacy, racism, and sexism are problems that research community indeed should consider themselves to fight against.. I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/hackernews] [The machine learning community has a toxicity problem](https://www.reddit.com/r/hackernews/comments/hm96uf/the_machine_learning_community_has_a_toxicity/)

- [/r/patient_hackernews] [The machine learning community has a toxicity problem](https://www.reddit.com/r/patient_hackernews/comments/hm9d23/the_machine_learning_community_has_a_toxicity/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*. This kind of "science" is imho for the classic Academical research field.

But this is now in the process to die. Regarding AI, most of research is done in the IT industry. Nothing the old, obsoleted and often pretentious Academic planet knows.... Re-hashing old work and claiming it as new by re-naming. 90% of the authors don't even do the literature survey right, what is the point of having 100s of people on your team?. Great post. Wow I'm fucking glad someone wrote on this issue. I just want to point out how it extends in all its ugliness to NLP publications (especially now thanks to BERT). NLP is now getting fuller and fuller with people who do not know linguistics or langauge and do not want to work on those skills whatsoever because they don't matter, and who instead simply make models that push the state-of-the-art up a notch and get published. This abuse is extremely facilitated by newly emerging ML methods. People have even gotten into the habit of hiding and shielding their codes from others who want to use or develop the code or replicate results. 

And many avoid talking about this because apparently bringing it up is 'toxic' but a blind eye is turned toward those who actually do this. 

I am incredibly sad to be in a field where I have to rush to learn patch-up skills in boot-camp style and compete on numbers rather than quality of results.. Wow! Hitting the nail on the head! Absolutely agree.. Sounds like....every academic field ever.. >However, this lack of diversity is often abused as an excuse to shield certain people from any form of criticism. Reducing every negative comment in a scientific discussion to race and gender creates a toxic environment. People are becoming afraid to engage in fear of being called a racist or sexist, which in turn reinforces the diversity problem.

It is not minorities and others responsibility to put you or your worldview at ease when deciding to call an act racist, because racism is alive and real as plenty of people have witnessed nationwide.   It is only important to discern if their comment is *invalid* or *valid*.

Ironically you say:

>The moment we start silencing people because of their opinion is the moment scientific and societal progress dies.

Yet it seems like in arguing for "best intentions" what you are advocating for them *is* silence.. I'm working on technology that will ensure that marginalized and under-represented members of the public will appear equally often in any context. I can't get into all the details, but basically it involves taking percents like 0.05 and multiplying them by 10 to get 0.50 or fifty percent. That gives us equality. ;). If Gebru sees racism and Anandkumar sees sexism, are those not opinions they should be able to discuss? Do they deserve to be silenced because their opinions are not acceptable?. Well stated. But i dont see anything that will drive meaningful change. 


It does sound like folks at deepmind and stanford are the best place to start lobbying, though.. Some of these observations are applicable to academic research as a whole, it is not just in ML/CS.. take my üèÖ. I'm gonna sound super elitist. But sadly all that you describe is par for the course whenever a discipline expands beyond the breaking point of easy availability.. This is a good post and you're right, but there's one criticism I have:

>**Sixthly**, moral and ethics are set *arbitrarily* ... At this very moment, thousands of Uyghurs are put into concentration camps based on computer vision algorithms invented by this community, and nobody seems even remotely to care.

The way I see things, there is no such thing as evil knowledge. It's all just knowledge. The techniques, in a way, are there to be discovered whether we explore them or not.

What's happening in China is horrifying and I'm sure a lot of us care. I just think you have to aim your ire in the right direction, though, which is at the people doing evil things with that knowledge, not the people uncovering the knowledge.. Nothing from preventing you from creating your own far more accessible and more openly governed peer review journal except laziness.

The issues with institutions that run conferences and journals may be real, yet for some reason the broader "academic" community around many topics not just ML would rather complain and continue to pedestool institutions and select people rather than actually pool communities together and build your new forms of more resilient and democratized credability.

You're also conflating your point by going off on politically correct tangents concerning workforce demographics with assinine assumptions. "Going on parental leave during a PhD or post-doc usually means the end of an academic career".. uhm what? Do some research before you just repeat talking points. Edward Witten took quite a few years in an entirely different field before becoming one of the most signifigant theoretical physicists quite later. The reality is that woman prefer holistic lives generally and on average personality wise would rather live with more balance than sacrifice tremendously for narrow achievement in science. It's also true that men don't do enough generally to help raise their kids, so that number may well be slightly less skewed in time, but this number is always gong to be skewed so long as signifigant biological differences between sexes. 

I don't know how reddit is so retarded that a thread like this shoots up into outerspace upvote territory with so little substance or useful insight.. Totally agree! Paper acceptance has more to do with affiliation than anything else, and the quality has dropped significantly. 

The stuff coming out of FAIR has been garbage for years. (I haven‚Äôt seen as many deepmind papers I was critical about.)

When are we going to admit we have a naked emperor on our hands and start dealing with it?. ML/AI are particularly problematic. In some respects it is closer to literature than other branches of CS. There is little to no rigor possible. There is little deep understanding of what models do. The quality of a paper is judged by its coolness. With so little objectivity possible it amplifies all the inherent political problems in academia. Combine that with companies like Google using AI/ML publications as PR, a means to give them an image as something other than a giant hoarder of all our data. Also, combine that with the deterioration of the academic student-mentor model into paper and grant factories due to changing expectations. Then you have a system that is not fair, objective, fun or particularly useful..  T O X I C I T Y. [deleted]. Hey. Do we not all think the cause of a lot of this is the amounts of money that fb and google are paying to ‚Äúresearchers‚Äù?  They‚Äôre not actually contributing to the bottom line, so they have to justify that compensation some how, and production of accepted papers seems to be how they do that.

In the rest of machine learning, we do not call it a major accomplishment and submit a paper for publication every time a model converges. We just call it a day at the office. 

But for some reason, if a neural net is involved.... You make some good points. I definitely do NOT agree with everything you said tho. Welcome to science, baby. You didn't know?. Ummm.... no, sorry. You‚Äôre someone desperately trying to make minuscule accomplishments sound like significant achievements. 

A poster is a poster. It doesn‚Äôt mean your paper got published, or that anyone thought it was significant or novel. It‚Äôs just a poster, that people may or may not look at for a few seconds on the way to a talk. Although, at 4 minutes, what substance could there be in such a talk? 

The point you‚Äôre reinforcing is that standards in this field are a joke.. Good points for the most part, but those are not all the same problem.  And most of them are not new problems.  It is probably possible to improve the structures to help to some degree.

But my own belief is that those problems will exist as long as humans rule the earth.  Sometimes when reading lists like this, I hope that actually isn't the case for too much longer.  I am optimistic about the potential for AI to succeed humans.. This whole argument required an astronomical amount of intelligence to generate a very stupid argument.


AI: ‚ÄúAdjacent to Intelligence‚Äù

Fuck that community.. Your fifth point is fucking stupid, lmfao. "However, this lack of diversity is often abused as an excuse to shield certain people from any form of criticism". Nothing to back it up. oh so NIPS and ICLR is gamed too....its my dream to get a paper in there. now feels pathetic with people getting 5 papers are feeling like shit about themselves. 100% agreed. It irks me when really interesting research by less well-known researchers that can spark great discussion is posted on this sub and there are only 1-2 comments discussing it while at the same time a post about a random tweet by an ML celebrity garners 300-500 comments.. [deleted]. I don't deny that there is a worshipping problem, but I'd like to offer yet another hypothesis for why papers from Google/DeepMind/etc are getting more attention: Trust.

With such a huge number of papers every week, it's impossible to read them all. Using pedigree is one way to filter, and while it's biased and unfair, it's not a bad one. Researchers at DeepMind are not any more talented than elsewhere, but they take on more risk. When DeepMind publishes a paper, it stakes its reputations on its validity. If the results turned out to be a fluke it would reflect badly on the whole company, leading to bad press and a loss of reputation. Thus it's likely that papers from these organizations go through a stricter "quality control" process and internal peer review before they get published.

I am guilty of this myself. I regularly read through the titles of new arXiv submissions. When I see something interesting, I look at the authors. If it's DeepMind/Google/OpenAI/etc I take a closer look. If it's a group of authors from a place I've never heard off, I stop reading. Why? Because in my mind, the latter group of authors is more likely to "make up stuff" and have their mistakes go unnoticed because they didn't go through the same internal quality control that a DeepMind paper would. There's a higher probability that I'm reading something that's just wrong. This has nothing to do with me worshipping DeepMind, I just trust its papers more due to the way the system works.

Is what I'm doing wrong? Yes, it clearly is. I shouldn't look at the authors at all. It should be about the content. But there are just too many papers and I don't want to risk wasting my time.. I would really like this and would support such a measure! I think the idea is great and you should create such a meta-post.. Programmers: "I am a strong independent human and don't need no God."

Also Programmers: "All hail [insert scientist] and [insert YouTuber] and [insert podcast guy] and [insert electronic musician], they are my gods!"

Humans are wired to worship, whether we like it or not.. Totally agree with you on this. "problem" is not a good word for this since it implies it can / should be "solved", you can't (and should not) restrain people from admiring other people. On top of this, there has been a politically motivated push to rewrite ML history, like by renaming NIPS to NeurIPS (weirdly, no one had a problem with the name until 2018) or denying that the fathers of deep learning (LeCun, Hinton, Bengio, Schmidhuber, and a few others) were all white males. As part this narrative, the role of Fei Fei Li has been reimagined as much more than it was. Her claim to fame is to have been head of a lab at a time when Stanford created the ImageNet dataset. She has not invented anything.. > chronic allergy to negative results

As someone who just finished a graduation thesis this month about a noise-attenuation neural network (autoencoder) applied to microcontrollers... My results couldn't have been more negative, quite literally, and yet I am still presenting it based on the fact that it is also worthwhile to publish negative results, fully knowing it won't have that much appreciation.

And yet, to my surprise, my negative results were celebrated by the council. I am very confident of the value my work brings to the world yet I just had this idea that people supposed to evaluate my work would just not get it when I told them that I exhausted every possibility of trying to make something work and yet it didn't and all I have to prove is "don't do what I tried because it doesn't work no matter the configuration". 

Universities and professors should dedicate more time to let students and future PhDs know that proving something doesn't work is just as important to the world as the opposite. Thankfully I think this is becoming more self-evident as time progresses.. No authors from google or deepmind. Not worth reading. 

/s. > Some other points we touched: "lack of hypothesis" & "chronic allergy to negative results"

This oh so much this. I loved the synflow paper exactly for not being this (it lays down a hypothesis, shows the results, makes a prediction and shows it pans out) but ironically all the authors in that paper where not in ML departments. So you‚Äôre saying... he plagiarized your work?  /S. Sorry for dumb question, but what would results-blind peer review look like?. The problem with negative results in this field is that they are even harder to verify than positive ones. >we are not immune to the lesser parts of human behavior

Ironically, this arrogance feels like one of ML's biggest problems.

>Some of these are rampant in academia in general, what hasn't happened elsewhere is the spotlight (and $$$) that has been thrown at CS/ML in past few years. We see what fame/fortune does to a lot of people (outside academia) we are not immune to the lesser parts of human behavior.

Just posted some data on some of the problems in academia:

Graphs of parental incomes of Harvard's student body:

[http://harvardmagazine.com/2017/01/low-income-students-harvard](http://harvardmagazine.com/2017/01/low-income-students-harvard)

[https://www.nytimes.com/interactive/projects/college-mobility/harvard-university](https://www.nytimes.com/interactive/projects/college-mobility/harvard-university)

#Who benefits from discriminatory college admissions policies?

the advantage of having a well-connected relative.

At the University of Texas at Austin, an investigation found that recommendations from state legislators and other influential people helped underqualified students gain acceptance to the school. This is the same school that had to defend its affirmative action program for racial minorities before the U.S. Supreme Court.

And those de facto advantages run deep. Beyond legacy and connections, consider good old money. ‚ÄúThe Price of Admission: How America's Ruling Class Buys Its Way into Elite Colleges ‚Äî and Who Gets Left Outside the Gates,‚Äù by Daniel Golden, details how the son of former Sen. Bill Frist was accepted at Princeton after his family donated millions of dollars.

Businessman Robert Bass gave $25 million to Stanford University, which then accepted his daughter. And Jared Kushner‚Äôs father pledged $2.5 million to Harvard University, which then accepted the student who would become Trump‚Äôs son-in-law and advisor.

Selective colleges‚Äô hunger for athletes also benefits white applicants above other groups.

Those include students whose sports are crew, fencing, squash and sailing, sports that aren‚Äôt offered at public high schools. The thousands of dollars in private training is far beyond the reach of the working class.

And once admitted, they generally under-perform, getting lower grades than other students, according to a 2016 report titled ‚ÄúTrue Merit‚Äù by the Jack Kent Cooke Foundation.

‚ÄúMoreover,‚Äù the report says, ‚Äúthe popular notion that recruited athletes tend to come from minority and indigent families turns out to be just false; at least among the highly selective institutions, the vast bulk of recruited athletes are in sports that are rarely available to low-income, particularly urban schools.‚Äù

Any investigation should be ready to find that white students are not the most put-upon group when it comes to race-based admissions policies. That title probably belongs to Asian American students who, because so many of them are stellar achievers academically, have often had to jump through higher hoops than any other students in order to gain admission.

Here's another group, less well known, that has benefited from preferential admission policies: men. There are more qualified college applications from women, who generally get higher grades and account for more than 70% of the valedictorians nationwide. Seeking to create some level of gender balance, many colleges accept a higher percentage of the applications they receive from males than from females.

http://www.latimes.com/opinion/editorials/la-ed-affirmative-action-investigation-trump-20170802-story.html

"Meritocracy":

White Americans' anti-affirmative action opinions **dramatically change**  when shown that Asian-American students would qualify more in  admissions because of their better test scores and fewer white students  would get in for just being white.

At that point, **when they believe whites will benefit from  affirmative action compared to Asian-Americans, white Americans say that  using race and affirmative action** ***should*** **be a factor and** ***is*** **fair and the right thing to do**:

>Indeed, the degree to which white people emphasized merit for college  admissions changed depending on the racial minority group, and whether  they believed test scores alone would still give them an upper hand  against a particular racial minority.As a result, the study suggests that the emphasis on merit has less  to do with people of color's abilities and more to do with how white  people strategically manage threats to their position of power from  nonwhite groups. [http://www.vox.com/2016/5/22/11704756/affirmative-action-merit](http://www.vox.com/2016/5/22/11704756/affirmative-action-merit)


Also, Asians are somehow treated as *more* privileged than white Americans:

>white applicants were three times more likely to be admitted to selective schools than Asian applicants **with the exact same academic record**. Additionally, affirmative action will not do away with *legacy admissions* that are more likely available to white applicants.

"Legacy admissions":

The majority of Asian-Americans grow up with first-generation  immigrant parents whose English (and wealth) don't give them the same  advantages as "privileged," let alone what's called "legacy"

#Stanford's acceptance rate is 5.1% ‚Ä¶ if either of your parents went to Stanford, this triples for you

In any other circumstance, this would be considered bribery. But when rich alumni do it, it‚Äôs allowed. In fact, it‚Äôs tax-subsidized.

Worse, this ‚Äúaffirmative action for the rich‚Äù is paid for by everyone else. As non-profits, these elite universities ‚Äì and their enormous, hedge fund-esque endowments ‚Äì are mostly untaxed. Both private and public universities that use legacy admissions are additionally subsidized through student aid programs, research grants, and other sources of federal and state money. In addition, as Elizabeth Stoker and Matt Bruenig explain, alumni donations to these schools are also not taxed and therefore subsidized by the general population. They write, ‚ÄúThe vast majority of parents do not benefit from the donation-legacy system. Yet these parents are forced, through the tax code, to help fund alumni donations against their own children‚Äôs chances of admission to the elite institutions they may otherwise be well qualified for.‚Äù

If legacy preference ‚Äúshows a respect for tradition,‚Äù as supporters of the practice argue, that tradition is inherited aristocracy and undeserved gains. It is fundamentally against the notion of universities as ‚Äúgreat equalizers.‚Äù

It promotes those who already have wealth and power and diminishes those who do not.

It subsidizes the wealthy to line the coffers of the richest universities.

In other words ‚Äì elite education is predominantly for the rich.

And because these institutions disproportionately serve as feeders for positions of wealth, power, and influence, they perpetuate existing social and income disparities.

Yet these schools ardently try to claim that they are instead tools for social mobility and equalization. You cannot have your cake, eat it too, and then accept its cupcakes through legacy admissions. Children of alumni already have an incredible built-in advantage merely by being the children of college graduates from elite universities. They are much more likely to grow up wealthy, get a good education, and have access to the resources and networks at the top of the social, economic, and political ladders.

Legacy admission thus gives them an added advantage on top of all of this, rewarding those who already have a leg up at the expense of those who do not have the same backgrounds. William Bowen, Martin Kurzweil, and Eugene Tobin put it more succinctly: ‚ÄúLegacy preferences serve to reproduce the high-income/high-education/white profile that is characteristic of these schools.‚Äù

Right now we have the worst of both worlds. We have a profoundly unfair system masquerading as a meritocracy. If we are going to continue to subsidize elite schools and allow them to have the outsize impact that they currently do on our national economic, political, and social institutions, we need to start to chip away at the fundamental imbalances in the system. Step one: Get rid of legacy preference in admissions.

https://www.forbes.com/sites/joshfreedman/2013/11/14/the-farce-of-meritocracy-in-elite-higher-education-why-legacy-admissions-might-be-a-good-thing/, https://blog.collegevine.com/legacy-demystified-how-the-people-you-know-affect-your-admissions-decision/, https://twitter.com/xc/status/892861426074664960. Love your username. (Not a Bernie fan personally, but I know taste when I see it.). [removed]. I agree it's common but it definitely shouldn't be the norm. It's probably a large reason why PhD students are so stressed during those 4 years.. I think it's a problem in CS academia. My wife works in education, and they have different problems (social science reproducibility & weak results). But not the same level of jockeying, machismo, broken peer review, etc... And seemingly more self aware of the problems of weak results.

Machine learning people buy their own hype, which is a big part of the problem. 

What about working in industry?. Not only this, but by most metrics, BERT showed much better results than ULMfit, in a practical sense (wider sets of results against more applicable/watched tasks, some basically-SOTA).

There is a (IMO, I would argue, appropriately) big bump in citations for 1) showing that something can work *really* well and 2) showing that it has broad applicability.. YES, a thousand times YES.

The current situation is a bad one and you can hardly expect to solve real problems with the research process of today.

I forcefully went independent after my PhD lost funding. I completely burned out and with all sorts of psychological damage -- maybe the best thing that happened to me because it got me out of hell. I can research real problems now not being pressured just to write papers, albeit it's harder without any community. Not that I had an active advisor or other staff to help.

Another thing I have a problem understanding is why such intelligent people tolerate this bullshit. It would be very easy to reform the entire research process with the skills and knowledge this community has.. [deleted]. Completely agree. Modern science is antithetical to doing actual science.. As a current PhD student I can totally relate to this! The more and more I go into my PhD the more I realized how I hate the way it works in academia. Although I really really love doing Science and i find it so exciting...!. This essay is good and helped clarify the problem of politics for me, would recommend. [Politics is the mind killer](https://www.lesswrong.com/posts/9weLK2AJ9JEt2Tt8f/politics-is-the-mind-killer)

> People go funny in the head when talking about politics. The evolutionary reasons for this are so obvious as to be worth belaboring: In the ancestral environment, politics was a matter of life and death. And sex, and wealth, and allies, and reputation . . .. I love this and would love to connect with others who aspire to also create such an ecosystem.. If you really want to understand what's going on, you should read Jordan Peterson.. There are a lot of very closely related fields that are a lot less competitive. Indeed in my department I think anyone would be way better off not being in one of the big ML groups, and working under another advisor with a smaller group (not too small though because that means the prof is hard to work with or doesn‚Äôt have enough money). My impression is that these giant groups are miserable to work in, highly competitive even within a competitive grad program, and run by senior grad students or post docs so you won‚Äôt even get to work with the ‚Äúfamous‚Äù prof, it‚Äôs just a nice line on your resume. But many advisors not in ML would be happy for their students to apply ML to their research, so there is really no need to be in one of those groups unless you feel it is really important to you. You should try to find an advisor that is willing to let you explore your interests, easy to work with, and has the time and money to support you. When you do campus visits, the most important thing is asking students in different groups how happy they are with their advisor. 

TL;DR don‚Äôt choose a famous ML advisor/at least know what you‚Äôre getting into. But work on ML anyway if it interests you.. I wouldn't let that scare you away. Working in ML is still greatly rewarding. And, I will say, most of the negatives you're seeing listed here are either limited mostly to academia (i.e. not a long-term factor if you plan to enter industry) or only really applicable to the 1% of the ML community with respect to notoriety.. Just avoid Twitter and the problem is 80% solved.. Don't hesitate, no field is perfect. Just read these kind of drama for fun and focus on your work. These problems are not solved by students anyway.. In the same boat after reading this.. The whole scenario is boggling for early career researchers. Conflicts and Politics > Science. But as others mentioned here, focus on the reason to do science and proceed ahead.. If I weren't on this reddit sub, I wouldn't have heard of half of these problems.. If this makes you rethink one of the funnest and most lucrative careers in our lifetime, you probably weren't cut out for it anyway. Critical thinking skills and independent thinking is how novel research is born.. Which part of the physics community? It's just less publicized there.. Not sure I entirely agree re physics. Physicists are opinionated as much as anyone & go pretty hard. Just browse Sabine Hossenfelder's blog as an example. Same with mathematicians, logicians, philosophers, etc.

Doesn't really make sense to compare fields like this imo.. i am working together with a few physicists in quantum devices. let me put it that way: they are surprised by how open our practices are because they would never trust their colleagues that far, let alone offer them an advantage in the form of "here take our code". I think they used the word "hostile" to describe their research environment.. Part of the reason is that ML is a new discipline. As such it doesn't have that heritage and older, more... grown-up scientists that would foster more civil discourse.. Eh, while Einstein was instrumental to QM it is certainly not any stretch of the imagination to say he considered it incomplete and very dissatisfying at the time. And while part of it was the Copenhagen interpretation, his major reservations to my understanding were to do with the major implications of QM - that uncertainty and probability were fundamental properties of the universe as opposed to a properties of an observer. Hence his attempts at formulating a Hidden Variable theory. 

The notion of hidden variables (in certain situations) were dismissed as impossible in a paper by Bell in 1964 and were thus dismissed by the community at large. Afaik, this is still the case, and in fact most researchers still *don‚Äôt* share Einstein‚Äôs views in that regard. (The Copenhagen interpretation is a different matter, but that too is/was the primary QM interpretation for Einstein‚Äôs entire life and much after it). This. I also read that Schr√∂dinger too was against the idea that an electron can be in more than one state, probablistically at a time. He proposed his hypothetical cat experiment to prove the absurdity in the Copenhagen interpretation. Ironically, it is used today to explain the probabilistic nature in QM.

I might be wrong. Read that sometime back.. [deleted]. Back in 2017, NIPS rejected a quite novel approach to language modeling that I had implemented and found quite effective. (Not my paper.) NIPS accepted essentially every NLP paper that came out of FAIR or DeepMind, even those that claimed only trivial improvements that were attributable to grid search, and those that were obviously grossly exaggerating their accomplishments. 

Reading the reviewer comments, I couldn‚Äôt help shaking the feeling that what was going on, was that the anonymous reviewers worked for the same companies and were just helping out their buddies. 

That was one of the events that led me to get out of NLP AI research.. I don't believe that just because majority of the reviewers accepted the paper(provide accept as a review, since acceptance is decided after discussion b/w AC, SAC, and reviewers) guarantees an acceptance. The confidence of reviewers and their expertise also matters also, AC is also there to supervise the process. If he/she feels that a submission is below par I find it reasonable for them to assign additional reviewers to the said paper.. That's an insane amount of papers...  


I want to believe this is sarcasm.. What is also disheartening is that future applicants such as myself who work in theory, as opposed to applications, don't stand a good chance in a unified pool.

For instance, in a field such as deep RL, where papers are practically published any time you observe "an improvement", you can't compete up with that amount of throughput. This is just my opinion.. I am sorry I can only upvote you 1 point.. You don't know what you don't know. It's OK. This is the reason for "if you aren't a \[discriminated minority\] you have no right to \[say there is no discrimination\]", which is similar, but not exactly what you said. 

From a logical perspective, one should take pause at saying something does not exist. But to say a discrimination system that wouldn't affect you does not exist is naive at best.

So LeCun was just oblivious. He then had several people try to educate him gently. IIRC, it was his initial dismissive answer to this that earned him the real heat.. He seems cool. Firstly, welcome.

Writing papers is not exclusive to academia. To cite an example described here, [the original BERT paper](https://arxiv.org/pdf/1810.04805.pdf) was written and published by Google employees.

To answer your question directly, historically (or perhaps ideally), writing papers and publishing them has been seen as a way to contribute to a collective body of knowledge, thereby advancing the state of the art. The number of papers published by an author was seen as a proxy measure for their influence on the field.

However, over the last few decades (I think? could go back further- I'm only a few decades old myself), research institutions started using that metric to measure professional performance among professors. Employers started using it to measure the bona fides of job applicants. Folks started looking at a private institutions' publishing record as a measure of legitimacy and prestige. And, unsurprisingly, this contaminated the incentive structure.

To be clear, this "publish or perish" culture is a known issue in academia more broadly, and is not restricted to our domain.. > Is papers seen as some way to get a salary or something?

This dramatically oversimplifies the issue, but yes. There is a strong correlation between the volume of output rather than quality of output, and this incentives as much publishing as possible.

> If you really wanted to do AI research, would it not be better to be payed by a private company?

You'll find that the most notable members of the ML community tend to split their time between academia and the private sector, or they are within academia yet funded by the private sector.. Papers used to be the equivalent of blog posts of the old times. Before the internet, journals and conferences were the only way to show your research to other people. If you did some cool research you had no way to "post it on Reddit" or to Arxiv.

At some point however, people started counting papers (and their citation counts) as a measure of how "good" a researcher you are. So people started slicing their research to Least Publishable Units. It became a game to win peer review.

In savvy groups, everything about paper writing is how to think like a reviewer, how to please the reviewer. This is pretty different from pleasing and satisfying someone who is already interested, like your actual readers will be who find the paper and read it by their own will.

However, that matters little for paper writing. When people care about post-publication impact, they usually make project websites, blog posts etc. The paper is still important of course, but you need to market it also through other means, release well-documented easy-to-use code etc.

Unfortunately, this type of work is less incentivized. Instead of cleaning up your code and writing an overview blog post (which perhaps nobody will read), you can churn out the next paper.

Publication and getting though peer review has become the trophy in itself, when it actually should just be a filter. The real test comes \*after\* publication. You know how each paper says "We propose ....", well, that's what it is even after publication: a proposal, that the research community may take or leave. \*That\* is the real question. Arguably, citations measure this, but most citations are in lists of \[these papers also tackled this task\] and in experimental result tables. That's not really meaningful engagement and does not mean someone took up the "proposal". It jut means your result got compared to. Sure that's not nothing, but it's not the same as being actually picked up as a method that the community now uses.

Most proposed methods never get adopted by anyone else.. Is he a high school student? His LinkedIn profile says he's a Research Scientist at OpenAI, and he has [multiple publications](https://scholar.google.ca/citations?user=piSiD-AAAAAJ&hl=en).. To be fair, this guy‚Äôs hot take was pretty stupid.. It's really sad tbh. In high school, she was a role model for many because she was doing such good work so young and reaching great heights. When she made it into academia at such a young age, there were many who were really proud of her. She was a veritable wunderkind. 

Initially it was great that she was speaking out against the culture at Amazon. It was eye opening. But from what I've heard, her crusade and going at it in public was a bad move because the company couldn't do anything without coming under fire and the advice she received from people was overwhelmingly to leave and go somewhere else. While everyone thinks these people are so cool, in the scheme of things at a big company, they are small fry. 

But now that's become part of her identity and it's exhausting. I don't know her personally but I followed her on Twitter to keep track of ML news. But all I got was random drama, and magnifying the voices of others who aren't good with machine learning but are great at using social justice topics to boost their own profiles. The most toxic thing she does is retweet every tweet that mentions her, especially in an argument. It just keeps the drama going for days. I don't get how she makes time to do actual work if she's fighting with everyone.

The thing I dislike the most is how now machine learning is politicized in the most toxic way. I've seen people in this field from all over the place and every sort of socioeconomic situation and political stripe and we all come together to do tech stuff, which has been quite uniting. Diversity at work is hard in practice honestly. But our passion for tech made us put our differences aside and focus on what we had in common, and broadened our perspectives along the way. That doesn't feel as possible anymore because of a small set of people who want to make everything an us vs them no-win situation.. Twitter is an angry, angry place. People thrive on abuse. That's why people loved watching Simon Cowell (was that his name?) on American Idol. He'd rip people to shreds. Now we get that off twitter, and ML students/researchers aren't any different than the average person.. 100% agree. She exemplifies the very toxicity she seeks to squash.

No doubt people will now want this whole thread shitcanned as it is harassing women, for giving an honest appraisal of her behaviour on Twitter. If that attitude is representative of how she acts I‚Äôd not feel safe espousing a contrarian viewpoint at Nvidia.. Completely agree to this. Shes very belligerent in any conversation. I recall somebody asking her questions about one of her papers and she somehow starts blaming this person for disparaging her work and wanted to block them.. I tried to engage her once on the merits of SpaceX as a company and she blocked me.. There are so many people who are smart and thoughtful and considerate in long form texts like blogs and podcasts but start saying whatever rubbish comes off the top of their head as soon as they start using twitter.. Well, he was right: it was controversial.. the point is shitty behavior should not discredit someone professionally if we want to isolate science from other mundane things.

if she is qualified she should remain at that position. > imagine if a white male researcher called a young female researcher an idiot on a public forum

Sure, I can easily imagine it because I've see it happen so many times before. And he'd justify it by saying, "Don't take it personally." And many people would defend it, saying things like, "She's just too sensitive," or "Let's not self-censor to protect other peoples' feelings.". "You are an idiot" is just a part of being online, IMO. I benefited a lot from people telling me that when I was a teenager. I didn't really like it at the time, but that caused personal growth. Toxicity is bad, but going into a cytokine storm in attempts to eliminate toxicity can be even worse.

I don't follow Twitter because I don't hate myself, so maybe the account is worse than this on a regular basis. In general, though, it'd probably be good if one lesson we took away from LeCun's debacle was to avoid caring so much about individual isolated tweets.. that's an amazing quote /idea.. In Italy during the rise of the fascism there were groups of people whose task was to go and menace (physically or not) those who had opposing views. That was called "squadrismo". Don't let squadrist mentality into research.. You make great points. The US seems to have a very anti science culture and people who conform to social norms aren't the ones who will go into science fields. My husband is a white guy in tech and I'm an Indian woman in tech and he always felt like the nerdiest person wherever he went before he met me and always tried to tone it down. Then he met my friends, who were moms with kids and musicians and every kind of person who all had chosen programming for a better life and his perspective just changed. 

With regards to diversity, the most diverse companies also tend to be the most chilled out, because people from underrepresented communities usually have a lot of responsibilities outside of work. And these companies don't survive very long. I've worked at a company that was heavily middle aged women, and it was great, but not having a culture of killer instinct and long hours and big results kind of let all the people who were good at posturing and politics rise to the top. We lost top talent to competitors, and we absorbed the worst of the competition. Right now that place is going through a crisis. Our cut throat not-diverse competition is thriving though.. My little sister excels at math and really quickly picked up modding games (mostly resource files rather than programming) when I showed her how to get started. But when I asked her whether she'd considered studying some kind of CS or engineering discipline she just went "yeah no that's not for me". Her overall impression was similar to the one you quote in Sample 1 ("CS is for antisocial people") , but she also said going into CS as a girl felt like a statement.

To some extent I wish the culture was different enough that she didn't have those associations, but most of all I think it's a shame that we've managed to convince her that she wouldn't fit in as is.. My anecdotal experience shows that the best algomerithic thinkers are a bit on the spectrum. Out of my colleagues, professors and classmates. Maybe 10% was nerdy but of the top 10% thinkers 80% was nerdy. 

What surprised me was that someone would choose math when considering spectrum. In my university the spectrum is Math>Physics>Eng. Math/Physics>CS/EE>Other Eng.. I've never heard that explanation, that women are more sensitive to the nerd stigma. Interesting take.. This is a great comment. I really like the points you mentioned. Pushing underrepresented groups into the field for the sake of representation doesn't seem like a good idea in the long run for **any** party in this problem. I find it extremely ironic that in both stories the girls are so heavily prejudiced towards CS people. I wish all people crying about female underrepresentation would notice that it's not usually about sexism in CS field but more about this stupid "nerdy loser with social anxiety" stereotype that is unattractive to people (and obviously false). But, as you said, this is a high-school problem (I'd even say that an elementary-school one).  


I really can't understand why people behind all these promotional programs are so focused on fighting sexism for the good of young girls but at the same time they seem like they haven't even asked these girls what the real problem is. Maybe they could learn about the awful label of being "a little bit on the spectrum" (wtf?!) imprinted in kids' heads and come to a valuable conclusion that the problem they fight has its roots in completely different places.. >This is a "US high school and US culture" problem. Not a CS problem.

Not just the US, also Europe. The proportion of female students in my CS program in Germany was around 10-15%.. >there is no dearth of Indian women in CS

Indian college student here (CS major). I agree with a lot of what you say in your post, and I definitely feel like the examples you gave sometimes as well. I disagree on this though - perhaps you feel that way because you've seen enough Indian women in CS in the US (where I'm assuming you live). CS/ engineering culture in India is largely shaped by the IITs. The gender ratio at the IITs is horribly skewed (fewer than 10% are girls). Yes, getting into an IIT requires passing an intense exam and in the past, boys have certainly had the advantage of being pushed into the sciences from an early age. But from what I've noticed among multiple women around me is that they would *hate* being at an IIT. And yes, I'm generalizing, but, well, *most* people I've met from the IITs can barely hold up a conversation. I realize that it sounds tone-deaf, and I apologize, but getting into the damn school *demands* that you *only* work on that one exam for 3+ years with no social life (because in terms of numbers, fewer than 1% get admitted). I realize it's unfair to attribute this to CS/ engineering as *fields*, but the situation here is such that this perpetuates the stereotype. Now of course, to help with this we need more women (and the government has been trying, somewhat), but not enough women want to be the guinea pigs that bring about cultural change at IITs. (I chose to opt out of competing for the IITs entirely, because fuck that.). I just realized by reading my message that this adds to the toxicity of the field as a self reinforcing loop :( Sorry for that. But I understand criticisms raise if one spends so much time deciphering the truth from papers that try to hide it by trying to make sense of their results rather than just exposing them. Where did Timnit call LeCun a white supremacist?. Let's please give a shoutout to gwern here because he is brave enough to publicly state what a lot of us are thinking: https://twitter.com/gwern/status/1277662699279826944

This is what Taleb calls FU money. Gwern doesn't have to give a damn about being politically correct because it doesn't impact his career in the same way. He doesn't consider himself to be part of the traditional academic system driven by politics and obsessed with publishing irrelevant papers. Thank you, gwern! I wish there were more of you.. > without any basis

We don't know that. By the wording she chose ("I'm tired of...") and some subsequent messages, it may be justified. It's also a presentation by people in the same institution as hers. We don't know what's happening "behind the scenes".. > PPO Anyone?


Not sure if your saying PPO (Proximal Policy Optimization) is good or bad, but I've tried it on a few problems found it quite robust. So I think it's still a good choice, at least as a baseline.. I have somewhat of a following I guess.  I‚Äôve shared it.  https://mobile.twitter.com/citnaj/status/1278195451326394369. From what I understand, yes it is partly for salary or resume building, but I think some degree programs require you to publish X amount of papers for graduation.. You seem to be only considering the top hyped labs for doing your PhD. Many lower-tier labs don't expect you to have tons of publications before you start the PhD, in many cases not even one. But for some reason I guess you would not want to work with those profs. You want to work under a perfect (famous) prof, but complain that they only take perfect students. It goes both ways.

Tons of people get PhD's outside the elite groups and they can still have a career.

But I agree. If I look at famous researchers they often had a straight, perfect road. Undergrad in a famous uni, already working in the field, then joining a famous lab, etc. There's a wide selection possibility for famous profs nowadays. Why should they pick someone less accomplished? They got to where they are because they pick highly competitive people who put in insane hours and strive forward. You may not like it, it may not be for everyone and it may not even be healthy. There are also other things out there. Not all basketball players can play in the NBA. You can't have a well-balanced life and be Michael Phelps. It is not ML-specific, not academia-specific. It's a competition, a status game, just like anything else in life.. This is relevant. Know your enemy [https://www.youtube.com/watch?v=rSHL-rSMIro](https://www.youtube.com/watch?v=rSHL-rSMIro). In the outrage against LeCun, nobody had any disagreement with what he said, it was that he was, quote: "mansplaining/whitesplaining". In other words, the problem was not what he said, the problem was his gender and skin color. 

When we value people's opinions based on their skin color, that's called racism. When we value people's opinions based on their gender, that's called sexism. And researchers said this with their full name on Twitter, and it had apparently no consequences for them. The only consequences happened to the recipient, LeCun, who is now silenced. It is as if the world has forgotten all the principles people have fought for over the last 50 years.. careful their buddy. you can lose your whole career over a post like this.... Please tell me that's satire.. Please, archive tweets on archive.is or archive.org before linking to them.

EDIT: for those interested: https://www.ferretfeet.com/feeds/2430--r-machinelearning/items/373143--d-adji-bousso-dieng-calls-out-deepmind-lecture-by-mihaela-rosca-jeff-donahue-and-claims-that-her-paper-presgan-has-been-unfairly-looked-over-due-to-being-a-black-woman. > > Secondly, there is a reproducibility crisis. Tuning hyperparameters on the test set seem to be the standard practice nowadays. Papers that do not beat the current state-of-the-art method have a zero chance of getting accepted at a good conference. As a result, hyperparameters get tuned and subtle tricks implemented to observe a gain in performance where there isn't any.
> 
> Does anyone have any suggestions on how to avoid this scenario (other than from a conference gatekeeper's perspective)? I've yet to see any.

Newbie here coming from an adjacent field, but if I'm understanding correctly, it sounds like "tuning hyperparameters on the test set seem to be the standard practice" means the tuning process and the final score reported are using the same set, which sounds troubling to me. Tuning hyperparameters on a test set leaks information about that test data into your model - I've understood the best practice to be using a separate validation set for tuning and then a test set for reporting, which you (ideally) only ever run your model on once so there's no leakage into how your model is built.

If tuning on the same set you eventually report results with really standard practice these days? I get that in practice it's usually not feasible to only run on that test set a single time, but surely a tuning process that uses it is basically using your test set to train an aspect of your model, which sounds like a huge problem.

And, if I'm understanding correctly, it sounds like the solution is for reviewers to be incredibly wary of test set leakage into a training protocol.. > That doesn't make anything you said less true, though.

How about the stuff about diversity?

Is it really the "machine learning community's" fault that there are so few Africans involved, for example?

Did LeCun say *anything* wrong or insensitive about bias?. Machine learning is not a purely theoretical field though. It's applied math, which has consequences in its usage even today. Social science and science very rarely can be separated. [http://www.faculty.umassd.edu/j.wang/feynman.pdf](http://www.faculty.umassd.edu/j.wang/feynman.pdf). Per point #1, it seems like it should be possible to submit a paper to a site like arXiv with provisional anonymity -- either time- or date-based that allows the paper to be posted publicly while also not divulging the author prior to peer review.. > Most students/researchers/advisors I know who work on a research project (either via actually leading it or a substantial amount of advising) have no more than 5-6 NeurIPS submissions a year?

It is telling that you don't see anything wrong with someone having 6 Neurips submissions in a year.. For point 5, I attended a pretty liberal small college. I remember a friend thinking a fairly political class (gen ed requirement) and being a white male just decided that it'd be much better to be silent as he felt any opinion he gave that wasn't near identical to the general class opinion would be criticized a lot. I also know as someone who mostly agrees with Lecun's comments I have little desire to enter publicly in discussions on a topic like that on twitter and would expect to get similar complaints.

&#x200B;

On 8, calling out a sexist comment as a sexist statement is fine. Just calling someone a sexist while it fits definition wise is likely to make them a lot more defensive and be a poor method of interacting with them and also likely to create that same fear of engagement. Mostly the difference in what feels like an attack of a statement vs an attack of a person.. Almost completely agree with this. On point 5. I think many people would like to engage in a civil way but in today's climate even engaging in a civil way runs a substantial risk which is why they (and I) choose not to do it.. > There's a very clear difference between 'engage' and 'tone-police'. As long as you're doing the former, I don't see why you should be "afraid".

This is a great demonstration of the very point Bengio was making.. To your point about #6 ‚Äî I disagree, most other science fields are much more tightly regulated than tech when comes the time to commercialize a technology. When you think about it, that‚Äôs precisely why animal testing is used in the first place. Hair gel might seem like a trivial application, but it does end up being used by millions of consumers: you don‚Äôt want to use and deploy a new additive in a formula without testing its toxicity level first.

My point is, there is certainly room for improvement in the regulatory frameworks of other scientific fields, but the problem with ML is that it‚Äôs **non-existent**.. so you are saying if you are simply 'engaging' there is nothing to fear? this is so dumb.. I personally saw three Phds basically ended by a paternal leave. I left academia myself for freelancing as a data scientist for \~1.5 years‚Äìtook me at least as long to get back on track.. You don't have to ask if they've had kids.  You just have to look at their CV and notice nothing was published one year.  You'd then reject that person for someone who had published that year without even considering that a baby might have caused the lack of research productivity.. https://course.fast.ai/. https://twitter.com/mmitchell_ai/status/1277696179464069121. I think they're pointing out how it negatively effects the double blind review process.. >simply don't prefer these fields on average.

It's intellectually lazy to not wonder why. For decades, science and engineering have been biased against women - this has been comprehensively covered, and anyone who is mildly perceptive will notice this. I've seen peers who are internationally accomplished math olympians be told they must have slept with older men to finish their problem sets. At the top of this thread, a user is claiming Fei Fei Li "has invented nothing." I've heard countless stories of sexual harassment despite attending an engineering school that is 50% women (MIT). Women in certain departments have been ranked "by hotness" in the past.

There is a consistent doubting of technical skill based on nothing other than gender, from middle school to tenure-track. This before even analyzing the historic enforcement of gender norms.

Edit: also, not to be an asshole but you have over 420 posts on incel subreddits like /r/braincels and /r/incelswithouthate...

Oh my god, I just realized your username is a reference to being an incel. Jesus Christ, this fucking subreddit.

For the unaware, [these are incels.](https://www.splcenter.org/hatewatch/2018/04/24/i-laugh-death-normies-how-incels-are-celebrating-toronto-mass-killing). The real question is, why don't they prefer those fields?. [deleted]. I think it's more along the lines of everyone, even those of us whose problems are fairly away from the cesspool that is the current US sociopolitical landscape, are forced to deal with and argue from a US-centric cultural perspective. Not every problem is like that of American racism. India has different caste-related problems, Europe has its own race issues that are VASTLY different from American ones. Yet we must all participate in this US-centric critical theoretic bullshit which is little more than status chasing and navel-gazing, at least when senior AI researchers who are some of the most overprivileged individuals currently out there, do it.. We are burning venture capital funds at an exceptional rate. Some of us get exceptionally rich while delivering empty promises and developing vaporware.

That counts as doing it right, doesn't it?. crunching that data yo. Getting Uyghurs into concentration camps, it would seem.. The quality of papers is a Zipf distribution - a power-law distribution. By pumping out so many papers ML increases the number of groundbreaking papers, which is a good thing despite making it harder to find them among the noise. 

The other thing we do mostly right is open-source of code and data. Despite some papers still not having code, it is now pretty standard for papers to have code which lists all the dependencies and use publicly available datasets. There will often be several reproductions by other authors shortly after a popular paper is released. Combined with the open-source nature of basically all ML frameworks and tools, this is the single biggest reason behind the growth in ML over the last 8 years IMO.. I agree, especially point 3. Having people to look up to is inspiring and motivating.. > Women are prejudiced against ML. That‚Äôs their bigotry, not ML‚Äôs.

So, it is completely irrelevant who is at fault. What matters is that it needs to be fixed, and that appropriate measures are taken.

A community that is representative of a society will tackle the right questions and find the right answers. That is why we want sth like 50% women, and represent minorities as well. It is not necessarily to be "fair" or "just", although these are things to strive for as well.

Even if your hypothesis that women are biased against ML is true, the question remains why that is so. Because programming/math/writing/experimenting is inherently male business? Or maybe because entering a dominantly male community is less inviting than entering one that has a healthy gender ratio? What scientific evidence speaks for the respective hypotheses? Show your work!

Of course you can jprefer easy answers and continue being sexist. But I wonder why people like that are interested in ML!. Different types of diversity are not mutually exclusive. How better to get diversity of thought than by having an environment where nobody thinks their identity and group memberships will hold them back professionally? Welcoming environments with lots of representation of the diversity of the membership might be a useful way to ensure you get maximum membership, which could help you ensure your field produces the best diversity of thought.. Oh boy, here we go. [deleted]. Hyperparameters should be tuned on a validation set, and a separate subset of data (held out and not exposed to the system under test during training or validation) should be used to evaluate the tuned parameters. This is a strawman. They can discuss those viewpoints while staying respectful rather than engaging in personal attacks. For example, I could have levied all kinds of insults because I fundamentally disagree with the angle you are taking here; instead, I'll explain how I see it.

There is a difference between discussing an opinion: "do you realize these viewpoints are dangerous, and similar to those espoused by white supremacists?"

and engaging in ad hominems:  "you are a racist, white supremacist". I like Gebru's work a lot, and I write about white supremacy in US academia as a volunteer under the direction of a professor in that field. My take is that it's OK to talk about racism and ask someone to read your work. It is also OK to inform someone that they are accidentally reinforcing bad ideas that come from supremacy.

If what OP says is true, that Gebru called LeCun a *white supremacist,* that's quite different and crosses a line if substantial evidence is missing. When I conduct archival research and write about historical figures, I don't use that word lightly. For example, some abolitionists were actively against white supremacy but argued it based on arguments *informed* by white supremacy. The distinction is especially important when people are alive.

I don't follow Anandkumar, so I can't opine there.. What did LeCun even say?. [deleted]. Part of this has to do with the growth of the sub. A few years back a much greater proportion of participants were ML specialists who knew how to identify good research in their field regardless of how well known the authors are. ML hype over time has resulted in this sub being overrun by AI celebrity gossip and news about Siraj Raval. Don't get me wrong, ML deserves a lot of the hype it's been getting, but that energy would be better spent developing new models and creating better datasets as opposed to the social media bullshit that's taken over ML's public perception today.. I'd also say that interesting research requires significantly more effort to engage with than a simple tweet.. Counterpoint: Reddit is just not good for serious research discussion due to the inherent popularity contest with up/downvotes. I get my research news from twitter, I just come here for the drama and the (very occasional) super hyped research.. >It used to be that science was embedded inside Western liberalism ideals of "nothing is absolutely correct everything is possible", but in recent times it has increasingly become binary, concentrated on "if you're not right you're wrong". Applies not just to science but many other things.

There is no "used to be". These are problems that have always existed across all fields. It is human psychology. We only perceive that those problems didn't exist in the past because of various biases.. I agree with the first two paragraphs, but the way you do research is probably quite different then what I do. There might be really good ideas, comparisons, notation, etc. you cannot afford to miss in these random arxive papers. So, I try to skim as much papers as I can and not read the author names or institution, citing and taking everything useful into my work.. I don't doubt that pedigree is one of the less-bad metrics to use when faced with such an onslaught of literature but I question whether Deepmind/Google/wherever are less likely to make stuff up than a group from a less prestigious institution. The big boys know that they can fart out any old nonsense and loads of people will respond with "OMG deepmind made a publish they're so great" while a less-known group don't have that luxury.. i heavily disagree with this. The size of the group and well-know-status is heavily influenced by the type of research it is doing. place matters too, but there are non-hype topics in ML and groups that specialize on that typically are smaller. And of course they have a more difficult time to get stuff published. Because the name matters to the AC.

in my experience some of the highest quality papers are no-name research groups.

On the other hand, i got used to the fact that some of the articles by high quality groups are so ambiguously and unscientifically written that it is impossible to understand what they are doing without the code. I remember times where we wanted to reproduce a result of a paper and it took us forever to find the permutation of algorithmic interpretations that actually worked.. Come on now. Saying that Fei-Fei did not invent anything is ridiculous. I wouldn't put her in the category of Hinton et al, but she has clearly been one of the top leading computer vision researchers for almost 2 decades, and her contribution to the field of computer vision has been massive. If I finish my career with 1/10th of Fei-Fei's achievements, I would be a very happy and lucky person. A research scientist with 100+ h-index, and circa 100K citations has definitely invented something, actually, invented a lot. She didn't get those citations for having a cute name, instead, she got them because she did awesome work.  


NIPS to NeurIPS was fine. Not many people had a problem with it, but some had. At the end of the day, the name did not change, only the acronym did change. It is a very small thing, and if it makes a few people feel more comfortable (Anima et al), then I am all for it. It was a legitimate claim, which for many people changed nothing, and for the rest made it more comfortable. No one is worse off cause the conference added an "eur" in the acronym.. > She [Fei Fei Li] has not invented anything.

Let's avoid worship culture *without* hyperbolically erasing the career of a scientist with a 100+ h-index.. NIPS made people giggle about nipples

14) Machine Learning has a 12 year old boy sense of humor problem. On the other hand, proving that something doesn't work (properly) is so much more work than proving that something does work. I think we should definitely appreciate negative results more, though.. Negative result is also a result, that's what my professors encouraged too. 

And i think, at Springer joirnals or somewhere else, to counter this "allergy" they introduced the format of "research report". Which is essentially "we tried this, here's the outcome". So both positive and negative results should be equal, because you do not report on the "new effect discovered", you just report on input-methods-output. I really hope this becomes a more prevalent format for scientific publications.. > And yet, to my surprise, my negative results were celebrated by the council.

...as they should (assuming you evaluated and documented everything properly). Being able to recognize that your original hypothesis is likely to be incorrect requires intellectual honesty, which is an essential characteristic for a good scientist/engineer.

Unfortunately, these days, presenting negative results also requires some level of courage, so... kudos for that.. To me negative results are often far more interesting than positives one, when I have an idea, I try to find a related work on scholar and if I prefer to find a paper with a negative result rather than no paper and lost time with a bad idea.

But, the problem with "negative" paper, is that you don't get much citation. As literature review and related work section, tend to only cite previous SOTA results. The only way to get citation for a negative result is if someone tweak your approach and makes it works which is a huge bet and can be seen by some as "pejorative citation" even if it is not.

IMHO, literature review paper should cite more "negative result" papers.. I would love to read your thesis! Give a link here, or send to me by email jon AATT [soundsensing.no](https://soundsensing.no) . From someone who does Audio ML on microcontrollers :). Thanks for sticking with it and publishing. Autoencoders are finicky buggers. I've "wasted" lots of time trying to get things to work that by all means should, yet they fail to produce useful output. I think that there is a ton to learn about these structures and what makes them tick.. >	proving something doesn‚Äôt work is just as important to the world as the opposite. 

The only issue I‚Äôve seen with these is that people hide this fact until the very end of the paper. Which is why I read the results first.. I'd love to read your thesis. If anonymity and/or confidentiality isn't a concern, can you share it?. I'm not Schmidhuber :).. You start with a hypothesis as proper science should be. You lay out your arguments supporting your method based on math, past research and/or domain knowledge. Then you propose your experiments. Reviewers accept or reject and propose suggestions to your experiments. If you get accepted, then you run your experiments, report the results and add a long discussion section. This way you are accepted whether your results are positive or negative as science should be.

In current system, we're all just HARKing.. Thank you very much, this is so rare voice in these circles. "Diversity & inclusion" mantra almost completely abandoned people from poor backgrounds or simply less educated families. The rate of stigma and rejection you get in academia, being from "lower" part of society, can be insane.. Thanks for the comment, makes thing more clearer now.. Part of the problem with systemic racism in the US is that by the very definition, minorities are highly under represented in the upper quintile of income distribution based on wage persistent wage inequalities for comparative work. This is well documented, I am not going to 'link harvest' here.

Which directly affects where they go to college, which directly affects social links for employment, which directly affects their future wage earning potential.

Which is why it is so damned hard to fix.

One thing that freaks me out to no end, being a child of the 60s, is that I personally experienced an uptick in minorities and women in engineering from the mid 1980s through the mid 1990s, then it stalled, and has slowly retreated ever since.

Two of the best hardware engineers (circuit design and VHDL/System-C) and one of the best software engineers (Linux Kernel) that I have ever had the pleasure to work with were women. Two of the best software engineers and one of the best hardware engineers I have ever worked with were racial minorities. So it isn't 'difference in ability' which really pisses me off when people try to raise that argument.

It is systemic racism and systemic sexism. It is because too many organizations are run by bullies. It is because to many people allow themselves to be cowed by those bullies.. Perhaps admissions should be more ‚Äúblind‚Äù, like paper reviews (are supposed to be). It is very much in physics and math as well. The backlash against Abigail Thompson for criticizing diversity statements in academic hiring is just one recent example.. You absolutely will see the woke culture in those other fields. 

Now is the time to practice and develop ML OUTSIDE of the established community.

ML is too popular and too controlled by a loose bureaucracy controlling the funds. 

The more ‚Äúfair‚Äù that bureaucracy tries to make the allocation of those funds/ accolades, the worse it will become. 

A lone practitioner/ small independent group will make the next giant steps in ML. I think I'm accounting for that part with the spotlight (and $$$) line.  You see analogs to this in media/sports icons all the time.. Are data scientists or software devs less stressed? Going by rate of online complaints, it seems similar. They say it's always tight deployment deadlines, technical debt, clueless non-technical managers, overtime culture, everything is always on fire etc. They look at academic research as a heaven where you set flexible hours, can spend a week diving in a math textbook or a new topic, you work on your own research project and ideas, your manager is a professor in your field not some MBA, etc. etc.

I'm saying this as a stressed PhD student, but I think people are biased to imagine the grass is so green on the other side.

Competition in general creates stress, and you have competition in corporate industry careers as much as in academic research.. I fucking despise the supposedly blind peer review. I say supposedly because the editor in the middle knows the parties involved. I'm jumping into industry once I have my PhD. (Applied math: stochastic optimization, not machine learning).. and also why they quit academia once they are done.. I would imagine somewhat less but not enitrely non existent as OP mentions.. > Another thing I have a problem understanding is why such intelligent people tolerate this bullshit.

The very vocal ones are true believers in the critical theory mindset. The rest are terrified of being "excommunicated" from academia or tech for "blasphemy".

I use the religious terms because it's often like listening to a geologist argue for creationism and that dinosaurs walked the earth 6000 years ago.. How does your independent research work? The search results I'm looking at for Indepentent research make it sound as if it's only undergrad research.. Chinese publications are worthless in terms of citation index compared to their English counterparts, especially in ML/CS. I don't think any serious researcher in this area would publish again in a Chinese conference or journal. It's basically academic suicide.. I'm always a little suspicious when I read a paper by a research group in China - I feel the probability of the results being not reproducible is higher considering the history of faking results or plagiarism in Chinese universities.. Jordan Peterson is a hack though, Slavoj Zizek showed it clearly in the debate he had with him. He is good at impressing YouTube armchair philosophers but if the profesional community doesn't take him seriously, why should we.. When you say closely related do you mean an ML subset like CV, NLP, or do you mean something like Electrical Engineering or Statstics which can have heavily overlapping subject matter depending on the area of interest?. >TL;DR don‚Äôt choose a famous ML advisor/at least know what you‚Äôre getting into.

There are some famous advisors that do have labs with a nice work environment and do take time for their students as well.

I'm not sure if this can be taken as a rule, being famous is not really a defining characteristic.. This is true, but unfortunately, twitter is a great way to keep in touch with recent advances, and being able to interact with the author directly is awesome.  
A balance needs to be found IMHO. What‚Äôs funny about that statement is that I am an undergrad shifting parts of the undergrad for others in the future into more well-placed discussions and decisions with the department chair at my University. (I.e. Switching Intro to AI from PandoraBots over to a TensorFlow tutorial on Image Processing with flexibility on TF/PyTorch.) So, lmk how the whole ‚Äúthese problems are not solved by students anyway‚Äù goes for you :). This is poor advice.. Seriously, if I asked my lab mates probably 80%+ wouldn't even know what drama stuff I'm talking about. Lots of people focus on their research and barely have time to take care of their health, friendships, family, partner, general life stuff (moving, doctors, getting children, finances etc.) beyond all the research and teaching work. Only a small minority has time to waste on Twitter controversies.

For me it's just some gossip to kill time with here and there. Nobody has prodded me with this drama stuff IRL. I only read it when I seek it. It's possible to focus on the work.. I doubt Bengio is basing this post off r/Machinelearning. I have a MS in an engineering discipline and it was relatively non-toxic, thanks for the concern and condescension about not being cut out for ML, though. The irony is palpable.. Probably the parts that has to do with big collaborations. I'm currently on one of those, and there's a heavy incentive to not misbehave since no one would work with you otherwise, and that's almost always a death sentence since you'll never not need help working on a big collaboration.

I have seen those behaviors from smaller labs and more independent researchers though. Thankfully the field is moving on the right track as older professors retire, for some reasons.. Exp√©rimental particle physics and theoretical physics. I don‚Äôt know if the physics community is actually chiller than the ML one, it‚Äôs just that from a Twitter perspective physicists feel less passionate and link less their beliefs with their job. I may be wrong. Sabine is a suuuuuuper edge case though, she has strong opinions about everything and will always fight people for it. It's probably more helpful to look at the average phycisist, although I have no idea how you would even go about that other than anecdotal evidence. But overall I'd say the field is less politicized and more concerned with petty drama, if only for the fact that the majority of physics is detached from most of real life.. As far as I have understood after reading Maudlin, I have to say that what bothered Einstein was not indeterminism, but rather the "spooky action at a distance". Indeed the paper by Bell did not dismiss at all certain hidden variable theories, it just show that so long those theories did not include non local effects they were unable to reproduce QM results. The "flagship" hidden variable theory (Bohmian mechanics) is explicitly non local and (I imagine) Einstein would probably dislike it.. You are [depressingly correct.](https://www.smbc-comics.com/index.php?db=comics&id=2524#comic). Well, for both this and /u/manganime1's question, you can take a look at http://horace.io/OpenReviewExplorer/

There were 9 papers at ICLR rejected with a (6,6,8): 
such as https://openreview.net/forum?id=SJlDDnVKwS, https://openreview.net/forum?id=ByxJO3VFwB, https://openreview.net/forum?id=HkxeThNFPH

Some papers that were accepted with extremely low scores: 

(1,3,3): https://openreview.net/forum?id=rJg76kStwH

(6,1,3): https://openreview.net/forum?id=H1emfT4twB. [deleted]. It is mainly sarcasm, but there is a hint of truth :/

To be competitive as a grad school applicant these days, you almost certainly need to be published in a competitive conference. I know one lab that filters out their applications by number of first-author publications in Neurips / ICML / ICLR. I think that's the most extreme example, but most labs do filter by the number of publications (doesn't have to be first author) and recommendation letters.

And for PhD students, the bar for being "good" is 2-3 papers in top tier conferences a year. My experience is only from being an undergrad and PhD student in a competitive academic setting in the US, so these expectations may vary.. My sarcasm detection model outputted a probability of 92.826% that it‚Äôs sarcastic. Depends on the group! The standard in our lab in Germany is 3 good conference papers over the whole of the PhD, which empirically takes between 4 and 6 years. Applicants usually come without publications or with one publication based on their master thesis. But we're also not a world famous hypercompetitive group. And that also means you have no jetpack names attached to your papers, so getting seen is difficult.. I think it's more facetious in tone than in substance. It really is hard to get into a good Ph.D. program without *multiple* top-tier publications in undergrad and/or master's.. You have to note that these are not papers from scratch. These are directly supervised and dicatated by the senior researchers, you're basically given an idea and told what to do so you're basically a programmer + secretary that writes down what the professor said out loud. Voila, a bunch of "top journal" papers as first author. It's not your own work though, you just were the messenger.

It's a whole different ballgame to come up with ideas and work them out and get results and then publish, all by yourself.

I've noticed that plenty of PhD's are closer to grad students than independent researchers. They couldn't research themselves out of a wet paper bag if there were told to come up with a paper without someone telling them what to do exactly.. > From a logical perspective, one should take pause at saying something does not exist. But to say a discrimination system that wouldn't affect you does not exist is naive at best.

Certainly, but he didn't say discrimination didn't exist. He listed the ways in which a model can be biased, and explained why some of those weren't applicable to that specific model. "You don't know what you don't know" is reasonable when you aren't a black person, but when we're talking about math, the ways in which bias can creep into a model are provable. There was no counterpoint.. It is an ad hominem.
Even saying "you're wrong because you don't have a diploma" is an ad hominem if you don't find any error in what they say. Goodhart's law: "When a measure becomes a target, it ceases to be a good measure". Thank you very much for that explanation. This kind of "publish or perish" culture seems dangerous. What prevents somebody from writing a fake paper? If the research cannot be reproduced entirely from a 3rd party by the paper, then anyone could publish something that is yet not achieved and take credit?. What prevents people from using harder-to-game metrics such as h-index or Altmetrics?  Is it because they're less intuitive? Or because theae metrics don't work for recently published articles?. Thank you for that answer. What is the benefit of staying in academia vs. full time private sector?. What, did you not have five 20+ citation papers in HS? Slacker /s.. That changes things.. They dropped out of high school, AFAIK.. Oh yeah youre right. He had ‚Äúhigh school dropout‚Äù in his bio and I remembered wrong. Thanks for pointing that out!. [deleted]. Oh yeah youre absolutely right. I just didnt like Anand‚Äôs public personal attack, but I think many of us had the same thoughts in our heads :)). [deleted]. I agree that if I‚Äôm on the receiving end, it‚Äôs a good practice to take criticism constructively no matter how toxic, but doesn‚Äôt it just seem kind of off to see a public figure acting like this?. While many are instigating chaos and then trying to lock in power grabs for their advantage, we do well to remain silent but resist when we cannot be harmed by mobs. We must stay calm, rational, and decent while others create mayhem.. [deleted]. to be honest 'being a little bit in the spectrum' is probably another result of the phenomenon that also makes people good at analitical thinking.

so it's not in people' head in my opinion, it's quite obvious.

that being unappealing is of course a social norm, but if it makes one unsociable, who can really challenge that?

otherwise i agree with all your and the parent comment's points. I was extremely annoyed by how Adji says "This gwern guy is researching embryo selection". If anything, I choose to believe that he's performing science, and not openly advocating for discrimination. I looked up a bit more, and he seems to be doing research in a plethora of fields.

Another tweet of Adji that annoys me is how she decides to ignore him, because she thinks he has eugenistic ideologies. I think it's very baseless.

[https://twitter.com/adjiboussodieng/status/1277689240990728198](https://twitter.com/adjiboussodieng/status/1277689240990728198). I didn't know that. I guess because I blocked all these people who stopped making sense in recent years. Thank you /u/gwern for standing up!. I see gwern mentioned here and on HN regularly but have no idea who he is or what he does. I always just figured he was a blogger (like Slate Star Codex) not really relevant to my interests, I had no idea he was involved in ML.

Can you ELI5 who he is and why I should read (follow?) him?. PPO is great except the improvements stems from 100 different other stuff, not the clipped objective. Read the paper ‚Äúimplementation matters in DRL‚Äù.. >You can't have a well-balanced life and be Michael Phelps.

I think this is a good insight, if difficult (for me at least) to hear.

A persistent theme in my life has been managing the inherent tension between the divergent paths that I'm led down by my insatiable curiosity, and my desire to make a meaningful impact in a domain, which requires focus and sustained effort for long periods of time. Sigh.. >When we value people's opinions based on their skin color, that's called racism. 

No that's not called [racism](https://docs.google.com/document/d/1S5uckFHCA_XZkxG0Zg5U4GQGbY_RklZARwu43fqJH0E/mobilebasic#id.9gabz1qjdt08), that's called racial prejudice.

Racism is different from racial prejudice, hatred, or discrimination. Racism involves one group having the **power**¬†to carry out systematic discrimination through the institutional policies and practices of the society and by shaping the cultural beliefs and values that support those racist policies and practices.. Only in your imagination.. [deleted]. People won't explicitly write this in the paper. They just say what hyperparams they used and don't mention how they got them. There are also a lot of small hyperparams that are not all even described in papers. Everyone knows it shouldn't be like that.

Proper scientific conduct is often a short term disadvantage. If you're careless, you still got a publication. If you're too careful you may never beat the scores of those who tune on the test set or play other tricks, use some ground truth information during testing etc.

The only way around this is having truly held out test sets and evaluation servers with limited evaluations. For some benchmarks, you need to submit predictions by email and the benchmark maintainers evaluate it for you.. You are right. People shouldn't really tune their models on a test set. Some people actually make the test set a validation set (stop training once the test score peaks). It's not standard ML practice, or standard science practice, but they do it anyway.. Yeah, exactly! And for young researchers that are concerned about their citations (and for good reason), a network-based citation system could be developed? Or perhaps simply keep track of citations in a researcher's profile but aggregate all anonymous references and retain their anonymity until the decision happens. A bit far fetcher, but certainly doable. I'm sure there are better solutions, but they won't implement themselves until we can come to a consensus as a community.. For a student that's quite a lot, yeah. But for an advisor that, say, has 5-6 students, it's not **a lot.** I mean it's certainly above the average/median, but not so much that I would be surprised.. 1. I'm not sure why your friend decided to not express their opinion out of fear of not "blending in". That's exactly the kind of culture we want to avoid: lack of inclusion. It goes both ways. I guess the online community (in general, not just ML) is to blame for rushing to form opinions based off others' opinions, without getting to know all the facts. Inherent human laziness, I guess? Something like the infamous LeCun thread might be a bit sensitive: I think as long as you do not ignore the context and address the problems in both sides of the conversation, it shouldn't be a problem. Anyone trying to troll people for that should definitely be called out.  

2. Fair point. Generalizing someone's character based on one statement is definitely wrong. Whenever there is such sexism or racism spotted, it should be us calling out that behavior rather than running to put stickers on the speaker. At the same time, accusing the person calling it out as 'being disrespectful' or 'emotional' shifts the focus of the discussion away from the real issue.. That was after he left, I'm after someone asking him to leave. [deleted]. There is no doubt that there are gender stereotypes and issues of sexual harassment and toxic communities when it comes to women and STEM which may be a contributing factor to them tending to stray away from STEM fields, but there is in fact something that suggests otherwise. [In a study,](https://www.researchgate.net/publication/323197652_The_Gender-Equality_Paradox_in_Science_Technology_Engineering_and_Mathematics_Education) boys and girls performed similarly in science, mathematics, and reading skills, but in countries with higher gender equality, women pursued careers in STEM less often than women in countries with lower gender equality, which is believed to be due to the belief in western societies to allow one to pursue one's own passions instead of emphasizing lucrative careers. If gender representation is that important of an issue, the right way to approach it isn't to set up gender quotas and offer incentives to women for going to STEM fields, as that ironically results in gender inequality. 

Also, ad hominem can't really invalidate one's points, but for the record, I'm no longer an incel but continue to use this account out of habit. Claiming that a researcher called another researcher a white supremacist, when in fact she didn't, then using that characterization as an example of disrespectful attacks, doesn't help anyone reflect. It's part of the reason why many minorities don't feel safe addressing these issues: "Hey this behavior disproportionately hurts minorities; this feels like a racist incident I've experienced in the past" gets  immediately exaggerated and escalated to "she called me a racist",  which in turn fuels the diversity crisis.. matter of perspective ;). Riding the gravy train into the next AI winter .... You are right in saying that male-dominated fields feel less inviting for women. What do you think about the idea that men tend to be interested in things and systems, while women tend to be more interested in people? This could partly explain why we see less women in technical fields.. > So, it is completely irrelevant who is at fault. What matters is that it needs to be fixed, and that appropriate measures are taken.

I don‚Äôt see it as a ‚Äúproblem,‚Äù I am totally indifferent to the gender ratio of the field. Women are free to do whatever they want. The door‚Äôs wide open.

> A community that is representative of a society will tackle the right questions and find the right answers. That is why we want sth like 50% women, and represent minorities as well. It is not necessarily to be "fair" or "just", although these are things to strive for as well.

This premise is totally *ad hoc* reasoning. Where is the evidence that diversity meaningfully advances fields outside if things like biology/psychology? Why is an additional women going to help more in solving the core technical problems in say, variational inference, more than a man? Their diverse life experiences will not help design a new loss function.

> Even if your hypothesis that women are biased against ML is true, the question remains why that is so. Because programming/math/writing/experimenting is inherently male business?

Because they‚Äôre more interested in ‚Äúhelping‚Äù fields, and ML does not fit well into their ideal image of themselves. Why would women want to stare at loss functions all day? They would rather be saving lives as ER nurses/doctors. Why is this so, I have no idea, but it‚Äôs not ML‚Äôs concern to try to control the choices women make.

Imagine your standard latte-sipping American girl. How the fuck are you going to convince her to go into ML? It‚Äôs impossible, and she wouldn‚Äôt be happy with the work if you did. People who make your argument, I always wonder, like, do you know any women? Can you imagine most of them willingly studying ML?

> Or maybe because entering a dominantly male community is less inviting than entering one that has a healthy gender ratio? 

What scientific evidence speaks for this hypotheses? Show your work! Or are you just going to prefer the easy answer that it‚Äôs actually your fault for being a gross incel that women won‚Äôt join the field? How could you ever disprove this hypothesis? It‚Äôs totally unfalsifiable. No matter how far the field goes to be ‚Äúwelcoming to women‚Äù you can always claim they haven‚Äôt gone far enough, and there is no way to ever prove you wrong.

> Of course you can jprefer easy answers and continue being sexist. But I wonder why people like that are interested in ML!

I can‚Äôt even imagine your point here. Why would a sexist not be interested in ML? What do you imagine the preferred interests of a sexist are?. > How better to get diversity of thought than by having an environment where nobody thinks their identity and group memberships will hold them back professionally

Absolutely, meritocracy should be the goal.. [removed]. How is the continent of Africa excluded?. Who is actively excluding Africa from anything?. I know. That‚Äôs why I‚Äôm surprised by the assertion that tuning HPs on test is normal now.. I find that whenever someone writes the former, people read the latter anyway.  It's easy to say "there's a legitimate way to discuss that", but somehow it always seems that people find a way to find fault in even the most obsequious discussion of racism or sexism.. I'm not aware of Gebru directly calling LeCun a white supremacist - although Twitter makes it pretty difficult for me to be sure.  What I saw was:

"Man I never thought this would feel EXACTLY like dealing with White supremacists. The "my Black friend" argument, a few Black men jumping in on that side, etc. Trump also has a Black friend who supports him, I'm sure he has many in fact..."

Which sounds a lot like the what both replies is insisting is the "correct" way to talk about racism.  And yet we see how easily the story is changed to "Gebru called LeCun a white supremacist".. He said the bias in the results of a paper came from the bias in the dataset, not the algorithm. 

He defended ML algorithms when other people were calling the algorithms themselves biased.. [deleted]. Very true. And I think the best way to remedy the situation is to have *less* of these drama posts. I have noticed that all of them are [D] posts ([R] and [P] are usually fine). Maybe [D] posts should be more heavily moderated/scrutinized to ensure they have actual substantial/technical content?. That‚Äôs only half true. Most fields of science could be covered in a single textbook for a long time (of course writing that textbook wasn‚Äôt easy!). The sheer number of researchers currently make us prone to a whole new group of fallacies. For example, most no longer take the time to sit down and personally evaluate what others do, and this shapes the landscape. Also, publish or perish is decidedly something that arose in the later part of the 20th century. And so on.. What are some non-hype ML topics?. There is Chinese last name "Huy", which makes Russians giggle, feel uncomfortable and become insulted. It is a tabooed word in Russia. There should be some federal or public commission on proper names that would filter such names like NIPS or Huy.. are you going to prove deep neural network doesn't work, like Minsky etc? :D 

Or if someone show you such a "proof", should you believe it?

On the other hand, there's a paper called BERT that's working, would you be better off believing something works or something doesn't work?. Neither positive nor negative results should be published behind a Springer paywall though.. Strong disagree with the word "often". Yes there can be super interesting negative results, but for every positive result there are a million things that just didn't work for mundane reasons. Imagine an exhaustive list of all the arrangements of mechanical components that DONT form a combustion engine. Yes, maybe there are a couple of super interesting examples in that set, but the vast majority of those arrangements will be extremely uninteresting.   


I think these very interesting negative results can quite easily be spun into an investigation that will be published in a top journal/conference.. Sure, I'm not on my desktop at the moment but in a few hours I'll send it! By the way I just checked soundsensing website and found out you guys have a lot of posts about noise, so hopefully you will find the work insightful and perhaps even have insights of your own!. I mostly agree, although a lot of the breakthroughs really are a feat of processing power in addition to the algorithms behind them. It's quite expensive.. I am sure these issues exist everywhere. But it seems in industry at least you come right out as being motivated to churn out more sales or profits, being the power hungry leader, so on and so forth whereas in academia you put yourself on a high pedestal as to being morally superior because of your work for the "greater good" (despite holding grudges for your competitors, power-plays against your competitors in "blind"-reviews, possessing the same qualities as managers in industry). Let's all be honest and accept presence of toxic people in all walks of life.. Same as you pal. Getting no respects with non sota results sucks even if they cover a good part of research. Gotta dive in to industry and make some real cash while leaving papers to ones whom adores overfitting their data test samples.. Where do you see yourself going? I've started work at company that does a lot of . optimisation/scheduling work. I'm interested in learning more about how this sort of stuff get's used in different places.. Fair enough at least itw more. Slow and painful. I work in industry to earn for living and so it's hard to have continuity. Slavoj ≈Ωi≈æek is a hack.  He is good at impressing Socialist armchair philosophers.  Not much else.. > Electrical Engineering or Statistics

lol, the best theoretical ML research comes out of these departments. Yes the latter. I‚Äôm in the Electrical Engineering + CS department, but on the EE side.. Yeah, I‚Äôm not saying it‚Äôs every group or every advisor, but that is my general impression. In any event, if you ask the students how they like the advisor that should give you the info you need. My main point was more that grad students shouldn‚Äôt feel pressure to get into a ‚Äúprestigious‚Äù group; but if you get a great advisor who is also famous, of course that‚Äôs great.. Just mute everyone who stirs up drama. I only look at interesting paper links from twitter. Politics needs much longer form to properly unpack ideas and nuance, this soundbite format leads straight to shouting matches. (Politics and society is important too, just don't consume it from Twitter). Who is Bengio?. I'm also on a big collaboration and there is drama/politics of course, it just doesn't happen in public/on twitter. But leadership often does try to keep everyone happy (even to the slight detriment of the science sometimes).. there is a lot of drama behind the scenes. It is not within a team but it clearly limits who gets ON the team. It takes some serious effort to get access to the data of the very large projects.. "It's probably more helpful to look at the average phycisist,"

&#x200B;

Just look for the ones with 1.998 arms and 2.4 kids. Yes, indeed, I'm not sure how much dr. Hossenfelder is representative of the physics community at large. That's probably one of the reasons she often complains she is alone and no one else "speaks up".. Thanks for that!. The rationale for the acceptance of these papers with low score was the response of the authors and the lack of further response from the reviewers. The Area Chair considered the authors' responses satisfactory and that the reviewers would increase their rating if they were to read those responses. Moreover, none of these were from Google, DeepMind, Facebook, Stanford or other mentioned institutions.

I recommend that people check out the reviews of these rejected papers and arrive at their own conclusions, but from what I read the Area Chair decisions seemed reasonable.. > https://openreview.net/forum?id=HkxeThNFPH

I wonder what people think about this one. The authors seem to be from Google and Facebook which according to the OP post should grant acceptance. 

However judging by reviews the meta-reviewer gets two weak accepts and one accept from a person who doesn't know much about this area, so AC writes a strong reject review and ultimately rejects the paper. Makes total sense from a perspective of a highly competitive program, but looks totally shady on the surface. i was reviewer for one of the rejected papers with high grades and i am perfectly fine with the rejection decision.

//edit actually i was mistaken. i know that paper and rejected it from a different conference. but one of the reviews could have been mine based on the arguments. weird.. Those two accepted cites are just posters, not papers.. It was an approach to multi-task learning in NLP where the RNN layers were trained to learn progressively more complex NLP problems. It wouldn‚Äôt be significant today in the transformer era, but at the time it was a step toward an alternative approach to solving high level NLP problems.. [deleted]. TIL: I am a bad PhD student.. Can vouch for this. Many first author ICML/NeurIPS not even getting an interview at top schools. [deleted]. > To be competitive as a grad school applicant these days, you almost certainly need to be published in a competitive conference 

What about ML journals with good impact factor ? I feel like journals are completely disregarded in the ML community.. What's the bar for being good for an undergrad who wants to apply for a competitive masters program?. Exactly! I had that in mind, but couldn't remember the name haha. Thank you!. Any reputable journal will subject all submissions to a process known as "peer review." An editor reviews the submission, then either rejects it or passes it along to other researchers in the relevant discipline who submit feedback to the editor. The editor then either rejects the paper, sends it back to the author for revision, or accepts it for publication.

Part of the process that follows is the reproduction of results by other folks in the industry. Note that this is something that is contentious in our field, as it can be difficult to exactly reproduce results which may rely on some (quasi)stochastic (i.e. random) process, or on highly-specified initial conditions (the hyperparameter tuning mentioned above). However, if nobody can even come close to replicating your results, then there's a problem. This is also true in other fields.

Taken together, peer review and reproducibility have historically done a fairly decent job of maintaining a generally acceptable standard of quality in publishing. Don't get me wrong, there are still lots of problems, and not even mentioned here is the paywall issue (paying massive fees for journal subscriptions just to *see* the research), but on the whole this has been the process, and it's gotten us pretty far.. One downside of more outcome-based methods like that is that they reward positive results more than negative ones by a lot, generally. This creates incentive for researchers to massage negative results into something positive, and invites variance since whether any particular approach succeeds or fails is largely chance.. Doing academic research in a private company is largely the same. You'll still be evaluated by the same metrics, papers and citations, and in some companies promotions will be tied to that. A lot of your colleagues will be in or from university academia. The main benefit is that your salary is better.

The benefit of staying in university academia is that, at least in theory, you can work on more long-term ambitious research without the pressure of producing short-term results for a company. I say in in theory because it's not that easy unless you have tenure.. How else did you think he got a position at OpenAI?  Sorry, your paper only cited 19 times, not good enough. Bai.. That‚Äôs a far cry from being a high schooler.. [deleted]. I'm not really agreeing or disagreeing necessarily, but I do have to note that I seem to notice people being silent or defensive way more often when the aggressor is a white male and I kinda have to wonder why. Even if the other person is simply responding in kind, they end up getting the brunt of the criticism.

I think it's good that you say you'll speak out, but maybe I've just become way too jaded and cynical. The whole "imagine if a white male did this" deal doesn't really do it for me.. I like environments in which there are minimal costs to skilled practitioners interacting with unskilled practitioners. Allowing for informality is one part of such environments. If she were abusing the kid's intelligence at length, that would be a problem. One sentence containing such as soft insult as "idiot" is fine.

Having said that, I did my learning on pseudonymous messageboards, which are a more private environment than Twitter, so maybe my calibration is a little off. But in principle, I think our happiness that there's exchange of views occurring should be bigger than our dismay that the interaction isn't perfectly polite. Replacing "idiot" with a euphemism wouldn't do much good, and impeding people from telling others that they're being idiots would potentially do a lot of bad. Creating an environment where people get many well-thought out insults thrown at them for casual use of insults like "idiot" would definitely do a lot of bad.. I don't think it's that complicated. It is just a function of number of hours people put in and how much they see the job as central to their identity and want to do a good job and prioritize the company over everything else. 

If you hire people like that, you're not going to hire people who have other responsibilities or have a divided focus.

I thought at first that you could have the work life balance and everything and didn't need to work long hours to be successful. Which is still kinda true. But you need to work very intense hours and you need to work enough hours to get to anywhere. I burn out after eight hours, but I have friends who just live and breathe their jobs and can do 12 hours a day everyday and feel very fulfilled. And they manage their other responsibilities well but they are very clear about work being a top priority. And they tend to have more career success than people like me who don't necessarily prioritize work.. I don't know much about him myself. I would describe him as a curious guy living off some old Bitcoin and a Patreon. He publishes high-quality research and experiments on his blog about whatever he currently finds interesting. That seems to be his full-time job. You should follow him because his opinions and publications aren't driven by politics. He's outside of the system.. You can find more about him at [https://www.gwern.net/Links](https://www.gwern.net/Links). [deleted]. What we need to understand in this new connected world is that our standards are distorted. We compare ourselves to the worldwide best. Some generations ago, you could be the best blacksmith or shoemaker in town and that would fill you with pride.

Today everyone looks at the superstars. We listen to songs by bands and singers from other continents, not the best musicians from our towns. Being the goto guy in some topic in your particular lab is not satisfactory. We'd all want to be Kaiming He.

This inevitable leads to disappointment. Attention and fame is zero-sum and compounding. Thousands of ML researchers cannot be famous at the same time. This is a problem for professors in the same way. To attract good post-docs and PhD students, they need to bring in grants, publish, etc. It's not only about students. And grant committees also have their metrics that they need to pursue. Universities need to convince the government and the public for more funding and for this convincing they need stats like publication counts and other impacts. It would be great if we could all just chill, research for years without having to publish anything, pondering things deeply etc., but the money has to come from somewhere. Theoretically you could set up funding for people who are then not measured on any metrics. But how do you pick them if not based on objective accomplishment? By connections? Who gets more recommendation letters? Measure their IQ? Or just subjective impression of a selection committee (will get you the smooth talkers and extroverts)?

To say something on the positive side: You can very well be well-known in a particular small research niche. There are still small, specific communities out there. But you won't be a celebrity researcher and you probably won't see your research covered in Wired and the NYT. But you'll still be respected in the specialist community. And if you invent something huge, the chance is always there even from a smaller lab.. dictionary.cambridge.org/amp/english/racism says nothing about power. Why is there a push to redefine the word racism?. According to several dictionaries you are full of shit.
"Racial prejudice" is one of the definition of racism and racism is also defined as the unfair treatment of people who belong to a different race, which is what happened here.
Words can and usually have multiple meanings.. The tweet.. [deleted]. > But for an advisor that, say, has 5-6 students, it's not a lot

Aah yeah, the it makes sense.. [deleted]. One simple example of the first is I know my school was very very pro affirmative action. Disliking affirmative action is a topic most are unlikely to mention a word on. Also leaving the context of race, pro life is another topic you likely would be heavily criticized for at my old school. There are some topics that when you know the school consensus culture is this is the obvious law/policy change, it becomes normal for students that disagree to stay silent. I‚Äôm aware of several other friends that also avoided similar topics. Some white males, amusingly some that are the relevant minority for the policy (like one black student uncomfortable with affirmative action). So I guess while race played a role in silence, for my old school the bigger aspect was if you had an opinion that leaned conservative on a major issue you likely would avoid discussing it. For race issues specifically even liberal opinions it tended to feel safe to avoid discussing if not a minority at all.

In the context of ml, I remember recently a friend working in an ml lab with the prof basically saying yeah this law is the obvious choice for affirmative action (some california prop this year) and feeling uncomfortable commenting on the topic.. It‚Äôs mind boggling that the community finds it more toxic that people are called sexist or racist than *actual* systemic sexism and racism.. > I'm no longer an incel but continue to use this account out of habit

Look, I'm basically out of gas for debating why gender discrimination in science and technology is a problem. But if what you say here is true, I still want to comment to congratulate you on leaving behind a toxic ideology because that's a nontrivial achievement.. There can be no new AI winter. ML is too effective and too easy to end up unpopular. My view is that it's going to end up like programming and CAD did, as something which pretty much all engineers are required to know and expected to have some proficiency in.. \> What do you think about the idea that men tend to be interested in  things and systems, while women tend to be more interested in people?

This certainly is the status quo. But is it because of genetics or because of a local minimum our society is in?

It does not appear to be controversial that the women tend to not enter male dominated fields. Even wikipedia has a lot of material on this.. > Women are free to do whatever they want. 

Ahahaha.

No further questions. Enjoy your bubble!. That seems like a really big claim.

I think the first piece, is separating the problem statement from the proposed solutions. Is there a problem with some form of systemic bias in research?

Studies like [this one](https://gap.hks.harvard.edu/orchestrating-impartiality-impact-%E2%80%9Cblind%E2%80%9D-auditions-female-musicians) to me says that yes, it's likely. In Orchestras at least, this says that gender does have a direct causal impact on acceptance probabilities. I also imagine the people holding auditions would all truly believe they're unbiased, which certainly makes things even more complicated.

So... is there a problem with bias in the research community? Given the fact that demographics there doesn't even remotely match general population demographics, and given cases like the study above from other industries that showed a clear bias, it seems likely to me that some people who should be allowed in given their merit may not be able to.

But here's the followup question that I think you're actually wondering about: was the right way to get more women in the orchestra to have a requirement that you have a certain % of players from each demographic? Would you dilute the talent of the orchestra if you did that? I doubt that solution would be anywhere near as good as the blind auditions.

So... even if there is a problem, obviously not all solutions are equal. Some probably would dilute the talent pool. Others would probably greatly strengthen it. The idea that not a single possible diversity centered improvement could do anything other than dilute the talent though, to me shows a profound lack of imagination.. Basic mathematical identities don't always belong on this sub.. This comment directly states that white men are more talented than folks of other races and genders. This is a dangerous and harmful line of thinking. Please re-evaluate this and consider deleting.. Ah I see. Thanks for the clarification:). I'll agree with you there. Part of the problem is in the current climate, accusations of racism or sexism short-circuit the conversation on both parties' behalves. The accused becomes very defensive and  (sometimes justifiably) upset, the accuser tends to double down on their attacks as once you've called someone a bigot, you can't simply go back on it. I don't know how this can be improved with the way the world is currently.. Link to that quote?. Oh, I remember that. I thought this was a different incident.. [deleted]. I think we'd need to message mods though, not sure how receptive they are.... Support Vector Machines for a starter. It is not that we are done with the topic, e.g. budgeted SVMs are still not really solved. But if you want to get published at big conferences: good luck.

In general it is an eye-opener to look at what got published~5 years ago at big conferences and compare that to today.. Please message me more about what that word means. Could you DM me a link please? I'd love to read it too. Yeah, that‚Äôs why I qualified it as a ‚Äúgiant‚Äù step. There will be a game changing development in the next 3 years; it won‚Äôt happen on a ML version of CERN. That‚Äôs my bold prediction/ guess. "We Didn't Start the Fire"

It seems to me that this may be more a factor of growing up. In another discussion elsewhere someone argued that the young adults who freak out about the state of the world (everything is going down the drain! Syria! China! Trump! Crimea! Covid! Brexit! Social media!), they are just growing up and noticing the world around them. It has over been like this. When I was a kid, there was war in Yugoslavia, before that there was a Cold War, dictatorships in Eastern Europe, in my grandparents' time it was actual war and cities flattened to ground.

By analogy, when people come out of school, they are bright eyed and naive, especially if they grew up in a protected environment. Whether you go to industry or academia, you meet the real world the first time. Now it's not about fake grades, but real status, wealth, respect. You are now a full adult and have to compete. And you notice that this involves politics and that people often compromise on the ideals that you had in your mind as a naive student.

It's a good opportunity to dive into philosophy (not the modern mathy kind, but the "what is the good life" kind, what to value, how to set up our lives).

Growing up is stressful. But anyone who tells me that life as a PhD student is so bad just doesn't have a big perspective on life. It's a bit like a post I read the other day, where a guy was lamenting that their life is practically over if they don't get accepted to MIT/Stanford/...

Seriously, you will do fine, having CS and ML skills that keep you afloat in a PhD program means you probably won't have problems with getting jobs or living an upper middle class life.

Compare it to the natural sciences, where PhD students are often not even fully funded, or they work on projects most of the time and research in their *free time*. It's crazy, but there is no funding. In comparison, industry is pumping loads of money into CS.

If you work in a richer country, you can go to various summer schools (free vacation essentially), where you're fed highest quality free food, can see a great location, meet famous people etc. Similarly with conferences, that are deliberately in places like Hawaii etc. Now if you work in a poorer country they can of course not afford this for sure, but I don't think it's only those people complaining.. I have no clue :'(

I've got work experience in operations research consultancy and I didn't like that much because the objective was to sell the consulting itself, not solve the actual problems. There are shipping companies, airlines, actual manufacturing, hospitals maybe, but my biggest hurdle is convincing many prospective employers that they could use an operations researcher when they don't even know what one of those is.. You just copied my comment, not an argument.. Well, traditional ML is a lot of signal processing and statstics isn‚Äôt it? I don‚Äôt know enough about DL to speak intelligently on the matter.. >In any event, if you ask the students how they like the advisor that should give you the info you need. My main point was more that grad students shouldn‚Äôt feel pressure to get into a ‚Äúprestigious‚Äù group; 

Yep, that's definitely good advice. edit: cant read. > I'm also on a big collaboration and there is drama/politics of course, it just doesn't happen in public/on twitter

This.. Yeah that's fair, I'm not privy to private politics that's not at my institution. But the leadership thing is true too and I'm glad it's that way, it's too easy to lose talent nowadays if your workplace is toxic.. What she blogs about is definitely representative for some of the reasons why some physicists choose not to specialize in theoretical high-energy physics (PhD in condensed matter theory here).. I actually thought that was great to see. The authors addressed the comments in the rebuttal, fairly answered all the reviewers points and even demonstrated that their paper _was_ novel and the reviewers didn't bother to reply/change score. Good on them for getting accepted.. I can't understand why this wasn't accepted. Also howcome there wasn't a response to the author's most recent comment?. ? what exactly do you think is the difference?. Yes, and I think publishing *less* will yield *more* meaningful results. Rigorous science has been discarded for more hackathon-style projects as a result of these publishing attitudes.. Same, but if _that_ is the bar I don't even care. That's so far beyond what I can achieve without checking into the closed ward, I'm fine with that.. How do you propose to evaluate people? Because it's physically impossible to get a place for everyone under Hinton or Jitendra Malik. There needs to be some selection. There are too many people with publications for all of them to be at a top lab.. holy. we are not hiring PhD students, they are PD already,. Getting a masters is a lot more straightforward than PhD but is academically competitive since schools heavily filter by GPA and GRE for masters. So first thing: get good grades and good GRE score. For example, one of my friends easily got into Stanford MS with a perfect GPA and perfect GRE quant score. Another got into CMU MS with a similar academic profile. Obviously getting a perfect GPA and perfect GRE quant score isn't easy, but you get the idea. These schools filter by academics first, so you need to excel in academics. Also, if you're a domestic student or you have high verbal and writing, that's a plus (but quant is still the most important).

Assuming you're going for research-based masters, you need to show some experience in research and have good recommendation letters from your advisors. The good thing is the bar for research is still less competitive than applying to PhD. Usually, advisors don't care about master's programs as much as PhD with respect to their reputation, so they will just write you a strong letter assuming you do alright under them. So just try to contribute as much as you can to your lab and impress your PI. Good luck!. Most papers are never reimplemented by anyone. I heard from several colleagues that they suspect fishy stuff in some papers as the results seem too good, and their reimplementation doesn't get close to the published results. Contacting the authors usually results in nothing substantial.

Sometimes people do release code, but that code itself cannot reproduce the paper results. Then if someone complains, Github issues often get closed with no substantial answer. There is no place to go to complain, other than starting a major conflict with the professor on the paper, who may also not respond.

Sure this is not a good way to build a reputation, but many are not in this for the long run. You publish a few papers with fishy results, you get your degree and go to industry. You don't really have a long-term reputation.

There are tons and tons of papers out there. Thousands and thousands of PhD students. Even those few that get reimplemented don't get so much attention that anyone would care about a blog post bashing that result.

What option do you have? You suspect the numbers were fabricated, but have to beat the benchmark to publish. Do you put an asterisk after their result in your table and say you suspect it's fake? Do you write the conference chairs / proceedings publisher? In theory you could resolve this with the authors, but again, they are often utterly unresponsive or get very defensive.

Also, many peer-reviewed papers lie about the state-of-the-art. They simply skip the best prior works from their tables. Literally.

In informal conversations at conferences I also heard from several people that some of they realized later that some of their earlier papers had evaluation flaws that inflated their score. But they obviously won't retract it, they ideologize it by saying the SOTA has moved on now anyway, so it doesn't matter.

Peer review is not a real safeguard.. Thank you once again for the detailed answer. What prevent this system from becoming a "review cartel". (lacking a better word). Say a group of people where to sit on all the power and just decide what gets approved and rejected.. That makes a lot of sense, thank you.. Agreed. I'm just the messenger.. Yeah youre right, I remembered incorrectly. Sorry about that. [deleted]. [deleted]. Thanks!. This is intersectionalism/"critical theory". Racism, sexism and bigotry is a problem. To combat this, intersectionalism then invented a formal system where the value of your opinion depends on your race, gender and sexual orientation. "To fight racism, sexism and bigotry, we need to be racists, sexists and bigots." It's dumb as bricks, dark and disturbing, but it's pretty damn mainstream at this point.

If you read the HybridRxNs link, you'll discover that according to critical theory, any argument against critical theory is racist, iff the color of your skin is white. Your opinion is worth more depending on how many historically oppressed groups you are a part of. So it implies a strict ordering of the value of all people, depending on how many oppressed groups they are apart of. The exact numerical value of each group has not been clarified AFAIK, and it's not clear if multiple group memberships has a multiplicative effect or an additive one. As you might guess, this didn't arise from the math department.. > Even to the point that maybe we shouldn't let the researchers set hyperparameters themselves.

I think that's not necessary. In the ideal-ideal world the test set would be fully held out, collected by a different group based on a short specification and researchers would submit *programs* maybe in a docker image or something, which would be called to make predictions strictly adhering to a predefined evaluation protocol.

In such a scenario you can do whatever you want, the result will be unbiased.

Another idea I had recently: to control the information leaking out from the test set through a series of evaluations (hyperparam tuning), there should be (Gaussian) noise added on the returned result. You can set the noise level, but if you want more precise eval measures, you will be blocked from submitting for a longer time. Therefore you can't submit 5 models and pick the best, because you can never be fully sure which was even the best!. From what you described, it sounded more like a fear of their opinion not being identical to the majority, rather than a fear of "personal grudges". For a situation like this, I feel staying quiet might not be the best thing to do. Although it may not be possible in places like workplaces, would you even want to be around people that would hold grudges against you just for expressing your opinions (and it being different)?

As I said, it's easier said than done to extend this to a work place and I agree with you on that part. What I meant to say is that we should strive to have an ideal workplace where expressing different opinions should not imply people holding grudges against you (at least not professionally).. > This certainly is the status quo. But is it because of genetics or because of a local minimum our society is in?

If you don't have the answer, do not assume either answer.. I am a male student of AI.

There was NO MOMENT in which being a male helped me, none.

I didn't get any tax cut because of my gender.

I didn't get higher grades.

And the rest of the field being mostly male also did not help me: how would it have helped me. The decision to studi computer science (and then AI) has been based on my passion, not on the percentage of other people doing the same things.
As a child, I liked working with fabric (a tipically female activity statistically). I did not care about the gender of other people who did the same.
Then, I got a passion for computers, because computers are awesome. And I started studying by myself. I was entirely unaware it was a tipically male field, because I knew absolutely none in the field.

Then I went to study CS in high school (high schools are specialized in Italy) and I found out it was mostly men in the field. By then, I was already in the field.

There is this perception that you need a "community" to accept you. That you need to FEEL part of a community and you need to see people that "look like you".

If that's you, you are not meant to study CS. CS is a passion, you love computers, not a community, there is no community.
If you want to start programming, or studying, you need a computer and an internet connection. Or an excuse to be lazy.. No it doesn't. It states that making a selection on attributes other than merit means that merit itself becomes less represented. Diluted as it were.

> consider deleting

Please re-evaluate your call for someone to self-censor a harmless observation of reality.. No. You are doing that if you think that people need to be forced to hire people according to their looks.. >This comment directly states that white men are more talented than folks of other races

No it doesn't. I'm merely stating that people should succeed based on talent and merit, not on their skin colour. The bar shouldn't be lowered for anyone, whether they are minority or not. The judging criteria should be absolute.. Consider this tweet from Gebru which I quoted in my other reply:

"Man I never thought this would feel EXACTLY like dealing with White supremacists. The "my Black friend" argument, a few Black men jumping in on that side, etc. Trump also has a Black friend who supports him, I'm sure he has many in fact..."

This is, as far as I can tell (and I apologize if I missed a different incident), what the OP means when he says, "Gebru calls LeCun a white supremacist".  But isn't this exactly what you said Gebru should have done?. https://twitter.com/timnitGebru/status/1276613820404789249. We're already removing a large portion of drama posts (believe it or not).

I think it's just the nature of reality that drama posts get a lot of attention - I don't particularly notice a drop in other discussion during times with a lot of drama (like now).. Google "russian mat".. Okay.  Jordan Peterson says things that are true but unoriginal.  They are things that aren't said much at the moment and that need saying.  He also says lots of wrong and unoriginal things, mainly when he is in Jungian mode.

≈Ωi≈æek is just a generator of fashionable nonsense that appeals to functionally innumerate Socialists.  Some of his nonsense is original but none of it is deep and most of it is context free.

Peterson is therefore far better than ≈Ωi≈æek.

But you knew that already, didn't you?. Indeed. Lots of ML is just signal processing/control theory/statistics rehashed. Even a lot of DL stuff goes back to signal processing (and more generally functional and harmonic analysis). If you're into more theory stuff, I'd argue that an EE or statistics department is actually the place to be since coursework and research is much more rigorous.. I‚Äôm not familiar with ICLR‚Äôs review process, but I‚Äôve been on both sides of the argument at other conferences. Usually accept/reject decisions go through program chairs, which means in this case the chairs might have either accepted AC‚Äôs recommendation as is (due to time constraints, etc), or had to reject the paper due to space or other constraints coming from the conference management (e.g. some programs like posters get space for X papers). 


Ghosting the authors‚Äô response is totally uncool but unfortunately that‚Äôs the most common move. After the decision is out the chairs would move to the next publication stage so there‚Äôs not much they could (let alone would want to) do to change the accept/reject decision.


It‚Äôs a gnarly situation all around and is a very familiar one for those who submitted papers to high profile conferences before. Some conferences like CSCW are doing really interesting things with 4 submission cycles per year, but it‚Äôs going to take years for the rest to catch up and change the system. I think the threshold is much lower for a poster than for a paper presentation. No one cites the posters.. This.. Yeah, but I don't think that's what causes this resentment. There's a fundamental difference between some modes of evaluation and others. If someone gets a higher mark on the SAT (for instance) than me, I will feel some envy or jealousy but I will inherently accept it because it's a test, and tests just fundamentally \*feel\* fair.   


A publication or a recommendation is a much more involved function of my connections and not just me in a room. As such, the process leaves people \*feeling\* that it's unfair. Also, these standards are much harder to apply universally due to their subjective interpretation. Add to the fact that the standards are likely to be waived for people who are known and you have a much more different situation than just comparing SAT numbers.  


I don't propose that we move to comparing SAT or GRE scores, obviously, because that heavily weighs English. But fixes like this are done in other fields - look at for instance the Math GRE, which is actually taken seriously for Math PhD programs.. Yea, these are big problems! I'll add that I was discussing peer review in scientific literature in general, rather than only wrt to CS. I think that SOTA-hacking is probably pretty specific to CS (not that other disciplines don't have problem of their own).. > What prevent this system from becoming a "review cartel".

I would say that those weren't prevented, and in fact they do exist within the community. Notably around some of the "celebrities" in the field.. These are all great questions and I don't think we have perfect answers to any of them! There are [definitely problems that arise with the peer-review process](https://link.springer.com/article/10.1007/s11948-017-9964-5), such as intentional delays, plagiarism, etc.

As commercial enterprises, journals have a real need to maintain- or to at least *appear* to maintain- fairness in this process. Each journal will use a different process for selecting the reviewers. In general, though, you won't see the same panel of reviewers for each paper; they tend to be researchers themselves, working in the relevant field and having the appropriate expertise, and are often either invited by the editor or recommended by the author. So for some journals, they use a different panel of reviewers for each paper.

Also, in an ideal world, the purpose of the peer review process is not to steer the competitive process, but only to ensure that the field is maintaining high standards and publishing legitimate, useful work. There are definitely reviewers, perhaps even most of them, who operate under this principle.. An example on Twitter of that *specifically*? No. But this type of thing is common enough in both in academia and industry. If you have female friends or acquaintances in STEM, ask them about it and I'm sure you'll get enough stories to change your mind.. Thanks for your explanation. This is one of the dumbest ideas I've heard in a while. That document was a total dumpster fire, your comment inspired me to read it, and I really tried to do so with an open mind, but jeez.. The problem with this is that the very definition of "merit" is subjective and therefore just as subject to bias as any other subjective criteria. Further, it exacerbates preexisting disparities by limiting access to high-paying jobs and the educational opportunities that lead to them by reinforcing the biased ideas that led to them in the first place.. [deleted]. Can you name an "absolute" judging criteria that does *not* reflect historical white supremacy at some fundamental level?. nope. she said dealing with LeCun was tending to become similar to dealing with white supremacists. she didn't say LeCun was a white supremacists. [deleted]. Is she talking about Le Cun, or just twitter trolls?  Hard to figure out (another reason twitter is so garbage).. That's good to hear. I was wondering if you think creating a separate sub for ML drama would make it easier for both mods and participants interested in technical content. Ok thanks. Thanks for the info, and how about on the applied side?. A little late to the party, but would you happen to have any recommended intro reading for someone relatively strong in signal processing/control theory? I‚Äôd like to know more about ML because that‚Äôs ultimately the direction my field (factory automation) is going, but I don‚Äôt come from an EE/CS background so I feel a little lost.. ICLR calls their acceptances "posters", "spotlights", or "orals".

I'm not sure what you're thinking of, maybe workshops?. There is too much soft human stuff involved in research for this to be enough. Being smart in general and being able to get through a PhD are very different skills. You can ace all exams and explain all the small details, but doing one's own research is quite different. I know many smart people who failed at it, burned out, cried in the professor's office, projects didn't work out, they changed direction too much etc. etc. then dropped out. There is no general recipe. Doing a PhD is not as deterministic as it is in university: "learn material from lecture" -> "get good grades". You'll need contacts and connections during the PhD as well.

My point is, prior research initiative and the social skills of making connections may be a better indicator than it may seem. All of this is legitimately subjective, just like it is subjective how good a prof is. It's a personal relationship between advisor and PhD student, for several years. The personalities, the "chemistry" must work. It's way different from admitting someone to a master's. And one good way to assess this for a famous prof is to see proof that similar work was already successfully done by the candidate with someone who the prof knows (i.e. recommendation).. I will definitely need to look more into this.

Thank you for explaining the process..  ‚ÄúThat which can be asserted without evidence can be dismissed without evidence.‚Äù (Christopher Hitchens). > limiting access to high-paying jobs and the educational opportunities

How does actively limiting access for one class of people in favour of another class of people, based on attributes outside of their performance improve the outcomes?. Trying to drive people away and shut them up is *never* the solution. Asking people to censor themselves is the antithesis of egalitarianism.. You're heading deeply into the ground of fallacy and this call for an example is disingenuous at best.

Your question contains bias itself.. Right, that's exactly what I mean.  Gebru doesn't actually say that LeCun is a white supremacist (which is what the above poster claims is unnaceptable) but rather says that dealing with him was similar to dealing with white supremacists (which is what the above poster claims is acceptable).  And yet, I'm the one who is accused of setting up a strawman.... You see what I mean? There's always a fault to be found.  "Well maybe she didn't call him a white supremacist, but she was still wrong in this other way!". My reading is that she is talking about both.  I -think- the "my black friend" line is a reference to this tweet:

https://twitter.com/ylecun/status/1275204723466022912

But it's very ambiguous.. No sane moderator would want the job of litigating what's drama vs a legitimate grievance, in this political environment, whereas if they say "no drama, period," they'll be accused of adhering to the "view from nowhere.". I can only speak for EE, but on the applied side you see ML techniques applied to more EE type problems. For example, in power engineering you deal with problems pertaining to, say, allocating power flow. This reduces down figuring out a good way to allocate power flow and people use ML techniques to figure this out. Another application area is in biomedical imaging (think MRI or CT imaging), and CV and DL has been really successful here. I personally think that these application areas are also much more interesting than what you see in CS departments.. That depends on what kind of ML you're interested in? Do you just want a primer? Or are you interested in a specific area? Anyone with an undergrad degree in something STEM can pick up background ML with ease.. I believe the ‚Äúposters‚Äù are literally just posters put up in the hall. ‚ÄúOrals‚Äù are serious presentations, and ‚Äúspotlights‚Äù are major presentations.. I agree in the core point. There is a core metric - research ability / social skill - that is highly predictive over general smarts. But in practice, I think you will agree that the former is hard to measure, and the latter could be measured easier and more objectively. If we place a high weight on recommendations as you say, we risk the chance of creating insider networks. Look up the history of standardized testing - it was created so that smart people who had no access could get in. Jewish quotas in Ivy leagues etc. were maintained by adding non-test portions on top. Chinese bureaucratic exams were also created so that there would be no hereditary aristocracy repeatedly referring each other.  


In fact, forget the problems of connections or research opportunities when measuring PhDs of today, when it comes to measuring research ability I cannot even objectively compare famous researchers of the past...on what grounds is Einstein better or worse than von Neumann ? It is not possible for us to judge. So I think the finding that smarts are a poor proxy is not surprising, because we are not even sure what would count as success. I would be very surprised if picking any metric allowed us to predict research success carefully at all because we cannot even pin down what research success is - rather, we know it when we see it.. The link you shared contained nothing even approximating evidence, so your quote is quite correct.. The  problem with this is that the very definition of "performance" is subjective  and therefore just as subject to bias as any other subjective criteria.  Further, it exacerbates preexisting disparities by limiting access to  high-paying jobs and the educational opportunities that lead to them by  reinforcing the biased ideas that led to them in the first place.. [deleted]. It's a rhetorical question to prove a point, the criteria doesn't exist. I'm not sure if I agree here. "I'm not saying you're a white supremacist but your behavior is the same as other white supremacists I've dealt with" is NOT generally a respectful attempt at discussion.

"Why do you believe in _____, that is a belief frequently held by white supremacists" seems to me personally a more honest attempt at a discussion.. You could just get rid of gossip (as in stuff ML experts say on Twitter that incites controversy). This wouldn't reflect a political agenda and at the same time keep the sub focused on ML. I mostly just want a deeper level primer. I understand what machine learning is but haven‚Äôt really found anything that describes the most popular techniques and how algorithms are constructed. For some reason, factory automation ‚Äúembraces‚Äù the idea of machine learning but you won‚Äôt find anything between complete fluff and journal articles from ISA or the like.. People consider all of orals/spotlights/posters to be "papers" at a conference. Orals are something like 10% of accepted papers, and spotlights are another 15%.. Pretty sure you're thinking of workshop submissions, which are sometimes in the form of a poster, rarely cited, and understood to be representative of an incomplete or poorer quality research project. But 90% of papers accepted to the main conference are also given a poster, and in the real world nobody distinguishes between these and papers chosen for a talk - 10 solid posters at NeurIPS is easily enough to get you a postdoc at any ML lab in the world, or a job at Google Brain.. This may also differ across countries. Here in Germany, a PhD is usually an employment contract and the profs have full autonomy in hiring who they want. It doesn't go through any admissions department or some such. So mandating a SAT-like standardized score would be a very big cultural shift. Maybe in the US, where PhD admissions are more systematized, it would be easier to change.. >  very definition of "performance" is subjective

Is it really? Performance seems like the one thing which is a solid metric. Non-subjective, quantitative.


> Further, it exacerbates preexisting disparities by limiting access to high-paying jobs and the educational opportunities that lead to them by reinforcing the biased ideas that led to them in the first place.

You didn't establish this the first time, repeating it doesn't make it any more valid.. What I said, or what the person who was told to delete their comment said?. > rhetorical

Absolutely. Rather than being rhetorical I read it more as a fallacy with no answer used as an anti-debate technique.

If the question were reverse it would be the very problem you're rallying against.. Here are two quotes, both written by you, one of which you claim is the acceptable way to approach the issue, and the other you claim is disrespectful.

"I'm not saying you're a white supremacist but your behavior is the same as other white supremacists I've dealt with"

"Do you realize these viewpoints are dangerous, and similar to those espoused by white supremacists?"

You're really trying to split a hair here.. I'd probably say to just go through the ["Understanding Machine Learning"](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf) textbook. It's a fairly standard reference.. Sounds like you‚Äôve had a few ‚Äúposters‚Äù at these conferences :p. That‚Äôs really sad.. Can you clarify on how, exactly, one might measure this undefined "performance" quantity in a way which is non-subjective and quantitative?. I mean, context is key. The first is dismissive. The second is a question expecting a response.

And life is messy, these issues are all about split hairs.. Thank you so much. üòä. Posters in the field of computer science (and especially AI) are papers. The vast majority of papers accepted are 'posters'. Top conferences have an acceptance rate of circa 22-25%, with each paper having to present a poster. The top 2-4% have in addition to the poster, also an oral, which typically lasts 4-5 minutes in computer vision conferences, while in Machine Learning conferences, short orals (aka spotlights) last 4 minutes, while long orals (typically 30-50 papers, so 1% of submissions) last 10-15 minutes. There is no difference in proceedings of the conference between a poster and an oral, in fact, that is not even mentioned on the official proceedings. When people say, that their paper has been accepted, in the vast majority of cases, it is a poster. In fact, as I said, roughly 3/4th of the submitted papers don't get a poster at all.  


Getting a poster is an honour in this field, not something to look down. Bear in mind, this is very different to more established fields where the conferences have not much value, and posters don't present top works. In AI fields, conferences are more important than journals, and posters are what people typically get when their papers get accepted.. Performance is not one specific calculation, it depends on the domain. 

It's easier to define what performance rarely if ever composes of. Things like hair colour, eye colour, skin colour, political persuasion, or genital formation size and shape.. I feel like we're playing Jeopardy here.  "I'm sorry, but you didn't address racism in the form of a question."  It makes it very hard for me to take seriously the notion that there is honestly a magically acceptable phrasing Gebru could have chosen.

Further, I'll remind you that you accused me of setting up a strawman in the same post where you tried to differentiate the correct approach from:

"engaging in ad hominems: "you are a racist, white supremacist""

It seems like perhaps THAT was the strawman here.. No worries. That book won't really have anything that's related to signal processing or control theory, so if you want to know about stuff that's related to those areas I can point you to some references. I'm also curious, if you don't come from an EECS background but have a background in signal processing and control theory, did you study something like Mechanical or Aerospace engineering?. Well isn‚Äôt this a part of the problem? I can tell you that outside the small world of people who work at these labs or are trying to, no one gives a shit about the posters. You guys have lowered the standard or what counts as significant work so far, so you can claim to have more publications.. In other words, no, you cannot.. You did set up a straw man. No one was saying people aren‚Äôt allowed to express their beliefs. The OP asks them to be expressed respectfully.. My degree is chemical engineering. We‚Äôre required to take a course in instrumentation, signals, and control. My specific niche is a little weird because companies pay for a ton of post graduate training.. You're either trolling or have no idea about how the field of ML has been progressing. It isn't better or worse, it is just different. Traditional fields have journals with established reviews and conferences for unpolished works that often have either no reviews at all, or very lite reviews. Our field has conferences with well-established reviews (it takes several months, at least 3 anonymous reviewers who don't know the names of the authors and vice versa, a rebuttal from the authors based on the initial review, a discussion between reviewers after that, and finally a discussion between area chairs to accept or reject the paper). If the paper gets accepted (probability is less than 25% for top conferences), it might get an oral. But very few of them get orals, and long-term it does not matter at all. Some of the most important papers of all time did not get an oral, some who got orals did not get many citations. In proceedings (the equivalent of the journals), there is no difference between orals and posters.  


What you are looking down as 'posters', in ML would be the workshop papers. Papers who are not good enough to go to the main conference often gets submitted to workshops that have a much lower standard of reviewing (but still are double-blinded) and are not part of the conference proceedings. Some very rare workshop papers actually become influential, but the majority are not. But a NeurIPS/CVPR etc paper is widely considered a strong paper regardless if it is a poster or an oral.  


Again, it is a bit different from other fields, but it is not necessarily worse and it has served us well. I think it would be nice to try to learn for something you are talking about, rather than making parallelisms between different fields.. this argument can be easily disproven.

none of the papers at neurips has a mention of their oral/poster status in the proceedings:

https://papers.nips.cc/book/advances-in-neural-information-processing-systems-32-2019

they are all relevant. just some get presented to a broader audience at the conference itself.. [deleted]. The fact remains that if you're classifying on anything other than performance metrics, you're optimising for something else other than those performance metrics and as a consequence the pool will be diluted.. OP may be calling for that on its face, but look at how they characterize Gebru's statements:

"Gebru calls LeCun a white supremacist"

Is that fair?  And if it's not fair, what does it say about the ability of Gebru to express her beliefs?  How can she speak up about racism if her words are twisted and then she's told that these twisted words are unnaceptable?. Or, perhaps, you‚Äôre part of the problem the OP is calling out. :). That‚Äôs worse! It means you guys are structuring things so people can‚Äôt even tell the real accomplishments from the bs ones.. Sorry, no one is impressed by your poster :p. You‚Äôre presenting a false choice, though, because objective performance metrics don‚Äôt exist. 

I agree that selecting on physical attributes isn‚Äôt a great solution- it‚Äôs kinda how we got here- but let‚Äôs dispel the notion of a mythical objective criteria that can be used to quantitatively measure or compare humans without bias.. Have you seen the actual twitter thread? Can you point out the offending supremacy tweets?. I think I am done with you. I tried to politely explain to you how the field works, getting trolling comments in return. You win this, here's a cookie.. they are all real accomplishments, but there is not enough space to present all at the conference via an oral. Therefore, between all significant advances, a few that stand out for various reasons are selected for an oral. This could be the quality of the research, or because a paper is very thought provoking. Some work is also ill suited for a 15min oral (e.g. a paper describing a 20 page proof of something). but the existence of the proof is indeed relevant.. It doesn't matter if you can broadly define *"performance"* for all domains. You certainly don't get it by making your selection on unrelated social justice based attributes.

>  let‚Äôs dispel the notion of a mythical objective criteria that can be used to quantitatively measure or compare humans without bias.

Sure, but we're talking about how injecting artificial selection biases into the process *dilutes* the attributes which were actually being sought.

> it‚Äôs kinda how we got here

Kicking the can down the road.. As far as I'm aware, the one I quoted earlier is the only one that mentioned white supremacists.  I've gone through Gebru's tweets trying to find other instances, but I came up empty.  But Twitter makes it hard to find everything that was said, so if someone is aware of a different tweet in which she actually says that, then I'll retract what I've said about it being unfair.. Dude you are not ‚Äúexplaining‚Äù anything to me. You don‚Äôt know me. 

You work in a field with standards so low, that the rest of us just roll our eyes at the latest claims. It‚Äôs been a long time coming, but the word is out.. No, they aren‚Äôt real accomplishments. It‚Äôs just the ordinary work that data scientists and machine learning engineers do every day. 

Can you imagine any other profession where every project that completed with even moderate success became a poster at a convention, let alone a publication credit?. It absolutely does matter, as it‚Äôs the entire crux of your argument.. You were clearly wrong on posters, had no idea what they are.  


We don't care too much about what the rest of you think about us, same as you don't care what machine learning scientists think about you.  


The posters and conferences have nothing to do with the OP's post on toxicity. 10 years ago, in ML conferences, the important articles were the posters in conferences, same as it was 20 years ago. Nothing has changed in that aspect, except that the number of papers has increased because the number of people actively working on the field has increased. But main articles being in conferences has been going on for a very long time.. *facepalm*. Replace the word *"performance"* with a hole.

Note how that hole gets smaller when you fill it with other attributes.

Add more salt to water and you have less fresh water.

Diluted. We can't escape this mathematical reality.

If you're selecting based on attributes which have nothing to do with the desired outcome, then you have this problem of diluting the attributes you are seeking. No amount of re-defining words changes this.. That the main publications in CS are conferences rather than journals has been true for a long long time, and nobody is disputing this. But posters are not at all at the same level as papers accepted for oral presentation. That's just ridiculous.. You don‚Äôt get it. When I speak at a conference, I present for 20 minutes followed by questions. The fact that I‚Äôm asked to do this, implies that my work is worth my peers spending 20 minutes learning about.

A poster? A 4 minute talk? It‚Äôs nothing. It doesn‚Äôt imply that anyone thought the work is worth spending time to learn about. It‚Äôs a phony accomplishment that let‚Äôs you put something on your resume.

You‚Äôre like children bragging that your second grade teacher gave you a gold star.. How it is ridiculous? There are posters who have got 10K citations and have won the test of time awards, there are orals who are on single-digit citations.

Orals, in general, might be better and they are given as a token of respect for what are considered as the best work. But at the end of the day, they get the same status. An accepted paper in CVPR is an accepted paper in CVPR, regardless if you had the extra 5 minutes to talk about it, or only the poster. In the proceedings, it is not written if it was an oral or a poster. When people read it, they won't even know it, all they see is that the paper was published on CVPR. That was my all point, that looking down on posters in top-tier conferences is ridiculous, considering that pretty much everyone is happy when their paper gets accepted, regardless if it got a poster or an oral. Of course, an oral is better, people like to brag after all, but in the long term, it makes no difference. Saying that a paper is just a poster is as ridiculous as saying that a paper is just an oral, it didn't win the best paper award so it is not a paper, and the standards are falling because of that.

This is very unlike to rejected papers (or workshop and second tier papers) who typically get no fame at all.. The original commenters were asking for papers that had been accepted/rejected. I provided a couple from ICLR.

He said

> Those two accepted cites are just posters, not papers. 

Which sparked this whole discussion. Obviously, dgc thinks that the original commenters do not consider "posters" to be "papers". He's obviously wrong on that.

Then the discussion devolves into whether people care about posters and some other stuff.. here is the reality: no paper on that conference can be explained in a 15min presentation. It is only an extended teaser. you might actually get further by explaining it to a few people at a time -> several hour long poster session

but really one has to read the paper itself. all beforehand is advertising. It is no price, but a duty to the community "look what i have done, now go and read it". 

your view that an oral presentation is important at all is pathetic.. i could go on on this, validly so: most journal papers do not have any form or presentation, you are EXPECTED to seek them out and read them. Are all of them useless because "they are not worth any peers spending 20 minutes learning about"? no of course not.. Yes, and there are models that were never presented that get used all the time.

But that doesn‚Äôt mean that a poster is a publication, or a significant accomplishment. Sorry, dude, you‚Äôre not that important.. Depending on the conference posters are either "not good enough to be submitted as a full paper" (i.e. for oral presentation) or "not good enough to be accepted for oral presentation" (aka as a full paper).
In some cases the authors decide beforehand to go for "real paper" or poster track, and in others it's the reviewers that downgrade it to "just a poster".. You‚Äôre fantasizing. Most of the ICLR papers I‚Äôve read in recent years, I said ‚Äúthere‚Äôs nothing novel here, there‚Äôs no real evidence of any improvement.‚Äù You think it takes hours to explain that you took last years model, added two layers, and ran a grid search? 

A paper presentation is not an advertisement or a teaser. Are you a child? Academic conference presentations are about sharing knowledge.

Maybe your attitude is one of the reasons the quality of neutral net papers is so low these days.. Simpler explanation: you are stuck in a narcissistic fantasy that your work has orders or magnitude greater value than it does.. It definitely is a publication by definition since it's in the proceedings. If you don't want to call it a significant accomplishment that's fine, but by identical logic you can call any paper that doesn't win best paper not a significant accomplishment, or any paper that is given a 5 minute talk instead of a 15 minute one.. Have you ever published in a top-tier ML/CV conference? Have you ever attended one? Can you tell me the name of a Machine Learning or Computer Vision conference who have different tracks for posters and orals?  


If the answer in all three cases is No, then consider that you might be wrong and don't know how this field works.. What conferences have 2 tracks for "real paper" or posters? All the conferences I'm aware of have a single track for "papers", which then, depending on reviews, get accepted as a poster, oral, or (depending on the conference) a spotlight.. i am not even doing research in neural networks. projecting much?

pathetic.. i couldn't care less what you think about my work :-). Really if our exchange is any indication, listening to what you say is important or right and doing the opposite is a better model of reality.. When I present at conferences, I‚Äôm giving a talk for 20 minutes followed by questions.

That‚Äôs how it is in literally every other field. 

It says something about the standards in neural net research today when you guys think a 4 minute talk is a presentation, and a poster is a publication. And what it says is not positive.. Wow, you are getting seriously defensive. If you submitted your paper to the regular track (e.g. because there's no separate poster track at that conference) and it was only accepted as a poster then the reviewers clearly didn't find it good enough for oral presentation. Sorry if that hurts your feelings.. The only thing it says something about is the ridiculous volume of papers that are flooding into the most prestigious conferences. Considering the numbers involved, it's not surprising the organisers of a conference with 1500 accepted papers spread across dozens of rooms with ten thousand attendees can't give more than a small percentage of accepted publications a talk, and most of them are given 5-10 minute slots with few or no questions because of time pressure. Sadly a necessity due to the popularity of the field.. It does not hurt my feelings at all. I am more than happy to get posters accepted. If for the rest of my life, I can make a deal with the devil that every paper I submit gets a poster (no orals, no rejections), I'll be happy to get it. Pretty much everyone else would.

I am also not being defensive, I am showing you how this field has evolved and how it works. In other fields, conferences are second tier, and posters there, don't do too much, they might even be not reviewed. That is not the case for the field here and for the top conferences. Posters are considered highly prestigious.

In fact, in many conferences (like CVPR), when you receive the email, it is either 'Congrats, your paper has been accepted', or 'We are sorry but your paper has been rejected'. There is nothing there about the orals. Only weeks later, the decisions in orals is made, which is more to show what are considered the best work. That decision is actually made from Program Chairs (not even Area Chairs) who haven't even read your paper. It is a badge of honor, but under no circumstances, it differentiates between a paper being a real paper, or a poster. They get the same tag in the proceedings. If you know people in the field who brag that they got a paper accepted in CVPR or NeurIPS, in 90% of the cases, that paper is a poster. Yes, I know, people from other fields got shocked about it. They got even more shocked when they realized that the 'poster' was actually reviewed. By 3 reviewers. Annonymous reviewers. Shock horror. It even took 7 months from submission to presentation. Why you didn't submit on the journal they asked. Because a conference paper is more prestigious and does more for my career. No way they said in disbelief. <<actually true story>>. No! That‚Äôs wrong! 

What it means is that the field is organized to promote work that isn‚Äôt significant and isn‚Äôt complete, or at a minimum that you guys can‚Äôt tell what work is significant or complete. 

Mostly what it means, though, is that you guys have a much higher opinion of your productivity than the rest of machine learning has of you.. It's demonstrably true that the number of researchers doing ML is significantly higher than basically every other area of CS, and growing rapidly every year - therefore it's obvious that ML produces more papers and it has nothing to do with anyone's opinion of productivity, nor is it evidence that uncomplete or insignificant work is being accepted. Paper quality in all fields follows a power law distribution, so the more papers published while standards remain constant, the more groundbreaking papers will be published, even while it becomes harder to distinguish the proportionally small number of them from amid the firehose of average quality papers.

There is no real evidence that the proportion of good quality accepted papers has declined from 10 years ago when ML was much smaller. From what I've seen of other areas of science, they all suffer from the same issue where the majority of papers are average quality incremental work, so it's perfectly normal.. No! The quality of neural net papers has been poor and declining for years! Now I‚Äôm starting to understand why. 

I‚Äôm sorry for saying this, but you guys seem to be suffering from some kind of narcissistic delusion.. As a proportion of total papers you can find just as many bad ones from 10 years ago, it's just that we only remember the good ones. Consider this quote from Fei-Fei Li at Stanford in 2009:

> Please remember this: 1000+ computer vision papers get published every year! Only 5-10 are worth reading and remembering!

This is still true today, except it's more like 5k-10k computer vision papers and 25-100 good ones.. That is not a defense, it‚Äôs an indictment.[D] This AI reveals how much time politicians stare at their phone at work. nan. Voting takes forever. What exactly are they "supposed" to be doing during a time when they're not allowed to talk to their neighbors? This is just "look busy" toxic work culture lol. The title is a bit misleading, since the percentage of each rectangle is most likely the confidence of the classifier that detects these objects. It is not the percentage of time each individual uses their phone in.. Afaiik this is actually an art project, designed to showcase surveillance and make politicians aware they are being surveilled too.. >Every meeting of the flemish government in Belgium is live streamed on a youtube channel. When a livestream starts the software is searching for phones and tries to identify a distracted politician. This is done with the help of AI and face recognition. The video of the distracted politician are then posted to a Twitter and Instagram account with the politician tagge

So, this tries to identify 'distracted' politicians, but only includes phones and excludes staring at laptops and tablets - for some reason? Is there a reason?

All I see is a system that detects whether some politician is using their phone or not.

Disregarding my (negative) biases towards politicians, this honestly says nothing of whether they're distracted or doing productive/non-productive work on their phones/tablets/laptops.. [deleted]. Better create a sleeping detector. All this shows is that you can successfully detect a phone and maybe a politician in the picture. 

It doesnt say anything about "how much time" or even about whether them staring at the phone is equivalent to them being productive or unproductive. 

Also, this looks like a simple object detection algorithm. It is not AI. It is a computer vision algorithm. 

It is time we start using the right terminology, be accurate in our descriptions of what the work is about and lastly, stop overestimating our work.. So probalistic facial recognition and smartphone utilization? How do you, or the viewer, know that they're not working?. Staring at phone may not always be unproductive if that is what being implied by this study. Lot of useful work is being done using phones now a days.. Cute project, utterly worthless at producing any valuable insights.. Soon: Your employer bought this AI that tells them how long you look at your phone at work. They use this to rank you against your peers to determine career progression.. what you should do is show the amount of time they spend with each lobbyist.. Amazing. Now we just need a model to determine how much of that time is spent watching porn and playing Raid Shadow Legends.. So, laptops and tabs/tablets are fine, phones are not.

they can be taking notes or working.

If its so important make a no phones rule, otherwise that's like judging a lions hunting ability by the time it sleeps.. Not useful in Italy. A lot of them doesn't even show up in parliament.... So, politicians are also human. I can't say I'm even a little surprised.. I hate politicians as much as the next person‚Ä¶ but they could just as easily be doing work on their phones.  It is how people communicate after all.. This reminds me of boomer comics. Does that mean they are on Reddit ?. This is kinda terrifying. Maybe you find it funny when it‚Äôs politicians, but what happens when corporations turn this dystopian tech on their employees? 
I love machine learning, but we absolutely needs regulations on things like this.. I hate how little care most politicians have for their job. 

Well I guess they work to keep their positions but I feel like after that it‚Äôs pretty minimal effort. We should lower their wages.. Source: https://driesdepoorter.be/theflemishscrollers/. Genius, now maybe we should tell their mood based on their facial expressions. People are way too focused on the why and not enough on the what.

Who cares that it's not a perfect representation of how distracted they are? Who cares if they're doing actual work or not on their phone? This is not about that, this is about an AI that can determine whether you're on your phone or not. That's it!

Why everything needs to be political?. I do at least 30 percent of my research (e.g. reading papers) and 50 percent of my business (following leads, LinkedIn, etc) on my phone. So I should be cancelled because I'm working hard on my phone? Dumb.. Should be combined with an reward system for not using the phone during the work haha. Great project. O! I think this is ImageAI(module for python). What about tablets and computers?. Ain‚Äôt they voting?. Skynet in the early stages of learning about its future foe. Why do we need an AI for this? You can literally just see them staring at their phones. This is such overkill.. I bet they don't like having their identity known by facial recognition. ü§≠. "Reveals" and "stare" are loaded terms that don't belong here.. Need to watch the live porn. Lol at Jan Jambon. Glad I didn't vote for him.. Checkin their crypto portfolio. Oh wait they are old dinosaurs. They dont have crypto. Just playing Candy Crush.. The politicians staring at their tablets instead: I don't have such weakness.. White=how sure that its that politican 
Green=how sure that its a phone. I guess being on an iPad doesn‚Äôt count lol.. To be fair their job is to lie to their constituents on Twitter, emails and Facebook and to vote the way their corporate sponsors tell them to. If they don't have phones they would have to remember what to lie about all in their heads. It can be exhausting remembering all the lies and keeping it all straight.. Very misleading op title and overall low quality discussion in the comments. 

I don't know but something about this post doesnt really fit with the sub, and I don't know if there is a rule or moderation approach that can keep that in check. This job must be boring. use this in Albania. Wait, do they vote one by one? Sounds inefficient. Here everybody has three buttons in their chairs, they push it and the result appears in a screen. Done in less than a minute. The exception is really important votes, they do vote aloud then.

This does nothing to stop them from staring at their phones tho.. They also might be working on their phones.. Not to mention people whose *job is coordination* might be using those phones to communicate about the vote. I wouldn't assume they're playing candy crush necessarily. It's supposed to shine a light on the power of ai assisted surveillance and the need to regulate it.. I never understand why voting needs to be done one by one. You guys know what to vote on the moment you stepped into that chamber, so why not just cast your vote and call it?. Why do they need to stay after voting. Come back in an hour or have an aide update you when they are ready.  

I'm sure there is plenty to do.. Non, that guy is obviously Bart Sommer 51% of the time only.

The only 49% he's Veronica Gillam, a bleu collar laborer from the west end.. Huh, I thought the numbers seemed a bit low.. true. There is no time in here so it's not really something incredible right now. yup, OP is full of crap. I think they made it to make a statement about surveillance.. The context is important here. [In 2019, a Flemish minister got caught playing angry birds on his phone](https://www.brusselstimes.com/belgium/71717/flemish-minister-president-caught-playing-angry-birds-in-flemish-parliament/). With this in mind, it is particularly funny to look at the politicians phone usage.. >for some reason? Is there a reason?

Perhaps they're doing last second research on the bills they're legislating?. Great example of bias in ai and ml. Well said.. I guess the assunption is you use the bigger screens for serious work and private phones (are they private?) for idle fun?. Underrated comment. Holy shit this project is brilliant. Object detection is a form of AI, how else do you think it is able to classify each bounding box in the picture? It has learned this through examples which resembles human learning.

Maybe don‚Äôt try to lecture people if you clearly don‚Äôt even work in the field. I would argue that object recognition is a large part of what ‚ÄúAI‚Äù colloquially means right now. If we are being prescriptivist, fine, but it‚Äôs too subtle or a distinction for the masses.. How do you know it's not using multilayer perceptrons? Ai is one way to do object recognition, but you're right that it's not the only way.. computer vision is a branch of AI though. but the rest is true, it can maybe detect humans sitting (not just politicians) and hands + rectangle-shaped objects, but hardly anything more.

I would be more interested in developing an eye-tracking model for the politicians' eyesight, which I think would be much more informative as to what is actually happening in their minds. At least, it would prove that they focus on the bottom of their female colleagues much more than they care to admit, which would be a good point to start a discussion on how to replace them with recommender systems for public policies.. I dig your comment so much, I had to comment in addition to thumb up.. [deleted]. >It is time we start using the right terminology, be accurate in our descriptions of what the work is about and lastly, stop overestimating our work.

No it isn't.. I think it's not facial recognition, because the seats are allocated to politicians on a permanent basis and the cameras are likely to have a static position and orientation. therefore it is sufficient to determine if a human is present where they are expected to be, to know that politician X is sitting on that chair. Ask yourself this: Did the poster ever make the claim that they weren't working?  


I think the more interesting observation is that EVERYONE is connected through the internet these days and has more faith in their own abilities to navigate the waters of the world than an institution.. If I were a politician and trying to be especially keen during a session, I would be fact checking every single thing, and I would be doing it on my phone.. Can't say I agree. Thought you were going to go in an actually useful direction with your post. I give you a 6/10 for effort. Here's where you should've gone with it:

>Staring at phone may not always be unproductive if that is what being implied by this study. Consider the damage these politicians would be doing if they weren't distracted by their little glass teats. For them, truly, doing nothing is more productive than the alternative. Ever heard the expression less is more? Well it's back and this is the Hollywood sequel.. Cute comment, utterly worthless at producing a valid argument.. As a member of the supreme master race that wastes time on their computer instead of their phone, I approve of this measure. We should spare no expense expediting its introduction (P.S. I think we can all agree any further measures would be excessive, so let's not even think about that). These are politicians. 

I dont care one bit about them. nor do I feel bad or sorry for them.

I care about the actual people though, and I dont think that level of McCarthyism should be allowed at work. I also dont think its legal with the current framework.. Sounds like a good thing.. Anyone have any idea why this comment from OP with further info got downvoted so much?. In your post scroll down and see - 

1. Object detection - correct
2. Face recognition - also correct

3. AI - marketing gimmick, seo strategy for post visibility but not present in the work. 

I am going to single handedly flag everything I see that uses the term AI frivolously.. bruh, it's not even actual AI - just normal CV stuff. it's not new, novel or complicated - simple object detection. Nor does it provide any insight whether they are distracted or not and thus represents a biased system.. politicians will always be political dafuq youre talking about lol. Politicians make rules about how their citizens are monitored.

They also form relations with countries that limit civil freedom on similarly technology as shown here.. So you want to look at C-Span for 24 hours and calculate how long they look at their phones rather than have an algorithm that does it for you?. FYI this appears to be a panel of Belgian ministers, not any kind of legislative body.  Still not sure what is wrong with politicians looking at their phones instead of sitting idly, there's probably a good chance what they're doing is work-related.. There was a very mild controversy about phones in the Dutch parliament a while ago. People started asking questions about what politicians do on their phones during their work. 

Turns out it's a bit of social media, but mainly fact checking and texting employees for certain files or plannings that correspond to their work in the parliamentary room.. You work on a laptop!

It's almost imposable to work on a phone, there at best on twitter.

You just need to cross check there social media accounts to the time on there phone to see if there posting cat photos online.

edit, now I think about it you may be able to tell what there doing up to a point from how they handle the phone, finger/hand movement will give a good impression of what kind of task they are doing.

Text will always have keyboard at bottom of display, reading will have long swipes up/down & more random movements will be games?. >It's supposed to shine a light on the power of ai assisted surveillance and the need to regulate it.

If so, then it's been *very* poorly communicated. I certainly didn't infer that from the image.. It also showcases the addictive power of AI selected digital content.. I don't even know why they need to vote at all. The descisions are already made ahead of time -- voting is a formality in western countries. Much of the east stopped pretending a while ago.. Tradition + transparency + accuracy + accountability.. [removed]. Person relaxes for a few minutes at work, more news at nine!

I really hate this praise of micromanagement of politicians, especially from people who otherwise hate micromanagement.. Jan jomson is in the photo.. lol. Perhaps someone else made a claim and they're trying to check if it's true?

Perhaps they're trying to listen and learn, but stay factually oriented at the same time?

Perhaps someone asked a valid question and they didn't have the answer, and they're trying to check now that there's some down time?

Perhaps someone said one of their claims was wrong, and they're trying to check?. Why or how is this a great example of bias in computer vision?. Indeed, it's impossible to send a tweet to your electorate about the vote. Phones only come with Candy Crush these days.. It's the whole friggin point of this work: https://driesdepoorter.be/. I don't think convolutions or backpropagation resemble human learning in any shape, way, or form, but maybe that's just me.. It is a reason for confusion.

Because of this misrepresentation notions like, "AI will take over the world" , "AI will be the reason we lose our jobs", "AI is biased" arise. 

Being accurate and being willing to explain these distinctions to the masses doesnt make me a prescriptivist.. 
(When you're a practitioner for a very long time, you know what was used, how something was done just by looking at the output. ) 

But for the benefit of your understanding and for others reading, Id take your question as an opportunity to explain this point further. 

Firstly, nothing in my comment suggested that I thought they were or were not using "multilayer perceptrons" , even if they were using neural networks or deep learning it would still come under the area of computer vision and not AI. So your rather emphatic focus on MLP (multilayer perceptrons) doesnt mean anything specifically or validate the stand you are trying to take as to why this should be under AI vs computer vision.  

AI is an umbrella term that could mean anything. Usually it is used when you have been able to port a supreme level of intelligence into your software or product through a combination of various tasks within various subfields of AI. AI combines areas like Classical Machine Learning, Natural language processing, Computer vision etc, which again can be broken down into tasks like classification, regression  (classical ML), question answering systems, text summarization, named entity recognition (in the case of NLP) and object detection/localization, object recognition, facial recognition, scene recognition (in the case of CV) . 

The reason AI does very little to actually inform readers about what is happening is because it could mean anything right from a heuristic based system to deep learning and it also doesnt give a clear idea of the kind of data that was used to generate a result. The way to describe work in the field is to mention the particular tasks used to achieve an objective , like object detection, facial recognition in this case. Instead of saying AI. 

For additional reference, please take a look at this collection of papers and how they describe the various tasks : 
https://paperswithcode.com/sota

A very common misrepresentation is when people use Machine learning and AI interchangeably. Machine learning is a subset of AI and isnt equivalent to AI. 

Now, riddle me this, does netflix use AI? Or recommender systems?. Computer vision is overlapping with AI but it is not a "branch  of" AI. There are entire vision tasks that involve only geometry.. Thanks for letting us know.. The percentages are actually confidence scores. Which denotes how confident the model is about predicting that certain label (phone or politician's name in this case) for that object within the bounding box.. [deleted]. Thanks for the breakdown, chief.. That's nice, but that's why you aren't a politician.. Let's agree to disagree and leave it at that.. >I also dont think its legal with the current framework.

lol. How do you train object detection models???. Looking at the profile of the original poster, I really do think the intent of the post was marketing.. I‚Äôm not really a politics person but isn‚Äôt that kinda their job? Social networking to implement political change lol. I believe that the root reason for having a parliament is to discuss, and to discuss you need to focus on who's speaking.. That's an ignorant observation. They are politicians, don't you think they may need to text or send a quick email to people? 

Regardless, phones are a massive part of most any blue collar job that involves communicating with people. Also, "almost impossible to work on a phone" is a short sighted opinion.. I know someone who is chair of two national sized companies (250+ employees each) and only ever uses their iPhone. It‚Äôs not that uncommon.. I wholeheartedly disagree and work on my iPhone all of the time. I hate reading dense text on a laptop. It‚Äôs much better on an iPhone. An iPad is great too, but less portable. I highly doubt it, but they could literally be reading a bill or annotating it.

As a software engineer, I think there is very little that can‚Äôt be done on a phone or tablet these days. Unfortunately, developing software remains one of those things, but most jobs don‚Äôt even need a computer anymore. Hence, apples ‚Äúwhat‚Äôs a computer?‚Äù iPad commercials.. I'm a sysadmin and can do my job 100% on my phone if needed. It's maybe not the most efficient but it can be done.. I work on my phone most of the day. Poorly communicated by the OP by not providing a link or description of the actual project/message.

> Dries Depoorter is a Belgium artist that handles themes as privacy, artificial intelligence, surveillance & social media.

https://driesdepoorter.be/. Yeah, I too only know that because I saw the  project site in a different sub. I don't have it on hand but others have already commented it here.. Tradition I grant you, but the accuracy, transparency, and accountability parts is BS. It'd be easy to display the individual vote on a dashboard, on a public website, and if you want, clear Yea/Nay indicator right next to their face. Hell give them 5 minutes between pushing a button and vote lock in, if you care to.. [removed]. Ôøº

Digital Culture

Artificial Intelligence

AI bot trolls politicians with how much time they're looking at phones

"pls stay focused!"

By¬†¬†Alison Foreman¬†¬†on July 5, 2021

¬†>¬†Life¬†>¬†Digital Culture

Sure, we've all snuck a look at our phones in dull meetings. But if you're working on the taxpayer's dime, you'd better be ready for artificial intelligence to call you out for gawping at the black mirror in the legislature when you should be, you know, legislating.

That's what digital artist Dries Depoorter did for his latest installation "The Flemish Scrollers." His software that uses facial recognition to automatically call out politicians in the Flemish province of Belgium who are distracted by their phones when its parliament is in session. The project comes almost two years after Flemish Minister-President Jan Jambon caused public outrage after playing¬†Angry Birds¬†during a policy discussion. (Really.)

Launched Monday, Depoorter's system monitors daily livestreams of government meetings on¬†YouTube¬†to assess how long a representative has been looking at their phone versus the meeting in progress. If the AI detects a distracted person, it will publicly identify the party by posting the clip ‚Äî on Instagram¬†@TheFlemishScrollers, and Twitter¬†@FlemishScroller.. >Person relaxes for a few minutes at work

Relaxes during a debate. It's like playing on your phone during a meeting, don't think that would come over too well.. I really hate apologists for the destruction of democracy, especially from people who otherwise love democracy.. I dont think at least half of politicians do that... and based on the number on their phone in this scenario... 

statistics are not in their favor.. [removed]. Not in the computer vision, but rather in the interpretation of the results

We see people looking at laptops and phones, so we assume they're slacking, when there's no reason to believe that's not working. Because the outcome doesn't match what OWilson90 wants, so what else could it be aside from bias in machine learning?. Or read a text from legal counsel‚Ä¶. Cool and terrifying. I don‚Äôt want to start the discussion about biological plausibility of deep nets here, because it is redundant to the discussion of the system being an AI. The functional concept is what determines if a system can be considered an AI, not so much the technical implementation. AI in the field is seen as mimicking some sort of smart human behavior which leaves no doubt when the system is ‚Äòlearning from examples‚Äô. I do understand however that the umbrella term has a vague boundary (back in the days even simple knowledge system were seen as AI), but object detection is far away from this boundary as it resembles complex human behavior. It takes great strawman-making skill to see convolutions or backpropagation in

>It has learned this through examples

...but if you insist on being pedantic, backpropagation does resemble human learning in the broadest way, as it involves changing internal information processing routines after obtaining information about past mistakes.. Yes, it is just you.. I think what I‚Äôm circling is that we might be better off teaching everyone what ‚Äúgeneral intelligence‚Äù is than trying to control public understanding of what ‚ÄúAI‚Äù is. AI is already a huge and messy term that‚Äôs been overloaded and pulled in ten different directions.. love your comment. but you are fighting a fight you cant win.
everything is AI nowadays as long ad it involved a NN. thats the power of the buzz word. What are some examples where it is okay to use AI to describe the underlying algorithms?. AI just seems like a silly marketing term. We have not even defined intelligence properly, how can we define artificial intelligence then?

Machine learning sounds more intuitive to me since we usually try to model a probability distribution in the best way we can. The machine literally learns the distribution in a defined model.. Yes it is.. Come on it's clearly AI, even if you do it rule-based. What you want to say is that it is not necessarily ML, to which we all agree I think, even though today it's handled in practice as a branch of ML. Unless you want to point to the fact that you can conduct computer vision tasks without computers, which would be a bit paradoxical.. Sorry, I am not used to the terminology in a parliamentary system, I should have clarified.  These are not MPs but ministers of finance, energy, etc.  In the US we would call them members of the cabinet (and their title would be secretary).  I don't know how they are called in a parliamentary system.. *White collar. > phones are a massive part of most any blue collar job

Either Gmail has become a really critical tool for stevedores lately or I think you might need to check your collar color.. I‚Äôve made tons of deals on my phone. Almost none from a computer.. Apropos of nothing, I find it highly depressing that the relatively free and open personal computer is being supplanted by these restrictive proprietary devices and their walled-gardens. The "what's a computer" ad genuinely triggered me. It's a shame we don't have an equivalent to the relative freedom of the PC in the portable devices space, because I'd be all over that.. Im surprised how defensive people can be. Are you making the assumption that the average politician is as skilled as a sysadmin/software engineer etc..

And yes you can work from a phone, that is why I was trying to think of ways to pull from open data to get an impression of what there doing. Im not sure where the project has been done and how open data is at that location so there may be more options to pull open data to work out what they may be doing.

Keep in mind they clearly have the option to use a laptop or tablet, in the photo you can see people using a phone over a laptop/tablet "@wbeke" is marked at 85% on phone when he clearly has some kind of laptop in front of him. So im going to make the assumption he is using applications that run on phones instead of a laptop, he is not a system admin so he wont be monitoring remote systems he more likely to be in privet chat or social media & it's not hard to find him on google etc.

I dont mind the downvotes but I am sad about how no one seemed to think past "well I work in IT and can do my job on the phone" when your talking about a different demographic.. You probably don't do anything productive most of your day either.. I mean if AI is going to **beat on politicians** I am all for it.

I literally use blueiris to keep an eye on the activity outside my house.. Depending on your institution they do voting even through electronic means manually slower to ensure that the results are accurate because of the 1988 Mexican general elections scandal. 

Whether it's actually effective or not who knows but avoiding election fraud is important. Going slowly ensures accountability by ensuring every single person who voted intentionally and everyone who witnessed it agrees with the observation. If there was any ambiguity in the process it could be taken advantage of.. [removed]. Nah.. Seeing some debates in my life I would rather have played on a phone.

They have become borderline useless imho. People who don't really care for the bill will vote party line, those who care have done their research beforehand. The public debate usually comes after the debates in the committees, with results and positions being available well before the debate. A speaker normally doesn't add anything new except maybe some slights on the opposition that make it to the news. Questions always seem prefabricated. The debating culture in politics is nonexistant.

The debates are also not a politicians "main work", when they don't have to join them as a speaker. It just appears so because the bulk in committees isn't visible.. [removed]. So, no bias in computer vision. 

Sorry but controlling bias in human interpretation is not something we computer scientists signed up for. This expectation actually infuriates me a lot as a ML researcher. 

So now we are supposed to control the bias in human interpretations? Interpretations by the same people who dont even understand the distinction between artificial intelligence and machine learning? Interpretations by people who mistake the confidence score to be a percentage representation of "how distracted a politician is" pertaining to this result? 

It is the individual's responsibility to be well informed and avoid bias to the best of their ability. If they dont know something,  ask questions or seek an explaination before jumping to a conclusion.. It's so sad that phones don't come with email anymore.. I'm not taking issue so much with the use of the term "AI" because its definition has become increasingly nebulous over recent years. I'm taking issue with the idea that AI = human-like = learns by example.

See my response to the other comment, copied here:

> That is a ridiculous argument. If all it takes for a learning process to be "human-like" is to learn by examples and change internal information, then linear regression and na√Øve Bayes must qualify as "human-like" too. After all, they have internal parameters that can be updated with new examples.

By that definition, we've had "human-like" learning algorithms since  ENIAC. Actually, even longer than that because there were plenty of non-digital computers that could implement naive Bayes. That is a ridiculous argument. If all it takes for a learning process to be "human-like" is to learn by examples and change internal information, then linear regression and na√Øve Bayes must qualify as "human-like" too. After all, they have internal parameters that can be updated with new examples.. Nah give it a few years, NNs won't be AI any more than SVMs are. AI is always whatever we couldn't do five years ago.. Don‚Äôt know, autonomous vehicles? Calling everything AI is like calling all connected TVs Smart.. This is a great question. You will notice anyone (business or people) actually working in this area or people who know this area will never refer to their work as AI. They will always break it down into the subfields. 

The ones who very rarely do tackle AI always give a very clear explaination as to why they think this would come under ai. I would say OpenAI's overall goal is to solve the problem of artificial intelligence through their various products. 

Autonomous cars is a great example where the end result could come very close to being an AI based system - it has computer vision based systems, sensor based integrations like LIDAR, it also has a lot of custom heuristics based modules, it has a lot of automated analysis and machine learning happening in the background trying to estimate optimal paths, fuel usage and many more. 

No subfield like computer vision , natural language processing can alone completely solve the problem of intelligence and replicate human level intelligence. 
Which is the primary goal of AI. AI as a term had a lot of ambiguity.  Using specific sub fields and tasks to better indicate what these algorithms are learning and doing specifically is a much better idea. 

Anybody who just refers to something as AI and is unable to breakdown that system to you in terms of the tasks/algorithms at play doesnt know whats going on clearly and you should go elsewhere if you are really interested in knowing/learning.. Yes , thank you for that explanation.. You can do computer vision tasks with only geometry. I don't see how that is AI, but I would agree there is a ton of overlapping concepts.. *black collar. And that's what they call *The Art of the Deal*. Have you heard of android? Its this hot new thing.. I'm just curious as to what work you would imagine politicians to be expected to be doing? I would have thought most of it is reading, coordinating, writing emails, replying to constituents etc. None of which particularly require applications that can't run on phones. Yeah a demographic it's is almost entirely email, IM, and calls.. 85% seems to be the confidence that this is a phone. As it probably is with their faces.. Most small business owners I've developed for do about half of their work from their phone. Communication is easily 80% of the modern business model.. That's quite the assumption. You do realise there are other jobs than what you presumably do for a living right?. General elections are separate issue.

With elected politicians working in parliament, there is absolutely no need to have anything else than electronic voting with instant voting result. Anything else (like the British lobby-system) is simply waste of resources.. Nice. [removed]. Debates are fairly useless I agree. But if you are going to get royally paid to sit there, the least you can do is actually follow them.. [removed]. You seem dedicated to not understanding what's being said

> So now we are supposed to control the bias in human interpretations?

No, nobody said anything similar to this. Yes, but our definition of smart has shifted. That doesn‚Äôt mean that a smart system couldnt have a simple naive bayes integrated within its routines. If anything that demistifies how human intelligence works is ridiculous to you, fine. I see little reason to challenge your view.

And yes, learning by example and changing internal information, while not exclusive for humans, is a pretty efficient way to learn. Nothing wrong with and nothing warranting its dismissal.. It's very weird you'd mention linear regression because it is metaphorically an atomic unit of the field of AI.. Thank you for this parallel.. Or they‚Äôre just management, and will regurgitate any buzz word if it makes your product shinier. [Note: there are many versions of what the goal of AI should be and various perspectives, dont want to get into that here. Also, I personally dont think AI's goal needs to be about replicating human intelligence exactly the way it is , it could find a different way of meeting similar objectives. ]. Android is an open-source project, yes, but the devices themselves are generally locked down, undocumented, and depend upon closed-source drivers and firmware unavailable to the public. You can't really just install your own OS on most of them, and the custom OS builds you can get are device-specific and usually buggy. That leaves you at the mercy of the device manufacturer when it comes to your OS, user privacy, updates, the things that ship with it, the things you can't uninstall etc, which sucks imo. Some vendors are better than others, but none of them come close to the level of user control that PCs provide, and the idea that these devices will replace the PC scares me.. I agree Android is great for this. Android is far from a walled garden. Mobile phones have taught us how to make the personal computer better. The freedoms and openness often come at a cost of security. Mobile devices forced software into sandboxes with elaborate permission systems. They introduced App Stores which allows software to be vetted before it is available to user. With Android, these securities are optional. You can always install a different App Store, root your device, etc. Under the hood, Android uses the Linux kernel. Apple and Google secure everything out of the box which has genuinely been very good for the user. You may be able to differentiate between the real Spotify and the one that a malicious website linked to on the web, for instance, but sadly many, many people can‚Äôt. The biggest challenge now is to provide anticompetitive legislation to prevent the makers of the hardware and platform software from being able to suppress freedom of competition.. But the interface is horrifically inefficient. Using smartphones makes sense; when laptops are not portable enough. I don't see how they're not portable enough there.

It's a transitory phase anyway tho; AR glasses will replace both laptops and smartphones. Hopefully in next few years, through progress is excruciatingly slow.. lol no, chatting on your phone isn't a "modern business model". It isn't and there aren't.. You would still need to confirm the results by having everyone agree they voted accordingly and everyone else agree as witness.. Debates are not for the benefit of the politicians, the actual debates happen behind closed doors (perhaps even in phone chats?). Debates are meant to inform the general public of the stances of a politician. There really is little reason for the others to listen to them.

Again, the main issue here is that we're judging how good politicians are by "butt in seat time" rather than how good decisions they make. Focusing on butt in seat time will give you as good politicians as focusing on lines of code gives you good programmers.. [removed]. You're shifting the goalposts. I'm not talking about the term smart - that's another nebulous term with no hope of a workable definition. I'm talking about the term human-like, which is what I originally responded to and what I take issue with.. You don't have to convince _me_. I'm just talking about the norms of the field. Try submitting a  conference paper that likens naive Bayes to human intelligence and see how the reviewers receive it. If they agree with that assessment, then I'll eat my hat.. Yes, it is an atomic unit of deep learning. So what?

A cell is an atomic unit of a brain. Does that mean that a single cell has human-like intelligence? But who am I kidding - you guys probably _would_ consider single cells to have human like intelligence. Bacteria react to external stimuli by modifying their internal states, and according to you guys that is the definition of human like intelligence.

The pseudo-intellectualism and AI mysticism in this thread is rising to absurd levels. I'm done. I'm out.. Yes that is true. Also sometimes in product descriptions or pitches , the details may not be very important so instead of describing something properly just say AI and get people intrigued.. I agree with you especially because android is (mostly) open source as opposed to most PC OSes. The good thing is that people who arent as tech-savvy have the option to use a secure app store. You dont really have that option with windows (windows store is a disfunctional mess). Texting and coordinating and reading documents and articles is extremely efficient on a phone, wtf are you talking about?. Advertisement, sales, stock management and display, and a dozen other forms of information distribution are essential to any business. As those are no longer done person-to-person by most businesses that lack a brick-and-mortar sales point, they're done via phone or computer.

A business is literally defined as an organization that creates and delivers value. The creation tends to be the easy part for most, which is literally why I have an income stream, myself. I streamline the communications and delivery process for them - allowing them to do everything from their phone.. Without voting secret, that's not an issue. With the fact who voted and what, it's easy to verify the vote is correct, as in, if my vote were shown incorrectly, I could easily notice it.. Yes, it's not a reflection of good nor bad policy. But it shows a clear lack of respect towards the speaker and the public.. [removed]. You're having such an odd reaction to the definition of a technical field that it's kinda shocking. This is a Machine Learning subreddit, you shouldn't be aggressive about people posting about the definitions in it.. It‚Äôs maddening. For years my CEOs been hawking our platform as some AI magic bullet and all I can do is sit on the sidelines and play out the Emperors New Clothes.

What‚Äôs the most complex system you‚Äôve built?. Yes, and sometimes walled gardens turn into nice places too. I love the Android project, but I have gotten roped into the apple ecosystem over the years. Their stuff just works exceptionally well together on everything. The convenience of getting my texts/calls on my Mac, my AirPods transferring automatically between whatever device I‚Äôm using and Apple Pay which I use for everything but Amazon makes life a bit more convenient. I absolutely think Apple overcharges for hardware and maintains an ‚Äúexclusive‚Äù attitude that I don‚Äôt like. I also think the App Store restrictions can become unfair, because their is no alternative on iOS and jailbreaks void all warranties and protections. 

That said, people forget that Apple has been a very large contributor to open source over the decades. They‚Äôre WebKit rendering engine (the backbone of Safari) was forked by Google to create Chromium. The Apple operating systems were largely based on free open source operating systems, like FreeBSD. They rejected Linux/GNU stuff b/c it has copyleft licenses. Those licenses force apple to make all of their derivative code open for the world. (Virtually all companies hate these licenses and they‚Äôre not really ‚Äúfree‚Äù in my opinion‚Ä¶ they force you to follow their definition of ‚Äúfree‚Äù which imposes more restrictions on your use‚Ä¶ making it a form of bondage). They also regularly publish parts of the iOS and macOS software at https://opensource.apple.com. 

I think everything in life has pros/cons. We just need to make sure the little software guys aren‚Äôt destroyed in the process. But mobile has made so much better. It even kickstarted new economies and sped up standards for the browser!. It's like people are expecting them to be writing unit tests or functional requirements  or something. Regardless of message length, you're gonna write it faster and with less effort on a full scale keyboard.

As for reading documents, yeah, smartphone can do it on par with laptops. But navigating to these documents.... They really aren't though.. Yes but you would still need to go through the votes and have everyone explicitly agree that they voted accurately and in sound mind while at the same time having everyone else confirm that you confirmed your vote so there's no way you could lie about it later of have any ambiguity in the process that could trigger a re-count.. [removed]. Im not here to endorse myself or my work.. Yeah fuck the little software guys! Fucking small people with little masks and pointy hats stealing my trash... smh. Boomer with sausage fingers foud. Imagine carrying your laptop around everywhere all day. Please explain.. I've been talking specifically about voting in parliament here, not general election.

There is no issue with electronic voting in parliament, as there is no voting-secret, and there are, for example, [this](https://images.almatalent.fi/cx55,cy0,cw889,ch666,570x/https://assets.almatalent.fi/image/5b6c2434-9e36-5c7c-b7aa-328fc0e83379) kind of boards around the building. That's voting result instantly showing in real-time. If somebody notices their vote is recorded wrong, they ask the speaker of the parliament to fix it before it's made official. After that, there's no changing the result.

There's absolutely no need for others to confirm your vote, as you can do it yourself. You cannot lie about your vote as it's public on the display. If you're not in your sound mind, well, there's a party for the crazy people.. [removed]. *arent* ‚Äî autocorrect removed my *t*.

> we need to make sure small software companies **aren‚Äôt** destroyed in the process.. It's not just enough for the voter to be validated that they voted properly, everyone else needs to verify that the voter is in agreement they voted properly and they do this by going manually slower. This is so no issue after the fact can be raised as to claiming there was a mistake or mixup after the vote has officially been cast because someone who who may want to halt a bill can claim after the vote that there was a mixup or that the results were changed after the fact. By going slowly and confirming each vote individually it eliminates this problem by forcing all participants to explicitly state there intent and have full attention on it while it's happening.. [removed]. You should really look up how it works in places where electrical voting is used in parliament. None of what you point out are issues, as because, there is no voting secret. Lack of voting secret voids all the issues you raise.

>This is so no issue after the fact can be raised as to claiming there was a mistake or mixup after the vote has officially been cast because someone who who may want to halt a bill can claim after the vote that there was a mixup or that the results were changed after the fact.

There will not be a mix up, as after the voting is closed and the official count is in, there is no changing of the result. By definition, there won't be mixup, as there cannot be mixup, as the vote is final.

>By going slowly and confirming each vote individually it eliminates this problem by forcing all participants to explicitly state there intent and have full attention on it while it's happening.

I'm not elected official, but I can assure you, if your only job is to press one of four buttons [(Present/Yes/No/Abstain)](https://img.yle.fi/uutiset/politiikka/article9617353.ece/ALTERNATES/w960/LKS%2020170419%20%C3%A4%C3%A4nestys%20eduskunta%20%C3%A4%C3%A4nestyslaite%20nappi%2038858586.jpg), you will have full focus during those 3 seconds it takes you to press the button. If you press the wrong one, you have multiple screens, including one right in front of you, telling how you voted, and you can ask the speaker to change your vote after you've voted but before the voting time ends. Also your colleagues can tell you if you made an obvious mistake, if there's for example a party-line vote.

The slowness of vote has no upsides. Quick electronic voting has absolutely no downsides, if you exclude power outages, but those are migrated by the parliament having its own backup-generators and still the vote can be done with row-call if the voting system is down.. [removed]. Than why do they continue to do it this way if it's such an objective and obvious waste of time? Do you think every single elected official in that room enjoys sitting there and waiting for it to end?. [removed]. That's the point, in many places electronical voting in parliament is the norm.
 [Here](https://youtu.be/vLVOPZzWUcI?t=88s)'s an example of full parliament (200 representatives) doing a full vote in 15 seconds without any issue. Everybody can confirm from the separate (from the number-displays) led-displays on the sides of the podium (which you can see at 2:00) that the numbers are correct, as those show every induvidual representatives voting decision by their seating position (the seats are personal and require authentication). During covid, also completely remote voting has been allowed for representatives in this particular parliament, but also even in UK representatives can now vote remotely, but still they have the lobbys locally.

So where electronic voting isn't the norm, it's either the politicians aren't willing (tradition) or don't  know better.. [removed]. [removed]. [removed][D] Types of Machine Learning Papers. nan. Everyone trying to squeeze out final drops from our poor cow.. The "We proved a thing that's been known empirically for 5 years" paper is really usefull tho. It allow you to have a solid justification on your use of that "thing" in your/all next researches.. "We rediscovered something known 30 years ago and we didn't cite it". Stop requiring your PhDs to have 10 publications before they graduate and half the issues will solve themselves.. That is why I read conclusion section first and most papers fail me there. As a biologist, this is most papers in that field too.. The unreasonable effectiveness of cliche titles in papers is all you need.. Hahahaha. 

Bottom left corner is why I left ML reasearch. What was ridiculous was CVPR actually accepting the <1% improvements.. As I am always saying: No papers about ‚Äûwe used hardcore mathematics and developed a new method‚Äú.

Oh just saw that in line 3, column 2 there is the kind of research which goes into that direction.. The Lego bit had me in stitches.. has anyone here ever wrote a paper? I'm supposed to write one for my university and I've never wrote one before, like what tools do you use to get the pdf and what other stuff should i know?. The Lego block paper is one of the reasons I decided not to do a Phd. Thank God that the scientific method can be applied to computer science. ü§™. [deleted]. Lego block paper writer here, tru tru

admittedly I'm a soft.eng. with chem flavor phd that uses machine learning not a researcher in machine learning. >\[...\] this time, I swear

This one got me good.. This is a very depressing testament to the state of the field. It's all true.. You forgot the GAN puns.. Credits: Maxhkw (Twitter). ‚ÄúHere‚Äôs another game that we ruined‚Äù. I see a lot of comments talking about all the short comings of ML. How there are too many people in the field, how there are not enough, too many graduate students, requirements are to strick or too lenient. 

As someone who is about to enter grad school, in ML, and who is committed to the idea of being apart of this apperant broken machine. How can I be apart of the change that results in something better? Sure, read more papers, be better at research, be more creative, blah blah blah, descriptors that are easy for the experienced to understand and impossible for the young and learning to interpret. 

The reason that science seems to be only nudged by the many and truly pushed by the few is because, in my opinion, success is hardly documented and faults and critism are plentiful. I think if more were willing to mentor, teach and share then we could see more progress. I know I could be better.

Finally, we need a less hand wavy approach to learning how to research. The best I have learned about and getting a mentor, hoping he/she will take you under their wing and emulate as much as possible. Research shouldn't require a parent. I don't have a better solution unfortunately but i wish there was one.. What is AGI?. as a phd aspirint this frightens me to the core. The resources these researchers consume to do 0.1% improvement is ridiculous. Like, have they even played Minecraft in their life? 

&#x200B;

Limited resources, limited life, limited money. Optimise and use wisely.. Isn't one problem that students have to write a couple of papers in their educational career and there is only so much groundbreaking research possible at a time?. I feel attacked. Grear meme that requires actual research experience to make.. Lol. Since when did the sub start accepting memes??. i hate the ones that begin with "towards..". why the tf would i want to read something that's incomplete?. You forgot the usual "Here's a theory with no applicable results, nothing to prove that it's true or that it works, but now that we've done this we hope someone will do all the job to prove we were right". [deleted]. God this is so accurate. I love it!. Missed opportunity for a Schmidhuber meme.. I love every one of these, but the deep learning one hits most home to what I have to deal with . ;). Omg I love this! Hilarious and a bit accurate on the literature survey!. Which type do you like the most?. bahahahha fucken oath. Did you guys hear that in a medical journal someone figured out how to compute the area under a curve?

https://care.diabetesjournals.org/content/17/2/152.abstract. In [Machine Learning](https://www.tibacademy.in/machine-learning-training-in-bangalore/), we have lot to learn and some of the papers had shows here. Keep up the good work. Nice one!!! .. i have been time and again fooled by "*we have figured out how deep learning generalizes this time, i swear*" only to be fooled by another similar paper...This really wracks my brain...and makes me question myself. We invented <<replace this>> in our lab in 1991.. BS++:We add a small bs on my old bs and the accuracy improves 0.01%. >Everyone trying to squeeze out final drops from our poor cow.

To be fair, in many applications the last drops are the most important. An algorithm with a 99.8% accuracy sure sounds really good and seems like it would be useless to improve, until you realize it's for a self-driving system and the alternative with 99.9% has twice the survival rate for it's users...

That 0.1% reduction in the error rate, I'd consider a 100% improvement.. Lol

But there still a long way to go!. Yeah was about to say, I know this is a meme but proving something only known empirically counts as a really good paper by any metric (and in any field for that matter). The dropout as Bayesian approximation paper (which I guess should fall in this category?) is by far my favorite paper because of this. >  It allow you to have a solid justification on your use of that "thing" in your/all next researches.

I'm not sure that this is quite fair. Other sciences get along just fine with empirical evidence. Why isn't empirical evidence good enough for machine learning?. I agree but it still sounds funny ;). No one said all ML papers wore useless.. Which, when, true (that it was a rediscovery and they hadn't been aware of the prior work) isn't even necessarily the researcher's fault. That's more on the hands of the reviewers. Different sub-field use different terminology. It's so easy to miss something when hundreds of papers are put on arxiv every single day.. Found Schmidhuber's alt.. *Hochreiter and Schmidhuber have entered the chat.*. [deleted]. PhDs require 10 publications!?. Yes, exactly.. Hey spotted you on YouTube pal, love your channel!! You always ask fun survey questions too :). A very smart move, full respect on that,. For me it is the reviews where applicable.. This is science in general, better than that, it's how exact sciences progress, measurable, reproducible improvement, even if extremely small, each person makes a miniscule dent in the bubble of human knowledge, not sure why people are expecting different stuff from the ML field. You should check out the [original](https://xkcd.com/2456/) of this image then.. May I ask which point you directed towards after leaving?. On the other hand, an improvement in accuracy from 50% to 75% is as impressive as 99% to 99.5%?

Both halves the number of errors. Imo no single research paper can make a huge impact on the field, it takes years of work.. That's funny, because industry (at least big tech) is all about making 0.1% improvements to ads models to bring in hundreds of millions of dollars in revenue.. "A Category-Theoretic Framework For Deep Neural Riemannian Image Classification"

&#x200B;

\>> 0.0001% improvement on MNIST. Overleaf. If you're not afraid of scripting, I highly recommend using LaTeX, it takes care of all the boring stuff, like formatting, naming/referencing figures, organizing citations, etc... There's a bit of a learning curve but it's really worth it. Here's a tutorial but there are tons of other resources https://latex-tutorial.com/tutorials/. Define where you want to publish and get an appropriate LaTeX template from the editor. You will need a few weeks to learn the basics but it is super robust. Different journals have different standards. If you have equations or other special formatting it is much easier. If it is only text and you are short on time stick with Word.  
I would also advise to work with a reference manager like Mendeley and import the papers you want to cite.  
Here is the scheme I teach students: 1) Write the methods section as you conduct the experiments. You will not remember all the details in six months. 2) Collect the results, finalize the main figures, write their captions, this is the main meat. If I have only 5 minutes to check a paper I will look at the first few figures. 3) Write the results section around the figures. Try to tell a story. 4) The methods sections is already mostly written. 5) Write the intro and conclusions. 6) Write the abstract, find a good title.  
You'd be surprised how many students start a paper with the title, then the intro, then the results, and then try to illustrate their points with figures.. Always use latex.most conferences and publishers have templates to work with that will let it look excactly like real papers. Word of advise, most universities have their own templates (the one I use in teaching is based of ACMs). Read closely related papers you'll get the feel for how to structure your work and how the specific questions are answered in those paper. It is very important to properly structure your paper(experimental design, related work discussion, future directions etc.). I personally love Mendeley for reference tracking. It can integrate with Overleaf to generate your bibtex file automatically, and it has a browser extension so you can save citations on the spot when you're on a web page or reading a paper.. I would use LyX. It's similar to LaTeX but much easier to use especially for beginners. It has a really good GUI-based equation editor. I'll probably get downvoted by the "I learnt the hard way and so should you! Only noobs take the easy route." brigade but it's genuinely better than writing equations in LaTeX.

The bibliography is always the awkward part. I used to use Mendeley but they got bought by pure evil so I don't know if it's still any good.. You're going to get a lot of advice to use LaTeX, and if your paper as *a lot* of equations it might be useful, but otherwise I would check with your supervisors/lab before going down that road.

Word can be a pain sometimes, but it's not as bad as people make it out to be, and you need something that others can read and contribute to.

I wrote a lot of my PhD work in LaTeX, and while I liked it for my thesis, is was really too much work for papers that I did with others.. Latex is bad but it's better than all other alternatives that have been tried. [deleted]. I totally get what you mean, but I do not think it should be expected of PhD students to come up with anything truly novel, especially if you do a PhD straight after a Master‚Äôs degree. It is very rare that people come up with something new at this level. The lego block paper at least makes you a specialist in a given area, which can pave the way for novel discoveries in the future.. Newbie here. What's the Lego block paper?. I‚Äôd guess the first is referring to the deluge of titles that play on a famous title (Attention is All You Need). The second is probably papers claiming to have solved a hard real-world problem (AGI) by experimenting on a toy problem (grid world being a simple reinforcement learning environment).. 1. ___ is all you need. Overused paper title in the last few years. 
2. Grid search on hyper-parameters(?). There is something to be said for applying techniques to a new field. It's not easy to go from playing Atari to folding proteins but you can definitely call it lego blocking.. Don't forget to also credit Natasha Jacques (@natashajaques on Twitter) -- she's the first author on this particular publication!. *FTFY: https://twitter.com/natashajaques/status/1387859601555554304?s=21. yet this felt like something straight out xkcd. This sub is so sexist. So classic to steal material without a proper citation (link to tweet)... and then only credit the male second author and omit mentioning the woman who actually came up with it :/. The reason academia is like that (some fields better, some fields worse) is that in order to get funding you need metrics that show to any grant, university, program etc committee that you/your department performs well (or even better, outstanding). That is why 5 crap papers is better and safer (and easier to produce) than 1 outstanding work, because it will be easier to convince people that decide who gets the money (and often have no idea of the subject, even the field) with quantity compared to quality. And going for the 1 outstanding work is also very risky, no one guarantees you will get something out of it. It's always about the money, especially in fields to close to the industry, like ML.. Artificial general intelligence. Basically an AI that can think and function like a human. have a doliprant :). When you're wandering in the dark, any sense of direction is welcome.. I dunno, I appreciate honesty in this way. I read it as "I'm onto something... wrote it down, if you figure it out first you should write it down, too". I read this like ‚Äúremote viewing‚Äù, and the feeling was awesome. LOLLLL. Except, since we're suffering a reproducibility crisis, that 0.1% might not mean all that much. Base on my own experience, sometimes you can get this  0.1% reduction just by using another randomize seed xD. If you grind the cow's bones to powder and stir it into water you get a bit more milk. Have we tried milking... goats?. I would love a link to this if you could provide.. sounds super interesting..  It turns what was a useful engineering hack into proven science that explains not just why it works, but hopefully also allow us to make useful projections and  predictions. Such as determining what other situation the technique would or would not work, make predictions and further improvements.. Because empirical evidence can still be wrong and not apply to certain situations without you knowing it.
Proof is more solid, simpler in a way and works better as a building block.. Sometimes, empirical evidence is good, but proof makes it better.

Proof ensures that the theory is applicable across a full domain. It isn't just a method that could fail or something anymore.. Are you the guy who stood up in front of everyone at NEURIPS the other year and told the paper authors of the "best paper" that what they proved might not be true because they hadn't examined all the datasets....?. I‚Äôve seen a similar thing recently with patenting. There‚Äôs something like 2k patents submitted to the USPTO every day, and with just over 10k employees I really doubt they have time to do a proper prior art check.

In patenting though, there‚Äôs are mechanisms to reverse / alter patents after the fact. In publishing, once the paper‚Äôs out and has seen enough citations, there no incentive to make any corrections.. Also, implementation matters a lot, especially in an empirical science such as AI.  
Some ideas turn out to be good, some don't. 

If credit should be assigned to the idea, so why bother to spend months on the implementation?. >	Which, when, true (that it was a rediscovery and they hadn't been aware of the prior work) isn't even necessarily the researcher's fault. 

Or even necessarily the proper thing for the researcher to do. Researchers should cite the work they directly depend on. If there is related work from a few decades ago they‚Äôre not aware of that‚Äôs not something that should be cited.. I apologize, I didn't see this before I added my redundant snark.. Back prop and GANs, probably?. Ehh it varies from field to field and university to university. I'm being hyperbolic here, but honestly not by much. My advisor wanted me to publish at least one conference and one journal paper for the 3 years of my PhD, which could have easily ballooned to 5 years.

Instead I got a job at a startup and never looked back.. Hot take: science is broken, 99% of papers in any given field are shit, academia functions largely as a make-work program for graduate students.

I'm not sure how we can do better but what we have right now didn't age well.. In general most studies are wrong or a waste of time. Then one study comes along that blows everything wide open. The recent example of this in bio is CRISP-Cas9 papers that allow us to edit a genome anywhere relatively easily. It came totally out of nowhere.. In my experience science *doesn't* progress by lots of people making miniscule improvements. It mostly progresses by a few people occasionally making big improvements.. The insignificance. How does improving 1% help at all? Figured I'd build ML products that help people directly.

My research lab would clobber up novel ways that don't make sense just to appear novel.. Godfather et al., 2012 beg to differ.. There are always exceptions of course, transformers did. GANs, LSTM... Depends on your field. I worked in neuroimaging with ML, and the top journals didn't use LaTeX unfortunately. My lab / PI used Microsoft Word .... Much to the chagrin of our mathematician.. i wrote my undergrad thesis in latex using overleaf + Mendeley with near zero prior experience with latex and it was an absolute breeze. Figures, section references and citations are so much easier than Word.

I'm absolutely never going back. Yep. I worked at a hospital lab doing ML for neuroimaging. When your PI is a neurologist, you're probably going to use Word. With much complaining from your mathematician.. LaTeX is what I've always used and I think it's a great system. Curious as to why you think LaTeX is bad? It has a pretty steep initial learning curve but I think overall it's a great tool.. Good luck :). It's when you take 2 models, stick them up, and tadaa, you say you have a new model. For instance you take Rnn + Transformer encoders and you say it's a novel model lol. To be fair, there's nothing wrong with lego blocking in general. It's lego blocking which doesn't contribute anything that's bad. 

Like you said, if people lego block and it actually does something useful then it's good. Isn't this Schmidhuber's meme?. Hah it looks like they edited one of his comics, the original is pretty good too https://xkcd.com/2456/. Thanks buddy!. i prefer to consider taking direction from someone who has confidence they know the way.. Every paper should come with code. It might feel embarrassing if the code is messy but we all have our flaws.

Also: please for the love of god use already known theorems, like all the classical stochastics. We have proven optimal solutions for some topics. Bake them into your nets.. This 1000%. And beyond reproducibility, 0.1% that does not generalize is noise IMHO.. The most important hyperparameter to tune /s. dont forget about leather. you can converge on any drink if you add another head in a siamese setup.. It‚Äôs a very famous paper so you should definitely check it out! https://arxiv.org/abs/1506.02142. Yeah, but empirical evidence is still solid justification. You don't need a proof.. No, I'm a guy claiming that you can have "solid justification" with just empirical evidence.. I think part of the problem with patents is that philosophically, I think they're more inclined to make that sort of thing the problem of the involved parties to figure out post-hoc through lawsuits if they care enough. Does the USPTO frequently reject patents because of existing prior art discovered by the independent research of the approving patent officer? I suspect that sort of rejection is rare, and probably usually comes from external parties issuing complaints to the process. But I don't really know much about patents. Interested if someone with experience here could chime in.. You Schmidhubered me!. you dropped out? If so, do you plan on ever finishing?. [deleted]. yeah - It's the quantity over quality issue.  Lab's are expected to publish frequently, and so it's actually a huge disadvantage to go after really difficult problems and take the time and effort to advance the field.  You can go after low hanging fruit, and get a resume that gets you future jobs/grants.  Or you can take a moonshot and if you miss you basically have to leave the field because your CV won't have enough papers.. What would a better system look like? Maybe not the exact specifics but general feel. Shows your lack of experience. 99% of experimental sciences consists of gathering more data from experiments, each data point a minuscule improvement over the status quo. 99% of theoretical science is just useless garbage. A tiny, mostly lucky, minority performs breakthrough experiments and creates new fundamental theories.. Correct. Only few make a big dent while the other leave their slime of mediocrity all over the research.. I think Panzerschiffe's asking what you gravitated towards, after leaving ML research.. It's the longstanding downside to "publish or perish". Find the best seed, make unreproducible claims, and compare it to the worst baseline model. There is so much pressure to show positive results there is a huge publication bias type problem in the industry.

Hell, a decade ago we just added more research participants until we got statistical significance, then threw those numbers into our paper and send it off. "Do the results mean anything though?" earns you a slap on the wrist. I'm sure there's more to that, but 1% improvement when the best model is already at 97%, in some industries can be the difference between applicable and not applicable, the difference between "Hey that's neat!" to "We can use that!".

And that's not even talking about the iterative nature of improvement, you're disillusioned with the scientific process as a whole it seems. I mean if that means 98% to 99% you have just cut the number of incorrect predictions you‚Äôll make in half, that‚Äôs kinda something.. Saves 1% more lives is pretty good, though.. The transformer paper was an important step, but it's still a natural one given the preceding work on attention. LSTMs feel similar, but it's hard to comment now on the research landscape in the early days of Rnns. GANs also didn't happen in a vacuum,  VAEs were already around and share some structure.

Of course all of these are still important papers, my point is that there was a sequence of work building up to each of these.. Good lord, at that point I'd tex up the math and send over pngs. The next step is to write up some functions in your favorite programming language to dump arrays to text and auto-format your images :). [deleted]. It's not bad as in unusable, but rather there is a lot of things that could be better, and better suitable for most use cases.

- Package management is awkward
- Syntax is often unintuitive (texttt lmao) or overly complicated for simple things
- It's overkill for most purposes, and does not scale down well

C is a great tool too but you wouldn't want to write your scripts in C.. Indeed. If anything, we probably need more of that. There's thousands and thousands of ideas that claim to be improvements in some way, but which have never seen any use whatsoever beyond their initial paper. There's probably way more value in figuring out which of them combine together to actually perform better (and, if possible, try to find some sort of pattern to what works and what doesn't) than in coming up with 1 additional "improvement" to add to the bottomless pile.. > Maxhkw 

http://xkcd.com/2456 but for machine learning :D. "Towards" means there's evidence that this approach holds promise.  It doesn't mean that's the answer or the end-all-be-all.  You don't get papers like that very often in any kind of science.  With something as challenging as AGI, for example, anyone that says they know with confidence where the answer is is either lying, mistaken, or using a definition that's off the mark.

"Towards a unified theory of gravitation" might be "incomplete", as you said, but that's like saying, "You only have one Michelin Star?" Or, "You only have one Nobel prize?" It's technically accurate, but feels rather pejorative to me.

It's probably obvious that I'm salty, and maybe I'm taking too personally something that was offered in jest, but I hope the grievance about the use of "incomplete" isn't entirely lost.. What do you mean by bake them into your nets? Can you give an example?. I absolutely agree.. Second that.. A neural network to choose the seed ü§î?. Thank you!. There's a difference between "this is BatchNorm, it works and we think it's because *handwaving*" and "This is why BatchNorm actually works". You really think the latter isn't an important paper to write?. Justification should explain *why* something works. No amount of empirical evidence can give that.... I gave you gold for the express purpose of writing a lengthy diatribe about how you don't deserve the Gold Award, in which I will cite my previous diatribe about how you didn't deserve the last gold you received, either.. "never looked back". Grad student descent. Another hot take: because the incentive structure in academia is all wrong, the next big thing is going to come out of corporate research. Or we rediscover, that tenure is an important building block if you want to nourish long term, risky research (that might lead nowhere).. I'm not sure if a lot of decent searchers can actually manage to beat a small group of excellent searchers.... Bit harsh. Not really researchers' fault that publish or perish exists.. Oh, startups. >Hell, a decade ago we just added more research participants until we got statistical significance, then threw those numbers into our paper and send it off. 

That would actually lead you to a (probable) verifiable result though. I think it'd be more accurate to say: keep trying new pilot experiments until something reaches threshold significance or just above whatever metric is deemed relevant. Aka machine learning p-hacking. And if you're really desperate, automate the entire endeavor by grid searching over hyperparameters. Now you've also got a separate methods paper as well.. I agree. The easiest way to get out of this is join a good lab. Better advisor, better peer group and you work on rewarding research. Now there are usually 2 paths to this (1) You either end up in the same school as the lab (most US/European undergrads -> MS/PhD) and you already have a good rapport with the advisors, (2) or end up working hard af to get an MS or a PhD (like most immigrants). 

I felt the effort to get into a top school wasn't worth it after seeing my peers and seniors put so much effort. 

I know a guy who worked as a masters graduate for 2 years getting paid 1/15th the salary of a normal software grad to get a couple of papers in CVPR. He's in VGG now at Oxford doing a PhD (so good for him), but thats such a hard fucking path with so much sacrifice. 

Life is more than research. And you need a decent amount of capital to actually experience these things and do what you want.. Ah 1% improvement in an experiment does  not have to result in 1% in the real world. Those are not the kind of improvements industries with that requirements are looking for. 1% is mostly meaningless without confidence / robust error modelling in these settings.. 33% fewer errors is a significant advancement.

However the question is whether it's 33% on only this specific testset, or in general. "97%, in some industries can be the difference between applicable and not applicable, the difference between "Hey that's neat!" to "We can use that!" -> What industry? What applications? Do give examples. After 95% nothing really matters. I know because I've worked on, at and with a lot of ML startups and applications with clients. 95-99%? Now that's a good jump. Maybe not so much practically (clients usually don't care) but it shows you're the best. 

"You're disillusioned with the scientific process as a whole it seems"
Hmm not exactly. There was a point I was really into research and experimentation but quickly realized I don't have the patience (to wait a few years). I like things that move faster.. There is a huge gap between attention and the transformer I think. The transformer architecture is very complicated and specific. They must have tried everything.. "Neuroimage" requires all equations be submitted in an editable format. Same with tables.. \\begin{center} \\end{center} or \\centering doesn't work?. We all have our own preferences. If "towards" doesn't bother you, that's fine but I hate it. Maybe incomplete was not the right term. Instead of towards something you don't have, then use a title for something you do have. Don't waste my time with your BS god knows there's enough of that in ML papers. Just my 2 cents please don't take it personally. =). I started with machine learning again and have to work with normalizing flows. I actually quite like that one since you use invertible functions. This is only basic math and nothing fancy yet but I like the idea.

Since I come from audio: E.g. expressing the Yule Walker equations as a net. Not directly the solution, just the beginning. Basically doing a LPC analysis, just baked into a net. There is a paper called lpc net but IIRC they used a lpc as basis and try to improve the estimate with a net.

Since I forgot to explain: LPC means linear Predictive Coefficients and estimates a optimal solution to a stochastic signal, with witch you can whiten it when using the resulting coefficents to filter the signal.  The whitened signal also has less energy. I think something like this can be interesting since it has known optimal solutions and works. The GSM standard uses exactly that for voice transmission. Just with a little more engineering added to make everything stable and sound nicer.. That won't do. How do you initialize this network then?. > You really think the latter isn't an important paper to write?

No, that's also important to write. But my point is empirical evidence is still justification.. You can have reasonable explanations of why something works without proofs.. I respect it. Help me step advisor, I'm stuck. Corporate research. You got that right. I was reading tesla's patent and those people are mad geniuses.

A vision, good engineers and scientists, focused goals and implementation...and ofc big daddy money - corporates provide all.. [deleted]. Ensembling weak searchers. I am sorry, I guess I need a scapegoat, someone to blame on. But yes you are right, its not researchers' fault.. They pretty much have all the same problems, except on steroids. And with an added effect of personal connections being central to the whole enterprise for further good measure.. Most significance tests assume that it's a random sample.  So if you just keeping adding more datapoints and repeating the test until you hit significance, it's analogous to p-hacking as you're guaranteed to hit significance at some point even if simulating under the null model in this case.    There's probably a way to correct for this (not multiple comparisons exactly, but something like it), but I'm guessing they deliberately didn't do that.  It's the kind of thing that's easy to get away with because unless you publish your experimental design *first*, there's no way to tell that's what you did in the end.. I went to a big shot school for cognitive neuroscience type research (WUSTL), and the advisors said to everyone, what do you do if your initial hypothesis fails to deliver? Retroactively find another hypothesis that does work. It's like an industry secret. 

Also everyone knows whose lab is submitting an article to Nature and the concept of "blind review" is not a thing. It really showed how the sausage is made, so to speak. They'll know whose lab it is and you have to play along with the system or you'll get kicked out,

There is a subtle conspiracy of 'just dont say it out loud and we all look good' undercurrent to the publication and review process. The idea of blind review adds credibility but it is kind of just for appearance, It's the same with machine learning research, too. "Don't talk about it you want to succeed" kind of thing,

I might be putting it in an overly cynical way, but there's some truth to that. Same goes for a lot of academia, and it is not at all just limited to ML or neuroscience.. In many ways minor improvements can be helpful when using an actually new technique and it is shown that the improvement is consistant on multiple datasets. What isn't useful are the 'hyperparameter search' papers that show improvement or when they try it on multiple datasets, but only publish the results on the dataset that actually showed improvement while ignoring that it performed worse 9/10 times. The first is just useless, but the second is actively harmful.. > after 95% nothing really matters

In my experience, there is often a threshold around 95-99% in precision where models start to match or beat humans. This is a big deal as you can switch from human review to automated review.. Medical & the automotive fields come to mind, every percent matters. In finance, improvements on the order of 0.1% can still mean monetary differences in the millions.... Let's say you train a model for writing to text recognition and it has 95% error rate. That means every 20th letter is garbage, sometimes whole words. Pretty terrible whichever way you cut it. For print text humans will have 99.9+% accuracy, and anything less is useless.. .... Ew. Ah I see, interesting. So rather than starting from scratch and using the net to predict everything, you start with the classical solution and use the net to predict a correction? Sort of a ResNet-like idea where you predict the difference rather than the whole value. Makes a lot of sense, thanks for explaining.. Just use another neural network. 1 more network, 1 more paper. "Publish or perish" -> solved üòâ. Empirical evidence is.. evidence.. Unless you have a proof which links your reason to your conclusion, you have a hypothesis, maybe even a theory, not a justification.. > focused goals and implementation

I'd argue I miss this the most at my current academic position. Agree with the points here, with the exception that I don‚Äôt think there‚Äôs actually some crazy mental barrier for certain discoveries that only geniuses can bypass. Maybe they‚Äôll do it faster, but as we‚Äôve seen historically, most major discoveries pop up in multiple places at once, as the latest technology has just enabled their discovery.. There are some problems that have moved into the realm of being solvable with recent advances. Then you‚Äôre really working on software more than ML problems, but it has its perks.. There are alternatives to statistical significance such as effect size. Significance is merely having a low <0.05 probability of finding a difference when there isn't one (as everyone here probably knows). It says little about whether that difference is meaningful, so if you are at a p value of 0.07 you just need another round of participants to make it past the precision threshold. You aren't supposed to do it like that, but the enormous pressure to publish and have some positive results encourages it. It is like finding no effect - you cannot get that into a prestigious journal.  Sometimes people just throw out the study and try again. It's also why effect size often goes down over time, as the research record favors the lucky.. When I type "cat" in Google photos on my phone and it shows me *almost* all the pictures I've taken of cats and a handfull of bizarre results 75% is good enough.  When someone uploads a database of photos of known terrorists to be matched by indirect cctv at airports, 99.9% is *not* good enough.. Robustness and solid error bars matter even more than miniscule increases.. I wouldn't take the statement as fact. It depends on the task. Good baseline measures can give a sense of when every decimal increase stops mattering.

And the best baseline, particularly for the examples you gave, is humans performing the same task. Yet this is a baseline you almost never see.. Another example of this is Andy Zeng‚Äôs work on Residual physics for learning dynamics models.. And a lot of evidence is good justification for believing something is true.. A hypothesis can be justified with reasonable evidence, without a mathematical proof.. Been there, done that :(. Yeah, Ive been trying to de-emohasize statistical significance and instead promote effect size and confidence intervals in my work as I think this is more actionable. Oh that's neat.. Since nobody wrote it here: You are missing one important step: a model. If you want to do science, which is literally gaining knowledge, just evidence is not helpful since it does not give you a prediction on outcomes. The whole point of evidence in science is proving (or rejecting) a model with a certain probability. If a model works really well, like e.g. general relativity, you can use it to make predictions. Like rendering a image of galaxies without distortion and double images. You need the ability to make predictions.

Only evidence is good enough for engineering. You just need your stuff to work and don't quite care about why until you have to improve a product. But not for science, which aims at explaining WHY.

Sabine Hossenfelder is a really good resource on science communication if you are interested. Her YouTube channel is amazing.. It _can_ be, but the proof is strictly better. Therefore, that type of paper is useful.. This is a good practive. Everything (well almost) reaches significance if you have a large enough sample.. I'm not missing anything. You don't need a mathematical proof to have a working model. You can have a model and provide empirical evidence that your model works. If you're model seems to work in practice you don't really need a mathematical proof.

And yes I'm a big fan of Sabine.. Well, yeah I can agree to that.. Scientific theories validated by evidence/data should have explainability and predictability. But Deep learning suffers from the lack of explainability and even predictability on all known data. If you can't explain why and when your model fails and don't know how to improve it, then that's not a scientific one.. Yeah but you don't need a mathematical proof to explain something.. If you are constructing a mathematical system from set of axioms, you need a proof to say some statement in that statement is true. Otherwise it is just a conjecture. As you might be aware, there were lot of really good conjectures which seemed true for so many cases we can verify, but didn't end up being true when proved rigorously. 

If you are talking about a scientific theorem for explaining observable and quantifiable phenomenon, it needs to be consistent, validated by experimental data and should have explainability and predictability. If not it is just a hypothesis. 

If you are talking about something else, then you need to define what exactly you are talking about.. Yeah, I'm talking about the second of these. Except I wouldn't say scientific "theorem". I think the term "theorem" is just for math.  I think I'd call that a scientific "theory".  But my point is you don't need a mathematical proof to explain and  understand something.. Typo ofcourse. I meant Scientific Theory. Yeah Mathematical proof is definitely not a must to understand or explain something. But strangely math fits so well with science that now almost any scientist or philosopher wouldn't deny that there is some strange connection between math and the natural phenomenons. It is quite mysterious to be honest.[N] AI can turn old photos into moving Images / Link is given in the comments - You can also turn your old photo like this. nan. This seems like the next museum gimmick where you have an AR app, you point it to a photo and you get the person telling you their life story. Alan Turing. Does that mean we're supposed to find and share the link? Come on, op..... I used some photos of my father and I'm speechless. He passed away when I was just a few months old, 29y ago. This is the first time I see him in motion, blinking, smiling... Thank you so much for this. ‚ù§Ô∏è. [cursed_cristiano_ronaldo_statue.mp4](https://streamable.com/rpxeuf). This feels scary but I would love to give all my old pictures a spin.. https://www.myheritage.es/deep-nostalgia. Just a warning, they will use it for porn. anyone know if github code exists for this? i would be curious to experiment with it. I love it when these things [break](https://imgur.com/a/bDISWZW) in the most spectacular and horrific of ways.. Harry Potter feels

Edit : Context here was that Harry Potter universe has similar moving pictures. [deleted]. Sorry but not any link. where is the link man. Link please?. where the hell is the link?. Colab link please. Thats amazing. One step closer to Hogwarts.. surprisingly poignant. To test it I put in some family pictures of people who are still alive and the results were frankly disappointing, awful even.  Uncanny valley mixed with nonsensical facial expressions mixed with inaccurate facial geometry.  More convincing for people you've never actually seen.  It may look like *somebody*, but not the real person in the photo.. [deleted]. Ahh, yes. Alan Turing, I backward-propagate?. That‚Äôs Alan Turing isn‚Äôt it? Appropriate.. Alan would be happy if he knew. Where! Link?. Where is the URL ????. Will this Turing pass the Turing Test?. uncanny valley presents: .... Reminds me of Harry Potter and the paintings on the wall haha! Pretty cool. Link please?. That‚Äôs Allen Turing he was one of the people who broke the enigma but after the war he got persecuted for being gay because in England it was illegal to be gay and and the made him take testosterone shots because they thought it would make him not gay but it messed with his mind and did not do well for his body and what basically led up to his suicide. Sorry for the long rant. Is that Alan Turing?!!!???. Kinda creepy or am the only one that thinks this.. Anyone gets Harry Potter vibes ?. That's some Harry Potter shit. [deleted]. Can somebody explain me why it is scary??. This reminds of the moving pictures in Harry Potter lmaooo. u/savevideo. https://www.google.com/amp/s/bigthink.com/amp/new-ai-can-create-fake-videos-of-people-from-a-single-picture-2638041019

This is an article with more examples that also links to the paper. Turing would be proud. Very cool work.. Waiting for the first haunted house featuring moving olde tyme photos. Harry: you always can. Givin the ol Harry potter treatment. Who would ever have said that 'Harry Potter' was a science fiction book..... That is creepy af. Looks cross-eyed. Was he?. From first computer fundamental to Artificial intelligence. Bleah.  Saw this in Interview With the Vampire 30 years ago.. Oh pleease, this isnt an "old photo"; its Nicholas Hoult. AI my booty.. [removed]. tried this deep nostalgia from  myheritage serveral times...Just getting errors after uploading a picture. Can't wait until movies of history actually have historical figures in then. That's going to be cool as shit. Wasn't able to cargando una foto for some reason. Saw this on Reddit a few days ago and even though it looks a tad unnatural, it‚Äôs incredible to try(and it‚Äôs free). I‚Äôve never met any of my grandparents but recently got photos of them so I spent the night watching my grandparents faces move through this app. Oh, the app also does a decent job colorizing the photos.. just insane...thanks for the link. I feel like he blinks to much. It was jarring. What's the difference between this and the first order motion model with a black and white filter?. re-animator. What kind of dark magic is this?. Harry Potter-esk. It is eery though. My heart. Well it passes the Turing test, aka the imitation game.. This reminds me of looking at a photo while tripping. straight out of a harry potter movie. Omfg it‚Äôs alive pls kill it with fire. Wow! That‚Äôs insane. Alan Turning. Earmarked. Creepy. I really wish AI would stop turning old photos into moving images. I think I saw this in Harry Potter. Maybe it takes a long time processing but it is amazing.. Wow this is cool!. This wouldn‚Äôt be able to get the correct facial movements and body language of the person though. Kinda ruins it for me. Do not! I repeat do not upload pics of your pets.. This is beautiful. Deserves an upvote. This is some Harry Potter shit.. This is some Harry Potter ass shit. Creepy/cool. !RemindMe 12 hours. Does this work with less than great photos or super realistic art work? I have some not great photos of my great-grandparents and painted portraits of great-great-grandparents with family members that are in stellar to decent condition (not an art critic, but it looks amazing). We think we found some more paintings of older family members but we cant substantiate; additionally the detail and quality is far inferior to the later productions.

I've never met anyone above grand parents, my mother was born to her parents at a late age, so I would love to try to animate my ancestors.. Dame Dane. Where‚Äôs the link OP. What in the Harry Potter is that. That's stunning. Wow.. Thank you for this, truly amazing. well this is next gen future. i will take look on my grandpapa lol.. No github? Some colab notebook ?. Link?. Wow!. Alan Turing woke up from his grave?. There's going to be a lot of racist pictures. I saw the same thing in Harry Potter's newspapers. Hope those muggles cited them.. Omg omg omg !!! Take my cash and i have millions of pics to be done ‚ù§üòçüòçüòçüòç. I think it's possible. I'm working on a project that involves ai searching video based on keywords and would like to invite anyone to help.. Interesting. Thanks for sharing!. does it pass the Turing test tho lol. Hello Harry Potter moving newspapers. Hey, it's some cool Harry Potter shit!. I realized that in Harry Potter this technique is already implemented a long time ago.. Where we can find the code for this to implement it for our own old photos?. This honestly feels like those paintings in Harry Potter.. Can someone explain the exact way to do this please ?. I don't know why but I want to hang this on a wall as a "painting gif". Interesting, what ML algorithm is behind this?. Shut up and take my money. This reminds me of the newspapers in Harry Potter.. Even more creepy thought, imagine having a conversation with an AI impression of yourself, based on all the data about you on the internet. This seems like Harry Potter stuff. A friend of mine is developing an app like that for cemetaries. Approach the tombstone and hear the person's life story as told by friends and family.. [deleted]. Or, it could be used for animating a zoom photo to pretend you're paying attention xD. I went to a museum where they had iPads in frames and actors playing parts to tell the story. When the program started and they went from still to moving and taking my 3 year old flipped out. Terrified. His brain could not handle that. I had to leave. It was pretty funny. 

But I like the idea of deep faking an actor onto a historical photo or painting and having them tell the story. Could make the experience more engaging. Just, you know, for slightly older kids. üò¨. This already exists for some years now in Museu JK in Bras√≠lia, where you see former Brazilian President JK himself talk about his accomplishments.. The 19crimes wine bottles did exactly this. imagine having this at Auschwitz. The vibessss man. YOO IM STEALING THIS. If that shit doesn‚Äôt get picked up by Disney for their rides.... What do you mean gimmick? That's awesome! I wonder if you could use this to generate enough frames of the photo person to perform a deep fake?. Very fitting choice of a model.. An incredible man with an unnecessarily tragic story. He was robbed of his dignity and never got any recognition while he lived. I hope he will be honored through the ages for what he's accomplished and enabled, and for the lives he saved by putting his genius to good use.. [removed]. I think they comment was filtered. Search ‚Äúmyheritage deep nostalgia‚Äù on Google.. This looks really similar, https://aliaksandrsiarohin.github.io/first-order-model-website/. Wow, that's so cool! What an amazing experience!. So happy for you. What software did you use.. It's only a fiction, a dream, a smoke-screen. Kill it with fire ffs. Thank you for your public service. died laughing!. [https://youtu.be/lhNNrhze3vs?t=83](https://youtu.be/lhNNrhze3vs?t=83). It's looking at me and I hate it. [removed]. They just use this company for the animation https://www.deidentification.co/reenactment/. !RemindMe 20 hours. The application of Rule 34 of the internet is as certain as the sun rising.. Idk what software OP is using, but there is this [https://github.com/alievk/avatarify](https://github.com/alievk/avatarify).. pretty sure thats all based on this or similar. https://aliaksandrsiarohin.github.io/first-order-model-website/

i played around with the colab getting similar results. they are mostly commercializing it, it would seem.. This is IP of MyHeritage.. I did one of my pics. It was [nightmare fuel](https://twitter.com/jsradford/status/1365815616192544772?s=19). Thank you for the nightmares. That's Alan Turing.. Thanks for the new nightmares. [deleted]. https://aliaksandrsiarohin.github.io/first-order-model-website/. I just used it a few times. It's creepy... If you use it on somebody where you know their facial expressions, the animation ends up looking nothing like them. However, for a picture of my grandpa who I never met, it's pretty fascinating :-)

I'm sure that the animation doesn't really look like him though. Of course not, that would end the whole industry. Please link the output.. Uncanny valley. Basically something that is *almost* human but 'wrong' in subtle ways sets off an alarm bell deep in our brain.. ###[View link](https://redditsave.com/info?url=/r/MachineLearning/comments/lui92h/n_ai_can_turn_old_photos_into_moving_images_link/)


 --- 
 [**Info**](https://np.reddit.com/user/SaveVideo/comments/jv323v/info/)&#32;|&#32; [**Feedback**](https://np.reddit.com/message/compose/?to=Kryptonh&subject=Feedback for savevideo)&#32;|&#32;[**Donate**](https://ko-fi.com/getvideo) &#32;|&#32; [**DMCA**](https://np.reddit.com/message/compose/?to=Kryptonh&subject=Content removal request for savevideo&message=https://np.reddit.com//r/MachineLearning/comments/lui92h/n_ai_can_turn_old_photos_into_moving_images_link/). Non-AMP Link: [https://bigthink.com/technology-innovation/new-ai-can-create-fake-videos-of-people-from-a-single-picture](https://bigthink.com/technology-innovation/new-ai-can-create-fake-videos-of-people-from-a-single-picture)

I'm a bot. [Why?](https://np.reddit.com/user/NoGoogleAMPBot/comments/lbz2sg/faq/) | [Code](https://github.com/laurinneff/no-google-amp-bot) | [Report issues](https://github.com/laurinneff/no-google-amp-bot/issues). [removed]. Can you imagine shouting that at a perfume tout.. Any sufficiently advanced technology is indistinguishable from magic.. I was thinking of the moving paintings on the walls haha. With a combination of the stuff like in Black Mirror (S2Ep1) Be Right Back feeding in any social media, texts, and video of the person you could 100% replicate the Harry Potter talking pictures of people. This is all very real technology and is only a matter of when not if.. Bitch this is reality, mathematics and cs. Microsoft already announced a few weeks ago that they have an AI chatbot that can talk like the dead person by using their history.. Came here to look for this comment lol.. Yeah except in Harry Potter magic was preserved only for "wizards" and attending school was invite only. If you didn't get a high school "wizarding" qualification you weren't allowed to use it and ended up on a government watchlist where owning a wand was prohibited.... If it helps people learn then go for it. It's ok the A.I. will then steal your job :). Let me guess, you work at the British Museum?. It would be funny if one of the displays had a real person in it but you thought it was a video and then they walk out of the display.. [deleted]. Just to expand on this briefly, for those who don't know... Alan Turing is, in many ways, the founder of modern computer science. So much of the technology we enjoy today is built upon his contributions to the field. Turing was a major contributor to the creation of the famous "Enigma" machine which was used to programmatically break the Nazi's encoded messages during World War II. It is estimated by some that Turing's work on Enigma shortened the duration of the war by years and saved millions of lives.

After the war ended, it was discovered that Turing was homosexual, which (at that time) was a crime in Britain. Turing was prosecuted for this and consequently, was chemically castrated by the government (I'm not sure if the castration was part of his sentencing or a plea bargain ‚Äî look it up yourself if that detail is important to you.) Shortly after this, Turing was found dead. Many believe that he committed suicide, although I believe this is still a matter of debate.

It wasn't until 2013 that the British government posthumously pardoned Turing.. Much respect for Alan Turing. And much sadnessüòî. yes, porn and credit cards. [removed]. I used my heritage website. Someone sent the link in this post.. C‚Äôest la vie.. [removed]. I will be messaging you in 20 hours on [**2021-03-01 12:37:18 UTC**](http://www.wolframalpha.com/input/?i=2021-03-01%2012:37:18%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/MachineLearning/comments/lui92h/n_ai_can_turn_old_photos_into_moving_images_link/gp6u3mu/?context=3)

[**2 OTHERS CLICKED THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2Flui92h%2Fn_ai_can_turn_old_photos_into_moving_images_link%2Fgp6u3mu%2F%5D%0A%0ARemindMe%21%202021-03-01%2012%3A37%3A18%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20lui92h)

*****

|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|. They way D-ID improved the separation between subject and background, at least in the examples, is really quite excellent and exciting. The momentum of the head staying one coherent object instead of continually morphing its boundaries is also both surprising and a little unnerving. Nope. They license it from D-ID 

FAQ
The technology that animates faces in photos looks like magic. How does it work?
The remarkable technology for animating photos was licensed by MyHeritage from D-ID, a company specializing in video reenactment using deep learning.. It‚Äôs beautiful. No it‚Äôs Harry Potter. [deleted]. This link works, don't know what everyone is saying no link for?. ok go away everybody
its like facebook..... Whosha good bot??. Good bot. Good bot. [removed]. Any sufficiently crappy magic is indistinguishable from Microsoft backed software.. Can I sell you on a Microsoft Share Point subscription to go along with that snark?. heard that in transformers haha. Yeah, I see the resemblance. Famous, dead, and still moving.. wasn‚Äôt that a black mirror episode?. Mind sharing a source for that?. Yeah okay... The pictures are pretty cool tho... Absolutely if it helps people learn go for it!!. [deleted]. Think of this implementation.

Have a viewing room of all victims, based on real photos recovered.

Generate movement like above link, then use RNN to generate voice generation. Just stating name, birth, when they arrived in the camp, I arrive at the camp on, I was a clerk, my mother was a nurse, and my father a professor, and etc. If I recall germans were notorious for record keeping and paperwork. Making a living instiution for guest to interact with. Would be very chilling. Imagine using GPT-3 to generate the audio too.. Turing is an inspiration. And let's not forget people like ada lovelace! Computer Science is the child of many innovators working together across history. Enigma was the German machine, not a creation of Turing. He did invent a machine to decode the encrypted messages of Enigma.. Handy meme image "[You like computers? Thank this gay atheist](https://i.imgur.com/dcRi147.jpeg)" (Turing, ofc). Didn‚Äôt he also have some sort of personality disorder? I can‚Äôt remember exactly where I‚Äôve heard that but I remember it being rumored he had a personality disorder that somewhat explained some rather strange habits of his as well as his behavior towards colleagues (which was rarely positive). Turing was an intellectual/mathematical giant. Still underestimated what he could've meant to  science... Not only for computers but for genetics etc. e.g. https://en.wikipedia.org/wiki/Turing_pattern

It's like killing Einstein at a young age because he was Jewish.. He ate an apple that he poisoned himself is my understanding.. I'd say mostly disappoinment and rage that other humans did this to him. We can always do better and it starts with love. [removed]. And I would bet they use a lot of external libraries that actually say they have.to share their sources too üòõ. To be honest, I die laughing every time I watch it. Ok, you convinced me.. Same. [removed]. Indeed.. It was the premise of Caprica.. Im stealing this. Have you ever been at a museum late at night and all the exhibits came to life on the same night robbers broke in to steal a priceless artifact but then it turns out the robbers are the rightful owners of that artifact and they were bringing it back to their home where it belongs and it turns out that artifact is what's brining the exhibits to life and then the real bad guy steals it and takes it to the History's Greatest Villains wax figure exhibit to bring them to life?. Why bring sexuality and religion into this ? Is this some kind of cognitive trick ? I really don't get it.. Being chemically castrated, sexually and romantically repressed on top of holding such a huge secret as having had to choose when to save lives or not during WW2, I don't think you need much more to find reasons why he may have behaved strangely... It could also be that the whole "personality disorder" storyline was born in a time when being gay qualified as such, and that's what it'd be referencing.. Agreed. The world was robbed of a brilliant man in his prime simply because of homophobia. He deserved so much better.. [removed]. [removed]. Which was a bit underrated imho. Maybe go learn a bit more about him.. I mean it could be, I remember it being something beforehand though, it was something like extreme OCD or something along those lines and it caused him to be very short with people because he hated stuff being out of place. But I get what you mean he definitely had some issues from his stress as well.. Religion is a plague 

It‚Äôs caused so much hurt with its texts. [removed]. [removed]. [deleted]. [removed]. [removed]. Yeah, I understand how I came across in the first one, I‚Äôll take the downvotes for that sorry, anyway yeah, that‚Äôs what I was talking about, and it was that in part that caused him to do a lot of weird stuff like apparently handcuffing his mug to his desk to that people wouldn‚Äôt move it?. [removed]. It could have been the other way around, that he was so bat shit crazy that they had to figure out a convenient way to get rid of him, and thus charged him with that homo stuff. Could be he just knew too much.. [removed][Project] From books to presentations in 10s with AR + ML. nan. Twitter thread: [https://twitter.com/cyrildiagne/status/1259441154606669824](https://twitter.com/cyrildiagne/status/1259441154606669824)

Code: [https://github.com/cyrildiagne/ar-cutpaste/tree/clipboard](https://github.com/cyrildiagne/ar-cutpaste/tree/clipboard)

Background removal is done with U^(2-Net) (Qin et Al, Pattern Recognition 2020): [https://github.com/NathanUA/U-2-Net](https://github.com/NathanUA/U-2-Net)

**/!\\ EDIT:** You can now subscribe to a beta program to get early access to the app: [https://arcopypaste.app](https://arcopypaste.app)  !. The future ü§Ø. Simple yet very useful. Thank you for sharing the code.. Almost guaranteed, Apple will copy your idea in 3, 2, 1..... Ohh the nightmare of making this into a stable product... Enough to drive you mad just thinking about it. Wtffff. Well that was incredible.. Apple can‚Äôt wait to steal this and not credit the creators. fantastic!. Why did the boxes in the diagram turn gray?. How does the Algorithm decide what it cuts out from the input pictures? 

For example it only cut out the two people in the picture and not the surroundings.

Amazing project though!. #WITCH!  BURN THEM!. Any sufficiently advanced technology is indistinguishable from magic.. This will be amazing if released, even as a beta. Definitely can see this being very useful. Really good work, thanks for sharing!. I'm extremely impressed with it cutting dark hair from a brown background. Is that the pixel's camera doing the hard work or is it U^2_Net ? Have you tried it with other phones? How does it deal with feathering? Stunning demo & thanks for posting this.. Super cool. Wizardry!. Woahhh that is so cool!!! I am wondering the speed wise from the initial snap till pasting it to computer.

If we could get it done >1s I think this project would be really fun and useful. Allow me to fork the project ;)

Thank youuu. This is God like!. Wow. What you did wlth AR is really creative and very impressive technically. Keep going dude you rock.. Holy fucking shit my jaw hasn‚Äôt dropped like this since I saw the GPT-2 demo. This is absolutely unreal‚Äîit is so precise + how the hell do they interact with macOS like that? Wow. Awesome work pal, so much respect.. Super cool demo.

But the more interesting part to me is the app actually look at the computer screen to decide what target the image/content is pasted to. 

Probably hard-coded, but super interesting idea.. This is amazing. Congratulations!!!. This is amazing! Thanks for sharing the code. Awesome! Recognize the catalog from Coder le Monde. That is so cool!. Awesome, will try it definitely.. Take my money. God this, and swiping a window to my laptop from my phone with a simple gesture, is what I have been waiting for sooo long.. cyberpunk level shit. Wow. Thanks for sharing!. This is really well done. From research to a simple yet useful use case!. beautiful. This is brilliant. Thanks for sharing.... Say sike ü§Øü§Ø. I saw this the other day and I thought it was incredible. I'm a novice on programming but ill do my best to deploy this on my PC just to play around with it! Thanks a lot for sharing this with the world!. Smart move. That's some next level copy -paste !. This is so cool! AI never ceases to amaze me.. 10 years ago people would laugh at this idea.. Wow this is so helpful, insane. This is so crazy!. So good it looks fake af. This is probably the coolest thing I have seen in a long while.  Great fucking work!. Wow, this is sick. You sir are a genius. What are the edges cases when this doesn't work? Does this require certain lighting conditions etc? How does it know to extract both people from the image?. Very impressive, thought it was fake at first.... ü§™. Amazing!. Wow.. This gets 100 very nices. This is insane.. Man, this is awesome!. This is amazing. If you have any intention of publishing this as an end user app, hit me up, I‚Äôll get make sure you get sponsorship for all the GPUs and other compute you need.. This is brilliant!. This is some crazy Tony stark shit. This is something really superb!!!!!!!!!!!  
I loved the technology...

AI and Machine learnings are actually contributing a lot in streamlining our daily processes. I mean, this is something, being a student I would need the most, instead of first emailing myself pictures from phone, then downloading them and inserting them in my doc.. Woke up in the morning and this is the first thing I see. A day can‚Äôt get more inspirational.  I can‚Äôt thank you enough for sharing.. Wow, this is epic!. What is difference between this and taking photo and sending it with email to computer? ü§î What is the main use case for this technology?. I really hope your idea doesn't get stolen. Also how do I keep up to date with your progress?. Did you train the ML model yourself? If so what data set did you use?. How were you able to get integration with chrome and slides itself? Are you able to load custom software through Google Slides somehow?. How do I do this?. This is so cool! Is it really necessary to point the phone at the screen to paste it?  Or will it just paste it into whatever application is currently focused no matter what?. [deleted]. u/fabiomb el otro d√≠a dec√≠as que andaba porque ten√≠a fondo de color blanco plano.. I'm more impressed with the background extraction on the photo than with the multidevice "copy-paste". u/vRedditDownloader. u/VeedditDownloader. u/vredditdownloader. That's so cool. There's no way that took 10s to develop, install, try and record an 57 sec video of. I mean, yeah, technology and stuff, but not in 10s. Sorry.. I will go through the damn code line by line!. Is that a Google Pixel?. I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/mattslinks] [Augmented reality cut n paste](https://www.reddit.com/r/mattslinks/comments/ht7vs5/augmented_reality_cut_n_paste/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*. This is beast!! Deff on to something!. Awesome.. How do we use this. more more..More..MOREEEEEE. Dude...love ur copy paste.... WITCH!. What the fuuuuuuuuck?. photoshop required ?. Omg! Thats awesome!!! I had a similar idea but using text. u/vredditdownloader. This is beyond science. My 5 years daughter thought of something similar..for her she wants that you take the object out of the screen and you show its hologram presentation..she said that would be a hard project to achieve :))
I will show her your project tomorrow, she will like it.. Oh sure. I find this after spending 29 days scanning in 21 years of issues of an instructional magazine on a flat bed scanner. [deleted]. Really impressive if it works as well with unseen data.

Still fun if it doesn't.. This is clearly fake....the last screen shot proves it.. In 3 seconds if you use anything else then MacBook üòè. Ok, that‚Äôs the coolest thing I‚Äôve seen in a long while.. This is awesome.. Tony Stark shit. Good job. Let we know when there's an easy and seamless way of doing this, or at least no-brainer. Wow dude, I don't know shit about ML, all I can say is this is superpower. I need this for editing for my small business. No more Adobe illustrator.. fuck capitalism


wreak havoc on the middle class. You‚Äôre going to be rich. If it wasn't for the link with the code, I would have straight up thought you were trying to trick us :o. This is insaneeee! I have a question about it's applications:

is it possible to use this to extract information? For example if you scan a receipt could it create a digital version with each line editable?. Holy smokes this is insanely awesome. Thank you. I mean, technically now it's the past.. Took words out of my mouth. His license even allows commercial use, so they are legally allowed to do that. Lol and he's using a pixel too. Likely, and it will be easier for them, the processing could be done in the iPhone and uploading can be done through airdrop (which supports 'aiming' at people and machines to share files).. Why would it be a nightmare?. [deleted]. They'll probably slap a patent on it too and sue the original creators.. > Apple can‚Äôt wait to steal this and not ~~credit~~ **sue** the creators

ftfy. Already stolen and and implemented into the next iOS.... Thanks!. U^2-Net decided not to remove the background of these :). Check out the details for U^2-Net on the official repo: https://github.com/NathanUA/U-2-Net. and any sufficiently understood magic is indistinguishable from technology.. >, even as a beta

The code is available, so you can play already with it :)   
[https://github.com/cyrildiagne/ar-cutpaste/tree/clipboard](https://github.com/cyrildiagne/ar-cutpaste/tree/clipboard). It is 100% handled by U^2-Net: Check out the official repo for more information and samples: https://github.com/NathanUA/U-2-Net. Yep maybe add a ghost non-transparent-background
to make up for the delay of the BG removal.

I'm just impressed by the copy paste AR stuff. Well done!. Hi! The coordinates are automatically defined by the receiving software in this demo but checkout my precious demo where I use OpenCV SIFT to find the correct coordinates on the screen. Please checkout the official U^2-Net repo for more information on the background substraction: https://github.com/NathanUA/U-2-Net
Edge cases mostly are busy scenes when there are no particular salient element. It just save time and headaches but the result is identical

Although you get background removal for free in the process ;). Looks like it also recognizes the photo subject to only copy the link important bits. Also faster. Thanks! For now the most updated news are on my Twitter!. I'm using the pretrained model from U^2-Net: Going Deeper with Nested U-Structure for Salient Object Detection, Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar R. Zaiane and Martin Jagersand: https://github.com/NathanUA/U-2-Net. Juste the clipboard and pyautogui to send the "paste" keystrokes :). Checkout the repository!. Good point! For now the code only paste at whatever app is active. In some apps (like Photoshop) you can paste at specific coordinates depending on where you point the phone. Not yet ;). Ah√≠ me gust√≥ m√°s üëç. *beep. boop.* I'm a bot that provides downloadable links for v.redd.it videos!

* [**Download** via https://reddit.tube](https://reddit.tube/d/1vfDX9G)

* [Audio only](https://v.redd.it/v492uoheuxx41/audio)

I also work with links sent by PM

 ***  
[**Info**](https://old.reddit.com/user/VredditDownloader/comments/cju1dg/info/)&#32;|&#32;[**Support&#32;me&#32;‚ù§**](https://www.paypal.me/synapsensalat)&#32;|&#32;[**Github**](https://github.com/JohannesPertl/vreddit-downloader). *beep. boop.* I'm a bot that provides downloadable links for v.redd.it videos!

* [**Download** via https://reddit.tube](https://reddit.tube/d/1vfDX9G)

* [Audio only](https://v.redd.it/v492uoheuxx41/audio)

I also work with links sent by PM

 ***  
[**Info**](https://old.reddit.com/user/VredditDownloader/comments/cju1dg/info/)&#32;|&#32;[**Support&#32;me&#32;‚ù§**](https://www.paypal.me/synapsensalat)&#32;|&#32;[**Github**](https://github.com/JohannesPertl/vreddit-downloader). have you had a chance to do that? has anyone tried running that code on their device?. Yes! But it also work with iphones. *beep. boop.* I'm a bot that provides downloadable links for v.redd.it videos!

* [**Download** via https://reddit.tube](https://reddit.tube/d/1vfDX9G)

* [Audio only](https://v.redd.it/v492uoheuxx41/audio)

I also work with links sent by PM

 ***  
[**Info**](https://old.reddit.com/user/VredditDownloader/comments/cju1dg/info/)&#32;|&#32;[**Support&#32;me&#32;‚ù§**](https://www.paypal.me/synapsensalat)&#32;|&#32;[**Github**](https://github.com/JohannesPertl/vreddit-downloader). Hmm, I don't think the 2nd image with the 2 persons could have be done with OpenCV?. What do you mean? The service runs remotely and it has never seen the images used in the video. I guarantee it's not fake.. Is the ELI5 version is this? - 

1. The React Native application on the mobile takes a pictures
2. The picture get uploaded to the server where the background is removed with u2-net, which is the brains of the operation
3. The removed background is then communicated through a python script > Photoshop plugin to be added on to a running project or is saved as a transparent image.

Right?. Yes. But you will need to a run OCR instead of the background removing application (U\^2-Net)  OP is using.. This would be the killer use case.. How can it be the past if its happening now?... id say the now its the future and past overlapping for a jiffy of a second. did you sue him. [deleted]. Well, why don't we already have cross-device AR interfaces and can swipe content between our devices seamlessly like Tony Stark? The U-net demonstrated here only provides a means to extract relevant sections of image data. The rest that needs to be done for this demo is far more difficult. Roughly, you need to identify the other device through the camera or NFC, pinpoint the relative position of the two devices for the onscreen insertion position, match the other device to a Bluetooth device or wifi connected device securely, set up a transfer, communicat data type and decide what should happen with the data... and do all of this across different OS and devices with different standards, handle poor connection, communicate all the issues to the user in a foolproof way. You can force the user to setup some of this manually, but then you'll loose 99% of the users and the product won't gain enough support/funding and gets dropped like a pair of Google glasses.. Wait. I thought they'll sew my ass to the mouth of another person who accepted the ToS.. But capitalism drives innovation by rewarding innovators. That's why we have all the smart humanitarian millionaires pushing humanity towards brighter future.

/s. I put up a public predictor API endpoint for the ML model so you don't have to battle with GPUs when playing with this. Simply start the server with `--basnet_service_ip http://basnet-predictor.tenant-compass.global.coreweave.com/` and that piece is taken care off.. Beautiful thanks! I'm excited to try this.. How long does inference take generally? Is your video realtime? Because it's surpringly fast for an HD photo from a phone.. How do you handle the domain shift between digital images and photos of images captured with a camera?

(i.e. perspective, glare, curvature, lighting)  
Or do you just hope the pretrained network generalizes well enough?. I see what you are talking about. Yess, definitely can do that!! OP is a badass. If it removes the background you want.. Oh...true. I was overthinking it haha. RemindMe! 4 days. Cool! I always assume the examples used in presentations are part of the training data unless told otherwise.

~~From a quick look at the code, I guess it's based on this paper?~~  [~~http://openaccess.thecvf.com/content\_CVPR\_2019/papers/Qin\_BASNet\_Boundary-Aware\_Salient\_Object\_Detection\_CVPR\_2019\_paper.pdf~~](http://openaccess.thecvf.com/content_CVPR_2019/papers/Qin_BASNet_Boundary-Aware_Salient_Object_Detection_CVPR_2019_paper.pdf)

&#x200B;

Nevermind, the description on Github answers that question, was just to lazy to read it before jumping into the code :P. Awesome! thanks for the reply! 

Completely understand if you're not able offer this, but do you know of any good, reliable OCR services?

All the ones I've found, tend to be geared towards a particular functionality and after testing several they just don't live up to the standard I'm expecting. No it wouldn't, because it already exists in dozens of apps. Including Google Lens, installed on hundreds of millions of phones.. Future, past, now is simply an illusion. Reality is one continuum, it can't be neatly divided into division as such. But, this kind of conceptualization might have tremendous utility in our daily life.. Why bother patenting it? Does he have the money to patent it, does he have the money to protect the patent against apple or google, can it be patented, does he have the intellectual resources to patent...

Patents are for companies to monetise their R&D, not for individuals to get rich. This dude would probably be happy enough getting some corporate credibility and could potentially lead a team if google or apple are interested in this.. Yeah ok, we'll wait here to hear the news in 3 years about whether or not it got approved.. Yeah man and this all those companies fault who use different standard for every fucking thing (microsoft and apple looking at u) .. You just need to run some visualbasic to check the camera on the phone to know where on the pc it's pointing to, then send that image to the pc with the coords to paste the amethyst. I can do this with one weekend.. What system does drive innovation then? Do you wanna say that socialism/communism pushed their country towards brighter future?. Video is real-time but inference is don't on a 320x320 image. But that's only the resolution of the alpha mask , the image can have native reslution. I will be messaging you in 4 days on [**2020-05-15 12:56:47 UTC**](http://www.wolframalpha.com/input/?i=2020-05-15%2012:56:47%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/MachineLearning/comments/gh1dj9/project_from_books_to_presentations_in_10s_with/fq9lu04/?context=3)

[**CLICK THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2Fgh1dj9%2Fproject_from_books_to_presentations_in_10s_with%2Ffq9lu04%2F%5D%0A%0ARemindMe%21%202020-05-15%2012%3A56%3A47%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20gh1dj9)

*****

|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|. Have you tried tesseract? One year back i tried it with a little project, it work quite well out of the box.. [deleted]. Where can I read more about this ?. https://xkcd.com/927/. Sure! Go for it buddy.. I think if workers were to own their workplaces, instead of stock holders, we would be living in better world. In effect that means you can only own a piece of company if you work there.

This is theory, I don't think this has ever been tried in any existing society. There are some worker-cooperatives but they have to compete with capitalist companies which do not have the moral restrictions a worker owned company have, so they are naturally at a disadvantaged position.. no never heard of it! kicking myself now that I wasn't able to find it whilst looking for a solution.

This looks promising so thank you :) I'm a designer with some programming knowledge myself so I imagine it'll take a while to really test it out for my purpose.

Thanks so much again!. Makes sense!. Idk I cooked that up [I'm a wannabe philosopher ;)] but you can find many parallels in some Philosophers' works like Neitzsche's Cause and effect theory.

I must add, both Space and Time are wholly inference based derivatives, we can't or haven't perceived them. All the Space-time continuum metaphysical talks are purely theoretical. No experiment have given emperical evidence of them.. Wow never thought like that cheers !!!. dId He dO It. So worker-owned companies have been tried and failed, but it's capitalism's fault?

Actually, how do these things even work? Who makes the decisions in the absence of clear owner? What constitutes a "worker"? How are the company shares divided between workers - evenly, or according to their positions?. Just to be clear: Apple is a company that was owned by its workers initially. Those workers decided to sell a piece of it to investors early on because they (the workers) decided it would help them grow faster. Then they were successful, and they (the workers) decided to sell more of the company so they could buy nice houses and make charitable contributions. Apple *is* the result of a worker-owned-company system, albeit one that gave those workers the freedom to sell their stock for various reasons along the way.. Thank you. Sounds fascinating!. >So worker-owned companies have been tried and failed, but it's capitalism's fault?

That's like saying "not stealing has been tried but it was less profitable to stealing, so now you are blaming thieves that the losers who don't steal lost?"

>Actually, how do these things even work? 

You can research these topics yourself. For example /r/Anarchy101 has smarter people than me to explain these topics.. That's interesting. Didn't know that. But Apple was worker-owned company until it turned into capitalist company. When they hired their first employee who didn't own a piece of that company, they made an ideological choice to exclude that employee from the profits the company produced and in effect created hierarchy inside their company.. >"not stealing has been tried but it was less profitable to stealing, so now you are blaming thieves that the losers who don't steal lost?"

The reason not to steal is because there are deliberate mechanisms in place to dissuade, not because it's a less efficient way to make money. In contrast, socialism *is* less efficient than capitalism.

>You can research these topics yourself.

Why is it that every single anti-capitalist assigns homework when poked with a stick? Consider that you are arguing in favor of uprooting industry as a whole and can't even articulate why.

>This is theory, I don't think this has ever been tried in any existing society.

This is another favorite. You are betting the farm -- hell, *society* -- on a theory. On something that has not even been validated. Doesn't that seem a little bonkers to you?. Actually, in most countries stealing is less profitable than any legitimate way of earning money thanks to law enforcement. So in the same vein, it's USA government's fault for letting capitalism go unchecked there, but no fault of the system itself.. I suspect for much of Apple‚Äôs life, the vast majority of Apple employees owned a piece of the company. I don‚Äôt know if Apple Store employees do - they may not - but Apple stores are a comparatively recent addition and I bet the engineering staff through the 90s were employee-owners as that‚Äôs the norm in Silicon Valley. At some point Apple made a decision to contract out manufacturing, so the people actually building Apple computers and phones are mostly not Apple employees and not owners. And it was certainly a significant decision to bring on venture capital investors (and later public shareholders) who were not employees of the company (though I think history would show pretty clearly that if they hadn‚Äôt done that we wouldn‚Äôt have Apple today). 

But I do think it‚Äôs worth noting that all of these *decisions* were, at Apple, mostly made by the early employees / founders, not by third party shareholders. That‚Äôs idiosyncratic to tech, an industry dominated by strong founders, but it‚Äôs true at Apple, at Google, at Facebook, at Amazon, etc - the decision makers are the founding employees. In fact at many of these companies the founders have put in place systems such that ‚Äúcapitalist‚Äù public owners explicitly DON‚ÄôT have control of the business, only the founders do, long after the founder ownership levels have decreased.

Even Goldman Sachs was a ‚Äúpartnership‚Äù for most of its history - ENTIRELY owned by a subset of its employee population. This is true for every major large corporate law firm today. Just because these businesses are ‚Äúemployee owned‚Äù clearly doesn‚Äôt mean they‚Äôre run ‚Äúfor the benefit of the people‚Äù - they‚Äôre run by rich early employees who want to get richer (and maybe have other motivations, like building great products, or personal celebrity, or whatever). 

I am a huge fan of ‚Äúemployee equity ownership‚Äù - most startups are built on the back of this idea - though most employees in turn eventually want to be able to sell their shares to other people (so they can buy houses and cars, or make charitable contributions or whatever). But I‚Äôm not sure employee ownership is a radical departure from ‚Äòcapitalism‚Äô as you describe it - the ends ultimately look pretty similar to companies that are not employee owned.. > Why is it that every single anti-capitalist assigns homework when poked with a stick? 

Because I'm not your teacher. You look up this stuff yourself if you are interested.

>You are betting the farm -- hell, society -- on a theory. On something that has not even been validated. Doesn't that seem a little bonkers to you?

That's the dilemma of sociology in general. You can't run controlled experiments without affecting human lives and you can't remove yourself from the equation and be a neutral observer because you are part of that society.. > Because I'm not your teacher. You look up this stuff yourself if you are interested.

I am interested, and I have tried to look it up. I'm pointing out that it's not a compelling argument to say "go look it up" when you are trying to change the status quo.

It doesn't matter whether you convince me or not; I'm not here to be convinced, and you aren't here to convince me. The problem is that *no one* can seem to articulate why we should be socialist. It always ends up as a homework assignment no matter who I talk to. 

>That's the dilemma of sociology in general.

I agree. 

Fortunately for capitalists (and unfortunately for you), capitalism isn't the outcome of a controlled experiment. Rather, it is [*emergent*](https://en.wikipedia.org/wiki/Emergence). The free market exists as a result of every individual acting according to his or her individual incentives, not because a committee decided that this is the way we should do it.

So not only is the statement that we should uproot the entire economic system incredibly arrogant, since it is predicated on the assumption that you will implement it properly (in the context of the incentives that exist for the people implementing it), proving that it will work at all requires evidence that cannot be obtained. As you noted, there is no way to conduct a controlled experiment in this area, so we are kind of stuck with what we've got.

[Here](https://en.wikipedia.org/wiki/Holodomor) is what happens when you get too clever.[R] First Order Motion Model applied to animate paintings. nan. I cant stop watching the actress, it‚Äôs like she‚Äôs studied Disney princesses all her life.. That moving pharaoh will be my next sleep paralysis demon. Taken from https://twitter.com/AydaoGMan/status/1234531519349350402

Utilizes First Order Motion Model for animation: https://arxiv.org/abs/2003.00196

Project Page: https://aliaksandrsiarohin.github.io/first-order-model-website/
 
Code: https://github.com/AliaksandrSiarohin/first-order-model. A friend of mine recently adapted this model for Skype, Zoom, etc. Very easy to install.
http://github.com/alievk/avatarify. I want this done on The Scream. That's delightfully creepy. Anyone think she looks like Elizabeth Holmes??. This could result very helpful to vtubers in the future. seriously impressive how different angles can be projected as well.. How can I see more that girl doing shit with her face?. What‚Äôs the painting in the top left though?. Kinda creepy. Can we train on this one from r/woahdude https://v.redd.it/iqptq372itu41. Sorry for the noob question, but what does "first order" mean here?. Hello, maybe this has been asked before but how can I get this software/ learn about it. I'm a motion designer with little knowledge of code but I'm willing to learn. I honestly don't understand the hype as this is old news.  A team at Samsung AI demonstrated this with few-shot learning.  [https://arxiv.org/abs/1905.08233](https://arxiv.org/abs/1905.08233). how do you do this? like what program? its cool. Same can be done live via face2face.. Nefertiti is scary!. Amazing. Harry Potter moving paintings??. The pearl earring girl looks freakishly realistic. The Girl with a Pearl Earring looks like she's having a stroke. Nefertiti looks so good. Nice.. Is there GAN for language? What's the best paper / code to watch?. Does this remind anyone else about the moving pictures in Harry Potter?. Couldn‚Äôt stop watching Nefertiti. What a babe!. Lovely. But I wish I can hear what they're saying to me.. Is it me or has this thread recently acquired a lot of members? This was posted yesterday, and now I believe it is the highest upvoted post.  After looking at the rules, I guess the crowdedness is more common on weekends.. Where can I test this out myself? Do I need my coding skills or a fast computer?. CUTE. It's perfect except for the wink. Ahhh yes, now I am terrified. Nice. [removed]. I‚Äôm pretty sure there was a post that had something similar to that. Completely mesmerizing.. Source: https://vm.tiktok.com/7vrjeu/. It's very TikTok.. I watched and marveled at the tech applied to the other three for about two loops, then caught myself watching her for more than that. Stunning.. How to move like Disney characters:
https://www.instagram.com/p/B0RgjtrllHt/?igshid=1i2dyqs7j2c3c. Every single person on TikTok does this same exact thing. It‚Äôs not that special.. YES DOOD. I‚Äôve noticed this a lot with Tik Tok style videos and people that seem to make a lot of them. It definitely seems like pumping Tik Tok content is good practice for nailing a facial expression for a certain emotion on demand. I think humans in general have been doing this a long time but being in enough social situations where this kind of skill is useful or necessary was more rare.. If you need it chased out, there's always the [Globglogabgalab](https://youtu.be/hLljd8pfiFg).... Given it's Nefertiti, should be a quite interesting demon.. [deleted]. Thanks for the attribution and link to my twitter! Much appreciated üòÅ. I've been showing up to all of my online classes as Obama for a week now with this. Great stuff! It's a bit laggy because my graphics card is a little bit old (GTX 1070), but it's really not that bad, although certainly not as smooth as in the video. sadly it requires a nvidia card to get accelerated.. The mouth doesn't work for some reason.... Well, THIS changes everything!
My D&D game has just been UPPED!!. Anyone with more technical know how than me have any thoughts/concerns about this?. [is this your friend](https://old.reddit.com/r/okbuddyretard/comments/g0fdtw/obama_kinda_vibin_doe/). It would be scary, I would imagine. Tried it. Hasn't worked so far w/the pretrained model I tried.   Perhaps too stylized a nose? Maybe not enough correspondence points?. All of them. Even the real one. Especially the real one.. I was thinking the exact same. It's the target, the AI is trying to reproduce her facial expressions on the 3 paintings/photos (Warhol, Nefertiti, Vermeer). top left has the tiktok handle. First order Taylor expansion. It's related to Star Wars. The authors have a colab notebook on the [github repo](https://github.com/AliaksandrSiarohin/first-order-model) for this project. It pretty much walks you through the process and lets you try your own. Pretty fun. Just open the [demo.ipynb](https://github.com/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb) file, it gives you an option to open in colab.. Adobe Character Animator will get you these results.. [deleted]. Oh no /r/all is here. 1. You get the code from their GitHub and run it on your machine which has Nvidia gpu preferably

2. You use Google colab https://github.com/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb
Using this file to run it on Google's server. It's pretty straightforward, but you should get some idea about programming I guess.. [removed]. Damn. Now you got me curious.. Is China not even hiding thier attempts at facial recognition on this app?. Every single actor acts, but some do it better.

Every single chef cooks, but some do it better.. You do it then. Thank you for reminding me this still exists after 4 yrs. I'm gonna tell God about this.. If you tell the truth, you don't have to remember anything. Well that‚Äôs stuck in my head now, fucker.. It's a woman. Nefertiti.. It's very cool- definitely the coolest FOMM demo I saw :). A 6GB 1070GTX can't handle this decently? Damn.... What were the responses from your teachers/classmates?. Try pressing F to match your camera to the avatar. That solves a lot of issues for me. Make sure you have good lighting and are close to the camera. this looks fun and would be badass to implement in a dnd game. r/woosh. that is a sculpture, print and an oil painting. [deleted]. What does First order Taylor expansion mean?. 
I see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't 
render large Jupyter Notebooks, so just in case, here is an 
[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:

https://nbviewer.jupyter.org/url/github.com/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb

Want to run the code yourself? Here is a [binder](https://mybinder.org/) 
link to start your own Jupyter server and try it out!

https://mybinder.org/v2/gh/AliaksandrSiarohin/first-order-model/master?filepath=demo.ipynb



------

^(I am a bot.) 
[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) 
[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) 
[^(Author)](https://johnpaton.net/). Doesn't character animator require you to define correspondences to different key mouth positions?   This is much faster when it works. Nothing to configure.  Also, I thought that was 2D. This is doing some 3D perspective warping when you turn your head etc.. Wait one year and you'll have it on your phone.. Is this C plus plus?. 
I see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't 
render large Jupyter Notebooks, so just in case, here is an 
[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:

https://nbviewer.jupyter.org/url/github.com/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb

Want to run the code yourself? Here is a [binder](https://mybinder.org/) 
link to start your own Jupyter server and try it out!

https://mybinder.org/v2/gh/AliaksandrSiarohin/first-order-model/master?filepath=demo.ipynb



------

^(I am a bot.) 
[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) 
[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) 
[^(Author)](https://johnpaton.net/). I think it was this one I was thinking about:

https://imgur.com/r/funny/98wyFSN. Excuse my ignorance, but what do you mean? What‚Äôs giving it away?. [removed]. Jesus wept!. Yeah, the frame rate is just choppy that's all. Honestly, maybe I had too high standards but it was pretty meh, mostly just a couple of laughs and then asking how I did it. But I had a online boy scout meeting and that had a lot better results and was pretty fun. r/yourjokebutworse. The transformation between two images (for instance, from a video of a moving face) is typically encoded by a dense motion field (or optical flow) which means each pixel has an associated motion which can be quite intricate.

In this paper, such a transformation is approximated by taking multiple points of interest in the image and for each point, estimating the Taylor series expansion of that particular sub-transformation while observing the transformation's effect in a small neighborhood, which is much easier than trying to estimate the dense motion model. This Taylor series only has the first order derivative term, neglecting higher order terms for simplicity.. Nope they have a live puppeteer option with mocap. It‚Äôs hit and miss.. Looks like python. No it's Fortran. That's actually a C+. The second plus reflects my attitude of how I felt about the C+. It was a typing class.. That little hand slap at the end.. They‚Äôre basically giving the labels in audio and asking users to record ground truth video for the label. More like classifying facial expressions, though.. Furthers thier ability to label and determine emotions and nuanced facial expressions. It probably is used to make their current facial recognition more accurate as well, larger data set. It's especially creepy that the voice is auto generated. You do not want a state power to have the ability to determine your emotions on the fly, for any reason whatsoever.. [removed]. Wonderful explanation. Thank you.. I mean watch the hand closely after it slaps the heart on lmao. [removed]. [removed]. [removed]data siens. nan. I was once reading this article that went as: ‚ÄúThe AI already predicted how many goals Cavani will score at Manchester United‚Äù. It was a linear regression.. How do you change your setup on Jupyter to look like this?. The reality of 9/10 data scientist job specs!. oh fuck üòÇüòÇüòÇthis had no business to be funny. ARtYfIshaL NtEliGeNtS. What's the problem here, this looks like my 8-5 ü§î. When Managers waste your time with their Excel problems. Damn is that GPT-3??. Relatable. Me after that bootcamp. I literally laughed out loud when I saw this, OP. Good job.. I laughed way harder than I should've, nice one. Dual monitor missing!. Best compliment I've ever received was "it's nice to work with someone who can add 2 + 2 and get 4". Data Sins 

Data Satanist. I like how it is written in comic siens. need more memes. I just laughed way too hard. Thank you for this.. You forgot the .fit() !. One day, I was looking around for a laptop that are capable for data analytics. The conversation with the retail associate went something like this:

* Retail associate (RA): Hi, how can I help you today?
* Me: yeah, I'm looking for a laptop
* RA: What kind of laptop are you up to? We have ones for data, gaming, or work.
* Me: can I look at the one for the data analytics?
* "RA proceeds to show me laptop design for (what I assume) data analytics"
* RA: What kind of data are you going to be analysing? This laptop supports Excel and will be excellent for any type of data (the laptop was running Intel i5 with no dedicated GPU, with 4-8gb of ram, IIRC)
* Me: Yeah, I'll be analysing data with, at a minimum, 400MB in size, and I won't be using excel. Do you have anything with at least 16gb RAM and a dedicated GPU?

I tried to explain that I don't work with excel. This conversation went back and forth for several minutes before I gave up.

Needless to say, I didn't get a new laptop that day since nothing comes without windows, or 16GB of RAM, or the last GPU.. Truth.. pls do data siens on excel dataframe. This is art. can relate. Dayduh signs. Just went to Master Degree In Data Science and Statistics, and impressed with this. This is briliant.. #SportsAnalytics. I‚Äôm a ManU fan. I want Cavani to do well. But I‚Äôm also finishing up my Masters in Data Analytics and this made me laugh out loud. Well done. tbf im pretty sure a linear regression could predict Lewis Hammertime as a 8 time world champion. Should I write an article and call it AI?. IMPORT EXCEL DATAFRAME. [deleted]. It would be...

Requirements:
PhD and 15 years experience required

Job duties: 
Meme. God the fact that he didn't even refence another cell is what gets me. He typed in =1+2. Right, this is the last place I expect to see a meme that kills me üòÇ. Read this as "artificial negligence" and it also applies. After learning how to use "if" function üòÇüòÇ. MuHsHeEn LeRnInG. üòÇüòÇüòÇüòÇ. You da best siens. I'm sure we can figure this out with 50% more data and a linear regression!. I got paid 6 figures at one point in my career to show people how to connect their Excel workbooks to external data sources.. Nothing like VBA to automate data flows though.. I think so fuck can anyone link me resources on how to get started in data science? I hear it‚Äôs the sexiest job in the world and I wanna make 200k someone help is this boot camp good?. Brain successfully melted.. Dude, same this is great. To be fair to the retail person, you should've said "I write code" or something. Most people don't have the slightest clue about the tools data experts use!. I love how they emphasised on ‚ÄúThe AI already predicted‚Äù as if it were some sort of westworld-esque superior mind that already made up its mind on how the future will unfold. Hilarious.. What program u in?. The answer is yes. Call all the things AI.. from excel import *. A middle schooler with 10 yrs of Tensorflow and Spark experience üòÇ. `from AI import machine_learning`

Engineering. I did that today. I'm just of six figures.. You deserve that. I got hired as an "Analyst" because I look nerdy. I'm practically just playing around with excel and googling everything rn.

Ps: help. Idk anything. Sir that is a US Army boot camp :P. Bruh it's easy, just complete this bootcamp from Stanford: [https://cs.stanford.edu/academics/phd](https://cs.stanford.edu/academics/phd). I did. Got a slightly better specs but still no GPU. I told them I need RAM, at least an i5, and a dedicated GPU

Edit: I guess I didn't explain enough on the specs-to-pricing categories they have. The categories could be broken down to 3 categories, basically. Low, mid, and high. The high-end would be the gaming laptops, mid would be the coding and data, low would be the for works only. Obviously can't buy from the low-end categories. The high-end is, IMO, a low-end gaming computer which has the similar specs as my current laptop just with a dedicated GPU. Si, when I came into the store, I thought the programming and data category would have better specs, since I've been doing some programming and IDE takes a lot of resources to begin with.

Edit 2: I also need Linux as my primary OS. Since MacOS is slowly becoming a walled garden and windows, well, has tons of bloatware and needs 2GB RAM at the minimum. Also that I'm used to work in a Linux environment and preferred it.. Lol. Please find and share this article with me. I can‚Äôt stop laughing. It was Precogs. I mean even put aside the fact that ai and ml are used without understanding, that's pretty horrendous. the joke here is that TF came out 5 years ago and Spark came out 6.. I, too, would like to get paid six figures to do this. Curious to know, what roles and types of companies are y'all in?. Oh shit thank you! Do you think this will be good enough for unpaid internship ?. Yes, that is the joke lol. [deleted]. Go get a PhD. Then you too, won‚Äôt want to get paid 6 figures to do mindless work.. To be fair, it's not like someone called me and directly asked this.  They called me up and told me what their problem was, and their proposed solution given the tools at hand.  They had some data in a Sharepoint list, and needed an easy to ingest it, so Excel is a tool for that with a low barrier to entry.

I work on a BI team, and we basically solve data-related problems for a huge organization.  Some of those problems are teensy tiny ones like today's. We have an overarching data governance plan that we are slowly working at achieving, so every time we get a cold call to solve a problem we at least have a general direction we want to go.. Maybe not on it's own, make sure you have interesting personal projects with thousands of users, and contribute to open source every day.. There are a lot of people who read this and aren't Data Scientists. Saved someone a search.. I laugh at those ads for a school or business that adds up the experience of the staff to make it sound like they're more qualified.

"We have over 2000 years of Java experience!!!"

If 10 students can play a symphony in 40 minutes, how long will it take 30 students to play the same symphony?. I'm starting to understand why people with my background look down on those who go the applied route. Well, I don't understand why you have to be a jerk about it, but I do understand that you just end up answering questions that you did in 5th grade.. I don't have a PhD and make just shy of 200k as a Sr. Data Scientist. I do have a few PhDs who work for me. Being a jerk doesn't make someone a good data scientist.. I hope you don‚Äôt think I was being a jerk about it haha. 

If you meant the academia people looking down on the applied people, totally agree though. I think people just end up feeling so superior because they‚Äôre so smart, and people ask them these 5th grade level questions. It‚Äôs bad reinforcement I guess... I feel it myself and I do not have a PhD. My managers look at me like I‚Äôm performing black magic at my computer, and I‚Äôm over there pissed off because I want a real challenge.. it‚Äôs complicated, idk. I‚Äôve had times when the superiority complex got the best of me, and times when I‚Äôve been able to stay humble. 

No one wins in the superiority complex thing. It‚Äôs just rude, as you said. Different people have expertise in different things. It is 2020 and obviously some skills are a lot more in demand than others, but if you‚Äôre a software engineer and you look down upon plumbers, you‚Äôre likely a fool (not you of course, just in general).. Not you. I come from math background, specifically algebra, and even algebraists are looked down on by Category Theorists and other 'more theoretical' disciplines. It's ridiculous.   


But you can take my approach and just drink yourself to death to help with the crippling anxiety of not having enough work to stimulate you, that getting a new job in this job market is terrible, and if you become unemployed you might as well just neck it because the US government doesn't give a fuck about you. Shiiit brother stay strong. Realize you‚Äôre likely in the 1% intelligence wise in the world. Don‚Äôt let it go to your head, but don‚Äôt let negativity bring you down. 

Your approach sounds... not ideal. You‚Äôll find something, the world is truly fucked up right now. But persistence outweighs everything else. If you need someone to talk to shoot me a dm. Stay safe